{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider training set (features and targets) and a linear regression model (univaraite) with $w$ and $b$ parameters, $\\hat{y} = wx + b$. \n",
    "Here $w$ is the slope, $b$ is the intercept, $x$ features (input), $y$ (labels)\n",
    "Using training set, we need to find $w,b$ that provide a good fit.  \n",
    "$\\hat{y}^{(i)} = f_{w,b}(x^{(i)})$\n",
    "\n",
    "Const function: compares $\\hat{y}$ to $y$. The Error=$\\hat{y}-y$.  \n",
    "- Squared error cost function:  \n",
    "$J(w,b)=\\frac{1}{2m}\\Sigma_{i=1}^N(\\hat{y}^{(i)}-y^{(i)})^2$ or  \n",
    " $j(w,b)=\\frac{1}{2m}\\Sigma_{i=1}^N(f_{w,b}(x^{(i)})-y^{(i)})^2$\n",
    "\n",
    "By iterating over $w,b$, a set of cost-function values can be computed. And $w,b$ that gives the smallest $j(w,b)$ can be found.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from lab_utils_uni import plt_intuition, plt_stationary, plt_update_onclick, soup_bowl\n",
    "\n",
    "# define th const function for lienar regression:\n",
    "def compute_cost(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the cost function for linear regression.\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,)): Data, m examples \n",
    "      y (ndarray (m,)): target values\n",
    "      w,b (scalar)    : model parameters  \n",
    "    \n",
    "    Returns\n",
    "        total_cost (float): The cost of using w,b as the parameters for linear regression\n",
    "               to fit the data points in x and y\n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m = x.shape[0] \n",
    "    \n",
    "    cost_sum = 0 \n",
    "    for i in range(m): \n",
    "        f_wb = w * x[i] + b   \n",
    "        cost = (f_wb - y[i]) ** 2  \n",
    "        cost_sum = cost_sum + cost  \n",
    "    total_cost = (1 / (2 * m)) * cost_sum  \n",
    "\n",
    "    return total_cost\n",
    "\n",
    "# define training data\n",
    "x_train = np.array([1.0, 2.0])           #(size in 1000 square feet)\n",
    "y_train = np.array([300.0, 500.0])           #(price in 1000s of dollars)\n",
    "\n",
    "# plot data using custom func # TODO make it work here! It freezes the kernal\n",
    "plt.close('all') \n",
    "fig, ax, dyn_items = plt_stationary(x_train, y_train)\n",
    "updater = plt_update_onclick(fig, ax, x_train, y_train, dyn_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_bowl()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descend\n",
    "\n",
    "Find minimum of a given function: $\\min(j(w_i...w_n,b))$. \n",
    "- Start with random intiial guess. Change parameters and see if $j(...)$ goes down. \n",
    "- Keep taking steps into the direction of a steepest descend. \n",
    "\n",
    "Local minima are the attractors of gradient descend.  \n",
    "\n",
    "### Implementation\n",
    "Algorithm: update **simultaneously** $w$ and $b$ as  \n",
    "| $w_{tmp} = w - \\alpha \\frac{\\partial}{\\partial w}(J(w,b))$  \n",
    "| $b_{tmp} = b - \\alpha \\frac{\\partial}{\\partial b}J(w,b)$  \n",
    "| $w = w_{tmp}$  \n",
    "| $b = b_{tmp}$\n",
    "- $\\alpha$ is a `learning rate` *how big of a step to take*. If too small, slow convergence. If too large algorithm may diverge. \n",
    "- $\\frac{\\partial}{\\partial x}$ 'derivative' the sign of whick determins the increase or decrease of a $w$ or $b$\n",
    "\n",
    "If the $J(w...)$ is already at a local minima, but there are **many** local minima. In the algotithm $\\frac{\\partial}{\\partial x} = 0$ at the minima and $w=\\rm const$. \n",
    "\n",
    "Gradient descend can reach a local minima with **fixed** learning rate, as $\\frac{\\partial}{\\partial x}$ decreases when approaching a local minima.\n",
    "\n",
    "### Building the model\n",
    "\n",
    "Consider $f_{w,b} =  wx + b$ and compute the derivatives  \n",
    "$$\n",
    "\\frac{\\partial}{\\partial w}J(w,b) \n",
    "= \\frac{\\partial}{\\partial w}\\frac{1}{2m}\\Sigma_{i=1}^m\\Big(f_{w,b}(x^{(i)}-y^{(i)}\\Big)^2 \\\\\n",
    "= \\frac{\\partial}{\\partial w}\\frac{1}{2m}\\Sigma_{i=1}^m\\Big(wx^{(i)}+b-y^{(i)}\\Big)^2 \\\\\n",
    "= \\frac{1}{2m}\\Sigma_{i=1}^m\\Big(wx^{(i)}+b-y^{(i)}\\Big) \\times 2x^{(i)} \\\\\n",
    "= \\frac{1}{m}\\Sigma_{i=1}^m\\Big(wx^{(i)}+b-y^{(i)}\\Big)x^{(i)}\n",
    "$$\n",
    "\n",
    "This also shows why `cost function` had $2$ in it. To cancel the term in the derivative.\n",
    "\n",
    "Then, the algorithm is: Repeat:\n",
    "$$\n",
    "w = w - \\alpha\\frac{1}{m}\\Sigma_{i=1}^m\\Big(f_{w,b}(x^{(i)})-y^{(i)}\\Big)x^{(i)} \\\\\n",
    "b = b - \\alpha\\frac{1}{m}\\Sigma_{i=1}^m\\Big(f_{w,b}(x^{(i)})-y^{(i)}\\Big)\n",
    "$$\n",
    "untill convergence (updating them simultaneously).\n",
    "\n",
    "Note. `Squared error cost function` will **never** have multiple local minima as it is 'bowl shaped'. This is so-called `convex function`.\n",
    "\n",
    "Note. If at each step of gradient descend, **all the training data** is used, it is called `batch gradient descend`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "from lab_utils_uni import plt_house_x, plt_contour_wgrad, plt_divergence, plt_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our data set\n",
    "x_train = np.array([1.0, 2.0])   #features\n",
    "y_train = np.array([300.0, 500.0])   #target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate the cost\n",
    "def compute_cost(x, y, w, b):\n",
    "   \n",
    "    m = x.shape[0] \n",
    "    cost = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        cost = cost + (f_wb - y[i])**2\n",
    "    total_cost = 1 / (2 * m) * cost\n",
    "\n",
    "    return total_cost\n",
    "# compute gradient of the cost function\n",
    "def compute_gradient(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      x (ndarray (m,)): Data, m examples \n",
    "      y (ndarray (m,)): target values\n",
    "      w,b (scalar)    : model parameters  \n",
    "    Returns\n",
    "      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w\n",
    "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b     \n",
    "     \"\"\"\n",
    "    \n",
    "    # Number of training examples\n",
    "    m = x.shape[0]    \n",
    "    dj_dw = 0 # derivatives\n",
    "    dj_db = 0\n",
    "    \n",
    "    for i in range(m):  \n",
    "        f_wb = w * x[i] + b  # linear regression\n",
    "        dj_dw_i = (f_wb - y[i]) * x[i] # its derivative w/r w\n",
    "        dj_db_i = f_wb - y[i]  # its derivative w/r b\n",
    "        dj_db += dj_db_i # increment\n",
    "        dj_dw += dj_dw_i \n",
    "    dj_dw = dj_dw / m # recall normalization by m\n",
    "    dj_db = dj_db / m \n",
    "        \n",
    "    return (dj_dw, dj_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyBlastAfterglow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
