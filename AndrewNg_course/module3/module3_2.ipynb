{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overvitting with linear and logistic regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Underfitting` = `high bias` (algorithm cannot fit the data well. Missing a trend). This is due to an underliying assumption about the data. If we chouse a polynomial with a higher order that fits the data we.  \n",
    "`Generalization` - building a model that should fit well not only the training set but also the new data.  \n",
    "`Overfitting` = `high variance` - model fits the data well, but predicts the values that goes against the trend seen in data.\n",
    "\n",
    "This applies to linear regression and classification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization to reduce overfitting\n",
    "\n",
    "Solution to **overfitting**:\n",
    "- Collect more data \n",
    "- Use **fewer** features (`feature selection`)\n",
    "- Regularization (gently reduce the feature impact, or size of parameters (at higher order))\n",
    "\n",
    "## Cost function with regularization\n",
    "\n",
    "Consider a minimizations that needs to be done \n",
    "\n",
    "$$\n",
    "\\min\\frac{1}{2m}\\sum_{i=1}^m\\Big( f_{\\vec{w},b}(\\vec{x}^{(i)}) - y^{(i)} \\Big)^2 \\textcolor{gray}{ + 1000 w_e^2} \\textcolor{gray}{ + 1000 w_e^4}\n",
    "$$\n",
    "\n",
    "where the last 2 terms aim to **penalize** model if $w_{3,4}$ are too large.  \n",
    "Usually one does not know which features to penalize and one penalizes them all! \n",
    "\n",
    "$$\n",
    "J(\\vec{w},b) = \\min\\frac{1}{2m}\\sum_{i=1}^m\\Big( f_{\\vec{w},b}(\\vec{x}^{(i)}) - y^{(i)} \\Big)^2 + \\frac{\\lambda}{2m}\\sum_{j=1}^n w_{j}^2 \\textcolor{gray}{+\\frac{\\lambda}{2m}b^2}\n",
    "$$\n",
    "\n",
    "$\\lambda > 0$ is the regularization parameter , and the last term is generally ignored for meing small; \n",
    "- If $\\lambda$ small -- **overfitting** (no regularization)\n",
    "- If $\\lambda$ large -- **underfitting** (only $b$ is left, line fit)\n",
    "\n",
    "# Regularized linear regression\n",
    "\n",
    "The algorithm is the same. The only difference is the cost function derivatives (and we do not regularize $b$)\n",
    "\n",
    "$$\n",
    "w_j = w_j - \\alpha \\frac{\\partial}{\\partial w_j} J(\\vec{w},b) = w_j - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}\\Big( f_{\\vec{w},b}(\\vec{x}^{(i)}) - y^{(i)} \\Big) x_j^{(i)} + \\frac{\\lambda}{m}w_j \\\\\n",
    "b = b - \\alpha \\frac{\\partial}{\\partial b} J(\\vec{w},b) = w_j - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}\\Big( f_{\\vec{w},b}(\\vec{x}^{(i)}) - y^{(i)} \\Big)\n",
    "$$\n",
    "\n",
    "It can be rearranged to get $w_j = w_j + w_j\\Big( 1 - \\alpha\\frac{\\lambda}{m} \\Big)$. This shrinks $w_j$ at every iterations. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regulirized Logistic regression \n",
    "\n",
    "Recall cost function for logistic regression. Modify it to include the regularization: \n",
    "$$\n",
    "J(\\vec{w},b) = -\\frac{1}{m}\\sum_{i=1}^m \\Big[y^{(i)}\\log(f_{\\vec{w},b}(\\vec{x}^{(i)})) - (1 - y^{(i)})\\log(1-f_{\\vec{w},b}(\\vec{x}^{(i)}))\\Big] \\textcolor{gray}{+\\frac{\\lambda}{2m}\\sum_{j=1}^nw_j^2}\n",
    "$$\n",
    "\n",
    "there, the large $w_j$ are penilized.\n",
    "\n",
    "The alorithm for gradient descend looks the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from plt_overfit import overfit_example, output\n",
    "from lab_utils_common import sigmoid\n",
    "np.set_printoptions(precision=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION\n",
    "def compute_cost_linear_reg(X, y, w, b, lambda_ = 1):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "    Returns:\n",
    "      total_cost (scalar):  cost \n",
    "    \"\"\"\n",
    "\n",
    "    m  = X.shape[0]\n",
    "    n  = len(w)\n",
    "    cost = 0.\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(X[i], w) + b  # Linear regression #(n,)(n,)=scalar, see np.dot\n",
    "        cost = cost + (f_wb_i - y[i])**2 # Cost function  #scalar             \n",
    "    cost = cost / (2 * m)  #scalar  \n",
    " \n",
    "    reg_cost = 0. \n",
    "    for j in range(n):\n",
    "        reg_cost += (w[j]**2)  # add regularization term  #scalar\n",
    "    reg_cost = (lambda_/(2*m)) * reg_cost #scalar\n",
    "    \n",
    "    total_cost = cost + reg_cost # combine into regulirzed cost function #scalar\n",
    "    return total_cost   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized cost: 0.07917239320214275\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,6)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "cost_tmp = compute_cost_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "\n",
    "print(\"Regularized cost:\", cost_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGISTIC REGRESSION\n",
    "def compute_cost_logistic_reg(X, y, w, b, lambda_ = 1):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "    Returns:\n",
    "      total_cost (scalar):  cost \n",
    "    \"\"\"\n",
    "\n",
    "    m,n  = X.shape\n",
    "    cost = 0.\n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i], w) + b  # Regression #(n,)(n,)=scalar, see np.dot\n",
    "        f_wb_i = sigmoid(z_i)  # scalar\n",
    "        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)      #scalar\n",
    "             \n",
    "    cost = cost/m                                                      #scalar\n",
    "\n",
    "    reg_cost = 0\n",
    "    for j in range(n):\n",
    "        reg_cost += (w[j]**2)                                          #scalar\n",
    "    reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar\n",
    "    \n",
    "    total_cost = cost + reg_cost                                       #scalar\n",
    "    return total_cost                                                  #scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized cost: 0.6850849138741673\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,6)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "cost_tmp = compute_cost_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "\n",
    "print(\"Regularized cost:\", cost_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADIENT DESCENT FOR LINEAR REGRESSION WITH REGULARIZATION\n",
    "def compute_gradient_linear_reg(X, y, w, b, lambda_): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape           #(number of examples, number of features)\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):                             \n",
    "        err = (np.dot(X[i], w) + b) - y[i]                 \n",
    "        for j in range(n):                         \n",
    "            dj_dw[j] = dj_dw[j] + err * X[i, j]               \n",
    "        dj_db = dj_db + err                        \n",
    "    dj_dw = dj_dw / m                                \n",
    "    dj_db = dj_db / m   \n",
    "    \n",
    "    for j in range(n):\n",
    "        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n",
    "\n",
    "    return (dj_db, dj_dw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db: 0.6648774569425727\n",
      "Regularized dj_dw:\n",
      " [0.29653214748822276, 0.4911679625918033, 0.21645877535865857]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,3)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1])\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "dj_db_tmp, dj_dw_tmp =  compute_gradient_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "\n",
    "print(f\"dj_db: {dj_db_tmp}\", )\n",
    "print(f\"Regularized dj_dw:\\n {dj_dw_tmp.tolist()}\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADIENT DESCEND FOR LOGISTIC REGRESION WITH REGULARIZATION\n",
    "def compute_gradient_logistic_reg(X, y, w, b, lambda_): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    " \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "    Returns\n",
    "      dj_dw (ndarray Shape (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar)            : The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros((n,))                            #(n,)\n",
    "    dj_db = 0.0                                       #scalar\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar\n",
    "        err_i  = f_wb_i  - y[i]                       #scalar\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar\n",
    "        dj_db = dj_db + err_i\n",
    "    dj_dw = dj_dw/m                                   #(n,)\n",
    "    dj_db = dj_db/m                                   #scalar\n",
    "\n",
    "    for j in range(n):\n",
    "        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n",
    "\n",
    "    return (dj_db, dj_dw) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db: 0.341798994972791\n",
      "Regularized dj_dw:\n",
      " [0.17380012933994293, 0.32007507881566943, 0.10776313396851499]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,3)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1])\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "dj_db_tmp, dj_dw_tmp =  compute_gradient_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "\n",
    "print(f\"dj_db: {dj_db_tmp}\", )\n",
    "print(f\"Regularized dj_dw:\\n {dj_dw_tmp.tolist()}\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyBlastAfterglow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
