{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomoly detection\n",
    "\n",
    "Examples: find \"unusual\" features in a dataset (aircraft engine $x_i$, heat output, vibration intensity, etc)\n",
    "Algorithm is trained in $x_{\\rm train}$ most of which are good engines, and evaluates $x_{\\rm test}$ and finds whether it is anomolus with respect to others. \n",
    "In other words, it checks whether new data is within the range of old data.\n",
    "\n",
    "Done via `Density estimation`.  \n",
    "Compute probability of $p(x)$, assess high and low probability of a new data to be within the dataset. If $p(x) < \\epsilon$, the _anomoly_ is detected. \n",
    "\n",
    "Used in:\n",
    "- Fraud detection (assessing features that user does and find _anomolies_ user), e.g., finantial freaud. \n",
    "- Fault detection in production. \n",
    "- Monoitor computers in a data center\n",
    "\n",
    "Data is modelled using `gaussian distribution` (normal distribution). \n",
    "$$\n",
    "p(x) = \\frac{1}{\\sqrt{2}\\pi\\sigma}e^{-(x-\\mu)^2 / 2\\sigma^2}\n",
    "$$\n",
    "\n",
    "Then, given a dataset $x^{(i)}$, the $\\mu$ and $\\sigma$ have to be found to fit the distribution to data. \n",
    "$$\n",
    "\\mu = \\frac{1}{m-1}\\sum_{i=1}^m x^{(i)} \\\\\n",
    "\\sigma^2 = \\frac{1}{m-1}\\sum_{i=1}^m(x^{(i)}-\\mu)^2\n",
    "$$\n",
    "These are **maximum likelihood estimates** of $\\mu$ and $\\sigma$. \n",
    "\n",
    "## Anomoly detection algorithm\n",
    "\n",
    "COnsider training set $\\vec{x}^{(i)}$ with $n$ features. \n",
    "Estimate the $p(\\vec{x})$, where $\\vec{x}$ is the feature vector. Assuming that features are independent, $p(\\vec{x}) = \\Pi_{i=0}^n p(x_i;\\mu_i,\\sigma_1^2)$, \n",
    "where $\\mu_1$ and $\\sigma_1$ are mean and varaince of the feature $1$. For each feature $\\mu_{i}$ and $\\sigma_{i}$ are different\n",
    "\n",
    "Building an algorithm \n",
    "1. Choose features\n",
    "2. Fit parameters, vectorized $\\vec{\\mu}=(1/(m-1))\\sum_{i=1}^m \\vec{x}^{(i)}$ and $\\sigma_i$\n",
    "3. Given a new example $x$, compute $p(x)$ as \n",
    "$$\n",
    "p(\\vec{x}) = \\Pi_{j=1}^n p(x_j;\\mu_j,\\sigma_j^2) =\n",
    "\\Pi_{j=1}^n\\frac{1}{\\sqrt{2}\\pi\\sigma_j}e^{-(x_j-\\mu_j)^2 / 2\\sigma_j^2} \n",
    "$$\n",
    "4. Compare $p(x)$ to a threshold $\\epsilon$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the algorithm\n",
    "\n",
    "It is helpfull to have some of the labelled training data.\n",
    "Assume that all the unlabelled data, $x^{(i)}$ is normal, with $y^{(i)}=0$ (not anlomoly). \n",
    "Extract that $x_{\\rm cv}^{(i)}$ is also given abd has labells $y_{\\rm cv}^{(i)}$ some which is $1$ others $0$. \n",
    "Extract a $x_{\\rm test}^{(i)}$, $y_{\\rm test}^{(i)}$ that also include $y=1$ and $y=0$. \n",
    "\n",
    "Generally, training set should be labelled correctly, but it is _ok_ to have some mislabelled data. \n",
    "The split of the dataset is generally $60+10+10$ percent. Use the cross-validation set to tune $\\epsilon$ so that algorithm correctly identifies $y=1$. \n",
    "Assess algorithm on test set. \n",
    "\n",
    "**Note** usually, only training and cv sets are used as the amount of data is small. \n",
    "But you then cannot evaluate the model performance on a new data. \n",
    "_Risk of overfitting_\n",
    "\n",
    "Evalating the model:  \n",
    "1. Fit mode lto training set\n",
    "2. on cv set compute $p(x)$ \n",
    "$$\n",
    "y = \n",
    "\\begin{cases}\n",
    "1 \\text{ if } p(x) < \\epsilon \\text{ (anomoly) } \\\\ \n",
    "0 \\text{ if } p(x) > \\epsilon \\text{ (normal) }\n",
    "\\end{cases}\n",
    "$$\n",
    "**Note** that dataset is heavily skewed and to evaluate the performace of the model consider  \n",
    "- True/False positives; True/False negatives; \n",
    "- precision/recall; \n",
    "- $F_1$ score\n",
    "\n",
    "#### Supervised learing VS anomoly detection\n",
    "Use _Anomoly detection_ if \n",
    "- Small number of positive examples and Large number of negative examples\n",
    "- If there are many ways (also unknown) to get a postive example (ie, unknown anomolys)\n",
    "- Examples: fraud (new/unique); Manufacturig (for new, unseen defects); Machines in the data centers\n",
    "\n",
    "Use _Supervised learning_\n",
    "- If Large number both positive and negatives are found\n",
    "- Future positives are similar to trained ones.\n",
    "- Spam emails: (generally similar); Manufactoring (finding known, seen problems, e.g., scratched screen); Weather predictions; Desease classification\n",
    "\n",
    "#### Choosing features for anomoly detection\n",
    "\n",
    "Generally, features should be gaussian. Plot the hitogram of a feature and see how close it to the gaussian. If it is not, $x\\rightarrow\\log(x+c)$ or any other mathematical transformation. \n",
    "Error-analysis test: see where the algorithm fails. Asser that $p(x) > \\epsilon$ for normal and $p(x) < \\epsilon$ for anomoly.  \n",
    "Common problem is $p(x) > \\epsilon$ for both cases. Sometimes a _new feature_ is required to make alorithm recognize the anomoly. THis is _feature engineering_. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
