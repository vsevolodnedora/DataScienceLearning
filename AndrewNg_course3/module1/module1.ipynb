{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning\n",
    "\n",
    "Data $\\{x^{(i)}\\}$; no labels.  \n",
    "Algorithms try to find structure in the data; groups of points. \n",
    "\n",
    "**Applications** Used in DNA analysis, astronomy (space objects clustering). \n",
    "\n",
    "\n",
    "## Clustering\n",
    "\n",
    "Clustering looks at data and finds what data belongs together, in one group. \n",
    "\n",
    "### K-MEANS algorithm\n",
    "\n",
    "_Start_ set a random guess of a center between two clusters (draws a line between all data)\n",
    "\n",
    "Main stages of the algorithm:  \n",
    "- Assign points to the cluster centroids\n",
    "- Move cluster centroids (centers of the clusters)\n",
    "- Repeat\n",
    "\n",
    "**In detail**:  \n",
    "- Go through all the points and check if they are closer to dirst or second cluster centroids, and assign them to the closest. \n",
    "- Compute the middle point between all points assigned to each cluster centroids and move the centroids to it\n",
    "- Repeat untill there is no more change (convergance achieved)\n",
    "\n",
    "In this procedure the cluster centroids will continously 'compete' for points getting as close to the center of the cluster as possible. \n",
    "\n",
    "**Algorithm**:\n",
    "- Randomly initialize $K$ cluster centroinds $\\mu_1,\\mu_2,...\\mu_K$,  where $\\mu_i$ has the same dimension as data $\\vec{x}^{1}$ and equal to the $N$ features in the data. \n",
    "- Assign points to cluster centroids \n",
    "- for each point, compute $c\\in{1,...K}$ - index of the cluster centroid closest to $x^{(i)}$. The distance is computed as $L_2$ norm, i.e, $\\min_k||x^{i}-\\mu_{k}||$\n",
    "- More cluster centroids:  \n",
    "for $k=1$ to $K$, compute $\\mu_k:=$average (mean) of points assigned to cluster $k$ on each axis. $\\mu_i = 1/k(\\sum x^{(k)})$ preserving data dimensiality. \n",
    "(If cluster has 0 points, we remove the clsuter)\n",
    "\n",
    "The algorithm can also be applied to not-clearly-separatable clusters\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization objective\n",
    "\n",
    "Consider $c^{i}$ index of a cluster ($1,2,...,K$) to which example $x^{i}$ is currently assigned.  \n",
    "$\\mu_k$ is the cluster centroid $k$  \n",
    "$\\mu_{c^{(i)}}$ cluster centroid of cluster to which example $x^{(i)}$ has been assinged to\n",
    "\n",
    "Cost function \n",
    "$$\n",
    "J(c^{(1)},...,c^{(m)},\\mu_i,...,\\mu_k) = \\frac{1}{m}\\sum_{i=1}^{m} || x^{(i)} - \\mu_{c^{(i)}} || ^ 2\n",
    "$$\n",
    "\n",
    "Minimize the cost function $J$ (distortion cost function?)\n",
    "$$\n",
    "\\min_{c^{(1)},...,c^{(m)}} J(c^{(1)},...,c^{(m)},\\mu_i,...,\\mu_k)\n",
    "$$\n",
    "\n",
    "> My shifting the cluster centorid to the middle between training examples minimizes the squared distance\n",
    "\n",
    "> K-means alorithm optimizes the cost function and thus converges on every single iteration\n",
    "\n",
    "Cost function should always go down or stay the same. If it stops, the algorithm is converged. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initilizing K-means \n",
    "\n",
    "0. Initilizie randomly $K < m$ number of cluster centroids.  \n",
    "For example, do it by selecting $K$ training exampls and setting $\\mu_{i}$ to these $x_{i}$ examples.  \n",
    "**Warning** there exist _local optima_ where K-means has reached the state, but it is not global minima of the cost function. This can be avoided by running the algorithm several times and picking the one realization with the minimum final cost function\n",
    "\n",
    "# Choosing the number of clusters\n",
    "\n",
    "There are no \"right\" number of clusters.  \n",
    "We can compute $J = J(\\text{K of clusters})$. There can be a minimum there, but generally it is monotonic and one can search for 'elbow' of the curve to cut the value. So generally, **there is never a minimum**.  \n",
    "Overall, the number of clusters is given by the _field knowledge_... \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
