{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principle Component Analysis\n",
    "\n",
    "To analyze/plot a dataset with many features. \n",
    "Consider a car with many features. The width of the car is generally uniform for different cars. Similarly, a diameter of the wheel does not vary too much.  \n",
    "Features like length and height, however, are more varied and worth investigation. If they appear to be correlated, we can consider length/hight, or length*hight, a new feature that we can use as a replacement for length and size.  \n",
    "PCA actually finds these _new axis_ along which data changes are more meaningful.  \n",
    "> PCA is used to reduce the number of features.\n",
    "\n",
    "Consider a dataset with $2$ features, $x_1$ and $x_2$.  \n",
    "1. Preprocessing. Normalization so the features have **zero mean**. Also **feature scaling** has to be applied.\n",
    "\n",
    "> PCA finds the **principle component**, an axis along which the data cahnges the most (axis that passes through datadapoints, ortogonal to the SVM) \n",
    "\n",
    "> Second principle component is ortogonal to the first principle component axis\n",
    "\n",
    "> PCA is not linear regression. The regression tries to minimize the the distance between the **line and label** (parallel to the $y$ axis). PCA tries to minimize the distance between the **line and the data** (ortogonal to the line itself)\n",
    "\n",
    "> **Reconstruction step of PCA** from new axis to the original ones is only approximate \n",
    "\n",
    "## Implementing the PCA\n",
    "\n",
    "1. **Preprocessing**: Perform feature scaling \n",
    "2. Run the algirithm to fit the data to get $2$ or $3$ axis for visualization (the fit function in $\\texttt{\\text{scikitlearn.fit()}}$) automatically does **mean normalization**! This gives new axis $z_{1...n}$ called **principle components**\n",
    "3. Assess how much _variance_ is explained by each principle component via $\\texttt{\\text{explained\\_variance\\_ratio()}}$\n",
    "4. Project data onto new axis via $\\texttt{\\text{transform()}}$) \n",
    "\n",
    "> Each PC axis explains a \\% of the variance. The sum of this should be 1\n",
    "\n",
    "#### Applications of PCA:\n",
    "- Data compression (not so much recently)\n",
    "- Speed-up training of a supervised learning model (again, not so much anymore)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
