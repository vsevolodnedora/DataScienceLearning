{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making recommendations\n",
    "\n",
    "### Predicting movie ratings \n",
    "\n",
    "Given a list of movies, $N$ users rated movies from 1-5. Set $r(i,j)=1$ if a user $i$ rated movie $j$. Then, $y^{i,j}\\in\\{0,1\\}$ is the rating given by the user $j$ to the movie $i$. \n",
    "\n",
    "## Collaborative filtrering\n",
    "\n",
    "Consider a table where columns besides last two are number of stars given  \n",
    "Last two columns indicate the movie content, action or romanse   \n",
    "Some users did not give stars/watch some movies.  \n",
    "We need to predict which movie to recommend to what user  \n",
    "|        | User1 | User2 | ... | User N | $x_1$ (romance) | $x_2$ (action)\n",
    "| ------ | ----- |----- |----- |----- |----- | ----- |\n",
    "| Movie 1 |  5 | ? | ? | 0 | 1.0 | 0.01  \n",
    "| Movie 2 |\n",
    "| ... |\n",
    "| Movie 3 |\n",
    "\n",
    "Assume that predicted rating is $w\\cdot x^{i} + b$ as a linear regression \n",
    "For $j$ user and movie $i$ the formula is \n",
    "$$\n",
    "w^{(j)}\\cdot x^{(i)} + b^{(j)}\n",
    "$$\n",
    "$w^{(i)},b^{(i)}$ are parameters for user $j$  \n",
    "$x^{(i)}$ is the feature vector for movie $i$  \n",
    "and $m_j$ is the number of movies rated by user $j$.  \n",
    "Goal: learn $w^{(j)}$ and $b^{(j)}$\n",
    "\n",
    "$$\n",
    "J(w^{(j)},b^{(j)}) = \\frac{1}{2m^{(j)}} \\sum_{i:r(i,j) = 1}(w^{(j)}\\cdot x^{(i)} + b^{(j)} - y^{(i,j)})^2 + \\frac{\\lambda}{2m^{(j)}}\\sum_{k=1}^{n}\\Big( w_{k}^{(j)} \\Big)^2\n",
    "$$\n",
    "where the summation is done over the movies that user $j$ actually rated, $i:r(i,j)=1$  \n",
    "Similar to the cost function of the linear regression plus regularization to prevent overfitting.  \n",
    "$\\min J(w^{(j)},b^{(j)})$ gives the values for the parameters.  \n",
    "\n",
    "For all users we need to sum over all users as \n",
    "\n",
    "$$\n",
    "J = \\frac{1}{2} \\sum_{j=1}^{n_u} \\sum_{i:r(i,j) = 1}(w^{(j)}\\cdot x^{(i)} + b^{(j)} - y^{(i,j)})^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{n_u}\\sum_{k=1}^{n}\\Big( w_{k}^{(j)} \\Big)^2\n",
    "$$\n",
    "\n",
    "Note, we got rid of the $1/m^{(j)}$. For.. simplicity?..  \n",
    "\n",
    "Now, use optimization algorithm to find the values.  \n",
    "\n",
    "\n",
    "#### Find fratures $x_i$ if they are not available\n",
    "\n",
    "Assume that we already have $w^{(j)}$ and $b^{(j)}$, then as we know the values for all users, labels $\\mathbf{y}^{(i)}$, we can compute $\\mathbf{w}^{(j)}\\cdot \\mathbf{x}^{(j)} + \\mathbf{b}^{(j)} = \\mathbf{y}^{(j)}$. I.e., we have linear system of equations that we can solve for $\\mathbf{x}^{(j)}$ (and assume that $\\mathbf{b}^{(j)}=0$). So we get a system of linear equations. Solving it requires inverting a $N\\times M$ matrix. Solving it requires many users. \n",
    "\n",
    "Cost function for learining the feature $x^{(i)}$\n",
    "\n",
    "$$\n",
    "J(x^{(i)}) = \\frac{1}{2} \\sum_{j:r(i,j) = 1}(w^{(j)}\\cdot x^{(i)} + b^{(j)} - y^{(i,j)})^2 + \\frac{\\lambda}{2}\\sum_{j=k}^{n} \\Big( x_{k}^{(j)} \\Big)^2\n",
    "$$\n",
    "\n",
    "For all features:\n",
    "\n",
    "$$\n",
    "J(x^{(1)},...,x^{(n_m)}) = \\frac{1}{2} \\sum_{i=1}^{n_m} \\sum_{j:r(i,j) = 1}(w^{(j)}\\cdot x^{(i)} + b^{(j)} - y^{(i,j)})^2 + \\frac{\\lambda}{2}\\sum_{i=1}^{n_m}\\sum_{j=k}^{n} \\Big( x_{k}^{(j)} \\Big)^2\n",
    "$$\n",
    "\n",
    "> In This algorithm the features can be **learned** \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Collaborative filtering algorithm combines the algorithm for learning model parameters, $\\vec{w}^{(i)}$, $\\vec{b}^{(i)}$ with the algorithm for learning fearutes $\\vec{x}^{(i)}$\n",
    "\n",
    "The combined cost function got learing $w^{(j)}$ and $b^{(j)}$ and $x^{(i)}$  \n",
    "\n",
    "$$\n",
    "J(x^{(1)},...,x^{(n_m)}) = \\frac{1}{2}\\sum_{(i,j):r(i,j) = 1}(w^{(j)}\\cdot x^{(i)} + b^{(j)} - y^{(i,j)})^2 + \\frac{\\lambda}{2}\\sum_{i=1}^{n_m}\\sum_{j=k}^{n} \\Big( w_{k}^{(j)} \\Big)^2 + \\frac{\\lambda}{2}\\sum_{i=1}^{n_m}\\sum_{j=k}^{n} \\Big( x_{k}^{(j)} \\Big)^2\n",
    "$$\n",
    "\n",
    "It is possible to minimize this cost function. \n",
    "\n",
    "The gradient descent algorithm then:\n",
    "\n",
    "$$\n",
    "w_i^{(j)} = w_{i}^{(j)} - \\alpha \\frac{\\partial}{\\partial w_i^{(j)}}J(w,b,x) \\\\\n",
    "b_i^{(j)} = b_{i}^{(j)} - \\alpha \\frac{\\partial}{\\partial b_i^{(j)}}J(w,b,x) \\\\\n",
    "x_i^{(j)} = x_{i}^{(j)} - \\alpha \\frac{\\partial}{\\partial x_i^{(j)}}J(w,b,x)\n",
    "$$\n",
    "\n",
    "**Note** $x$ is also a parameter now. \n",
    "\n",
    "> This is collaborative filtering algorithm\n",
    "\n",
    "Multimple users rated the same movie, allowing you to chose right parameters for this movie.\n",
    "\n",
    "\n",
    "### Binary labels for collaborative filtering\n",
    "\n",
    "User gives values $0$ or $1$. \n",
    "\n",
    "Examples:\n",
    "1. Did user $j$ purcased an item after seeing it?\n",
    "2. Did user $j$ fav/like an time?\n",
    "3. Did user $j$ spend at least 30sec with the item?\n",
    "4. Did user $j$ click an time?\n",
    "\n",
    "Meaning:\n",
    "- $1$ - engajed after being shown\n",
    "- $0$ - did not engage\n",
    "- $?$ - Unknown yet\n",
    "\n",
    "For binary labels predict probability $\\mathbf{y}^{(i,j)}=1$, and use logisitc function, aka, logisitc regression \n",
    "$g(z) = 1/(1+e^{-z})$ with $z = \\mathbf{w}^{(j)}\\cdot \\mathbf{x}^{(j)} + \\mathbf{b}^{(j)}$.  \n",
    "Modify the cost function as following. \n",
    "\n",
    "Loss for binary labels: $y^{(i,j)}:$ $f_{w,b,x}(x) = g(\\mathbf{w}^{(j)}\\cdot \\mathbf{x}^{(j)} + \\mathbf{b}^{(j)})$\n",
    "\n",
    "$$\n",
    "L(f_{(w,b,x)}(x),y^{(i,j)}) = -y^{(i,j)}\\log\\Big( f_{(w,b,x)}(x) \\Big) - \\big( 1-y^{(i,j)} \\Big) \\log\\Big( 1-f_{(w,b,x)}(x) \\Big)\n",
    "$$\n",
    "\n",
    "and the cost function is then \n",
    "\n",
    "$$\n",
    "J(w,b,x) = \\sum_{(i,j):r(i,j)=1}L(f_{(w,b,x)},y^{(i,j)})\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Faster algorithm : mean normalization\n",
    "\n",
    "Noramlize the labels to have consistent values. \n",
    "\n",
    "This helps to start values for a new user to be close to the middle of the range rather than just  $0$. \n",
    "Consdier each row of the matrix of movie ratings, and compute mean for each tow, then substreact this means from the actial moving rating matrix. \n",
    "\n",
    "Then, the ratings would go from $-a$ to $+a$, where $a$ is some value. Then, if a new user is initilze with $0$, it is closer to the mean value for this movie, so maybe it is closer to the truth... \n",
    "\n",
    "This makes the algorithm faster.  \n",
    "Normalize each row of the matrix to the mean.    \n",
    "It is also possible to normalize each columns (which is good for a brand new movie)\n",
    "\n",
    "### Tensorflow implementation\n",
    "\n",
    "Tensorflow can be used to construct other models that use the gradient descent. \n",
    "Add the cost functiion, J, $J = (wx - 1)^2$\n",
    "and use the $\\texttt{\\text{tf.GradientTape()}}$ \n",
    "to compute derivative (this is `Auto Diff` process) \n",
    "\n",
    "(see assignemnt code...)\n",
    "\n",
    "\n",
    "### Finding related items\n",
    "\n",
    "Using collaborative filtering.  \n",
    "Feature $x^{(i)}$ of item $i$ are hard to interprete.  \n",
    "Find other intem $k$ with $x^{(k)}$ similar to $x^{(i)}$.  \n",
    "Then, the distance between features $\\sum_{l=1}^n(x_l^{(k)}-x_{l}^{(i)})^2 = ||x^{(k)}-x^{(i)}||^2$. \n",
    "Multiple items with related features are similar items (close).\n",
    "\n",
    "Limitaitations of the ollaborative filtering. \n",
    "- Difficult as a `cold-start` problem. Requires pre-labeled datasets with many users, many labeld\n",
    "- Use side information about the items\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.sum((np.array([2,3,4])-np.array([5,3,4]))**2))\n",
    "print(np.sum((np.array([2,3,4])-np.array([2,2,1]))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyBlastAfterglow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
