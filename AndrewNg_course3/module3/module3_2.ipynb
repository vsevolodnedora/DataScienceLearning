{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continous variables\n",
    "\n",
    "If the number of states is continous, $x\\in[0,x_m]$, position. \n",
    "\n",
    "For a truck, there are positions $x,y$ angles $\\theta$, and velocities $\\dot{x}$. Then the state vecotr is \n",
    "$$\n",
    "s = \n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "\\theta \\\\\n",
    "\\dot{x} \\\\\n",
    "\\dot{y} \\\\\n",
    "\\dot{\\theta} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "the values for which are contonous. For a helicoptcer, there is also $z$ psoiton, role, pitch, yaw (for orientation). Also correposinding rate of changes. These comprise the state vector \n",
    "$$\n",
    "s = \n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "z \\\\\n",
    "\\phi \\\\\n",
    "\\theta \\\\\n",
    "\\omega \\\\\n",
    "\\dot{x} \\\\\n",
    "\\dot{y} \\\\\n",
    "\\dot{z} \\\\\n",
    "\\dot{\\phi} \\\\\n",
    "\\dot{\\theta} \\\\\n",
    "\\dot{\\omega}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "This are states for _continous state Markov decision process_.  \n",
    "\n",
    "#### Lunar landing application\n",
    "\n",
    "COntrol thrusters. The actions are \n",
    "- do nothing (fall under the force of gravity)\n",
    "- left thruster\n",
    "- main thruster\n",
    "- right thrister\n",
    "\n",
    "The state vector here resds,\n",
    "$$\n",
    "s = \n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "\\dot{x} \\\\\n",
    "\\dot{y} \\\\\n",
    "\\theta \\\\\n",
    "\\dot{\\theta} \\\\\n",
    "\\dot{l} \\\\\n",
    "\\dot{r} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where $\\dot{\\theta}$ is the angular velocity, $l$ is the position of the left leg on the ground, $r$ is the position of the right leg on the ground (these two are binary, 1, or 0). \n",
    "\n",
    "\n",
    "Reward parameters:  \n",
    "- The reword function for laning is $100$ $140$. \n",
    "- Additional reward for moving toward/away from pad.  \n",
    "- Crash: $-100$\n",
    "- Soft landing $+100$  \n",
    "- Leg grounded $+10$\n",
    "- Fire main engine $-0.3$\n",
    "- Fire side thrusters $-0.03$\n",
    "\n",
    "> Goal is to learn the policy $\\pi$, that given the state vector $s$, pciks the action $a=\\pi(s)$ so as to maximize the return. \n",
    "\n",
    "Train the neuroal to compute/approximate the state action value function $Q$ of $s$. This will result in a choice of a good action $a$. \n",
    "\n",
    "We consider a NN, that takes the current state, the current action and computes/approximates $Q(s)$. So the input to NN is the state vector $s$ and the action $a$, encoded with _one-hot_ method $[1,0,0,0]$ for nothing, $[0,1,0,0]$ for left, $[0,0,1,0]$ for right, $[0,0,0,1]$ for main and $a = [0,0,0,0,1]$ for left. So we column-stack the $s$ and $a$. This is input $\\vec{x}=[s,a]^T$, the output is $y=Q(s,a)$. \n",
    "\n",
    "Contrary to the supervised learning, we input here a paer $\\vec{x},y$ and try to get $Q(s,a)$. \n",
    "The method is to use _Belman Equation_ to create lots of learning exampls.  Then, use supervise learning as usual, to earn a mapping from a state-axtion pair $s,a$ to the $Q(s,a)$.  \n",
    "\n",
    "Computing the training set using Bellman equation:\n",
    "$$\n",
    "Q(x=\\{s,a\\}) = y; \\text{ where } y= R(s) + \\gamma \\times \\max_{a'}(Q(s',a'))\n",
    "$$\n",
    "We need to learn function \n",
    "$f_{w,b}(x)=y$\n",
    "so we need a lot of training examples. \n",
    "This is done by randomly choosing actions, and computing a list of tuples \n",
    "$(s,a,R(s),s')$.  \n",
    "\n",
    "Consider $\\Big(s^{(1)},a^{(1)},R(s^{(1)}),s^{'(1)}\\Big)$. \n",
    "\n",
    "To make the tuple we fill the table as $x^{(i)} = f(s^{(i)},a^{(i)})$ from the Bellman equatins, we get $y^{(i)}=f(R(s^{(i)}),s^{'(i)})$\n",
    "\n",
    "| x | y |\n",
    "| --- | --- |\n",
    "| (s^{(i)},a^{(i)}) | y^{(i)}|\n",
    "\n",
    "The $Q$ function is unknown but can be computed from random guess. \n",
    "\n",
    "| x | y |\n",
    "| --- | --- |\n",
    "| $x^{(i)} = (s^{(i)}, a^{(i)})$ | $y^{(i)} = R(s^{(i)}) + \\gamma\\times\\max_{a'}Q(s^{'(i)},a')$ |\n",
    "| $x^{(i+1)} = (s^{(i+1)}, a^{(i+1)})$ | $y^{(i+1)} = R(s^{(i+1)}) + \\gamma\\times\\max_{a'}Q(s^{'(i+1)},a')$ |\n",
    "\n",
    "This gives a training set can is used to train NN. \n",
    "\n",
    "#### Full algorithm (DQN algorithm)\n",
    "1. Start with a NN with random guesses for $Q(s,a)$\n",
    "2. Take a random action and get $\\Big(s^{(1)},a^{(1)},R(s^{(1)}),s^{'(1)}\\Big)$. \n",
    "3. Store $N=10000$ most recent examples, which are called _Replay Buffer_ \n",
    "4. Train the NN using the $\\{x,y\\}$ pairs where $x=(s,a)$ and $y=(R(s)+\\gamma\\times\\max_{a'}Q(s',a'))$. Where the last $10^4$ are stored. Note, we start with random $Q()$ function. The result of the NN is $Q_{\\rm new} \\approx y$. Then, update $Q=Q_{\\rm new}$. \n",
    "\n",
    "#### Efficient arcitecture for DQN \n",
    "\n",
    "The original NN is, given 12 numbers for numbers and each action (nothing, left, main, right is one-hot encoded in the vector). \n",
    "\n",
    "$\\vec{x} = [s,a]^T$ $\\rightarrow$ $64$ $\\rightarrow$ $64$ $\\rightarrow$ $1$ $\\rightarrow$ $Q(s,a)$.  \n",
    "\n",
    "This is inefficinet as the NN has to trained each time for each action separately.  \n",
    "A more eficient NN is with input of $8$ number and $4$ unit output for each action:\n",
    "\n",
    "$\\vec{x} = [s,a]^T$ $\\rightarrow$ $64$ $\\rightarrow$ $64$ $\\rightarrow$ $4$ $\\rightarrow$ $[Q(s,\\text{nothing}),Q(s,\\text{left}),Q(s,\\text{main}),Q(s,\\text{right})]^T$.  \n",
    "\n",
    "Now, only one inference is required. Also, now calculation of $R(s)+\\gamma\\times\\max_{a'}Q(s',a')$ is more efficinet, as it is computed for all actions at the same time. \n",
    "\n",
    "#### Algorithm refinement: $\\epsilon$-greedy policy\n",
    "\n",
    "In learning how to approximate $Q(s,a)$, actions $a$ still have to be taken. It is non-trivial to find, how to chouse these actions. This is done with **$\\epsilon$-greedy algorithm**. There are two options to do so.  \n",
    "\n",
    "Option 1: \n",
    "- Pick an action $a$ that maximsizes current $Q(s,a)$\n",
    "\n",
    "Option 2: **$\\epsilon$-greedy apolicy** ($\\epsilon=0.05$):\n",
    "- With probability $0.95$ pick the action that maximises $Q(s,a)$ _Exploitation step_.\n",
    "- With probability $0.05$ pick the action randomly. _Exploration step_.\n",
    "\n",
    "Randomness is helpfull, when the initial guess is heavily biased towards one specific action and the algorithm gets stuck with a subset of actions. In other workds it helps overcome \"reconceptions\". \n",
    "\n",
    "It is common to start with high $\\epsilon$ and reduce it during the learning. $\\epsilon: 1.\\rightarrow 0.01$. \n",
    "\n",
    "The algorithms in reinforcment learning are very picky in terms of the choice of the hyperparameters. One has to be careful. \n",
    "\n",
    "#### Refinments: Mini-batches\n",
    "\n",
    "Mini-batches is used in reinformcnet and supervised learning. \n",
    "\n",
    "- In supervised learning it is like the following:\n",
    "\n",
    "Mini-batches helps when the size of the dataset is very large. In standard gradient descend the updates need to be run over the enire dataset (recall $\\sum_{i=1}^m(f_{(w,b)}(x^{(i)})-y^{(i)}))^2$ term). \n",
    "In mini-batches, only a subset of examples is considered as  $\\sum_{i=1}^{n_i}(f_{(w,b)}(x^{(i)})-y^{(i)}))^2$ where $n_i$ is the size of the subset or a batch. \n",
    "\n",
    "The \"full-batch\" gradient descend goes to the minimum of the function reliably. nini-batch algorithm, however, because of downsampled data, goes to the minimum _noisily_ and _chaotically_, but less computationally expesive. \n",
    "\n",
    "- In reinforcment learning it is like the following: \n",
    "\n",
    "The reply buffer is split in subsets (sub-buffers) and use it to train the NN. This speeds-up the training. \n",
    "\n",
    "#### Refinment: Soft-update\n",
    "\n",
    "Recall, that when we train the NN in reinforment learning, we start with a random guess for the $Q$ function and update it to the $Q_{\\rm new}$ after each training run. However, this update can lead to a drastic change in the algorithm and it is not neceseraly an inprovement. E.g., unlucky steps are possible. Then, consider \n",
    "\n",
    "$Q=Q_{\\rm new}$, e.g., $w\\rightarrow w_{\\rm new}$ and $b\\rightarrow b_{\\rm new}$. \n",
    "\n",
    "In the soft-update method, we keep old and new model parameters as following: \n",
    "\n",
    "$w = 0.01 w_{\\rm new} + 0.99 w$ and $b = 0.01 b_{\\rm new} + 0.99 b$. With $0.99$ is a new hyperparameter, that controls how soft is the update. \n",
    "\n",
    "Such algorithm converges more reliably. \n",
    "\n",
    "#### State of the reinforment learning\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
