{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcment learning\n",
    "\n",
    "Imagine a heicopter with a lot of sensors. A program is required to control it. Reinformcent learning may allow given the **required** position of the helicopter, find the right movements of the control sticks. \n",
    "\n",
    "Using a _supervised learning_ it is possible to map:  \n",
    "state $s$ $\\rightarrow$ action $a$  \n",
    "$x$ $\\rightarrow$ $y$\n",
    "\n",
    "But when the helicopter is flying through the air it is difficult to have a one-to-one mapping due to randomness of the environment. Here the **supervised learning fails**.\n",
    "\n",
    "Reinforrcment learning allows to achive a solution using `reward function` that 'reward' the algorithm when solution is correct. \n",
    "E.g., if flying well $+1$ if crashes $-1000$ \n",
    "\n",
    "#### Applications: \n",
    "- Controlling robots, \n",
    "- Factory optimizations\n",
    "- Finanzial (stock) trading\n",
    "- Playing games (video games)\n",
    "\n",
    "\n",
    "### Applications for the Mars rover\n",
    "\n",
    "Consider a rover with 6 possible states (descrete variables). State $1$ shows the most preferred location type for the rover to study, while state $6$ is less preferred. Set rewares $100$ for state $1$ and $40$ for state $6$. If rover starts at state $4$ where reward is $0$, it randomly tries a set of states, untill __terminal state__ is reached which is state $1$ or state $6$ and the trail stops for the day.  \n",
    "\n",
    "There are many ways the robot can get to $1$ or $6$, by randomly picking a substate to go into and maybe returning back and 'curcling around'. \n",
    "\n",
    "#### Return in reinforcment learning\n",
    "\n",
    "Differnece between 5 bucks in 5 min or 30 buks in 30 minutes. How to assign reward? \n",
    "Reward is a sum of rewards for each step multiplies by the discount factor. \n",
    "$$\n",
    "\\text{Return} = R_1\\gamma + R_2\\gamma^2 ... + R_n\\gamma^n\n",
    "$$\n",
    "The $\\gamma$ makes the algorithm _impatient_. \n",
    "Usually, $\\gamma\\sim1$ but here we set $\\gamma=0.5$. \n",
    "\n",
    "In finantial applications the $\\gamma$ is like an interest rate, or a time value of money. \n",
    "\n",
    "Return = function of rewards, and in turn, a function of steps.  \n",
    "\n",
    "So the return depends on the trajectory, te set of substeps that the algorithm takes. \n",
    "\n",
    "One has to try all initial states to learn which trajectory is the most optimal, with the highest retern. \n",
    "\n",
    "Rewards can also be negative. \n",
    "\n",
    "\n",
    "#### Policy in reinforcment learning\n",
    "\n",
    "State $\\rightarrow$ policy $\\rightarrow$ action.  \n",
    "$s \\rightarrow a$.\n",
    "\n",
    "The `goal` of reinforcment learning is to find a policy $\\pi$, that teks is what action, $a=\\pi(s)$ to take in every state $s$, so as to maximize the return. \n",
    "\n",
    "|               | Marse Rover | helicopter | chess |\n",
    "| ---           | --- | --- | --- |\n",
    "| states        | 6 states | position | pieces on the board |\n",
    "| actions       | $\\rightarrow$, $\\leftarrow$ | stick movement | possible moves |\n",
    "| rewards       | 100,0,40 | +1, -100 | +1, 0, -1 |\n",
    "| discount factor $\\gamma$  | 0.5 | 0.99 | 0.999 |\n",
    "| return        | $R=\\sum_i \\gamma^i R_i$  | $R=\\sum_i \\gamma^i R_i$ |  $R=\\sum_i \\gamma^i R_i$ |\n",
    "| policy $\\pi$  |   | Find $\\pi(s)=a$  | Find $\\pi(s)=a$ |\n",
    "\n",
    "**Markov Decision Process (MDP)** the formalism that the future depends only on the current state (does not depend on the past events). \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State action value function\n",
    "\n",
    "This is a key function in reinforcment learning. It is also called $Q$ function, $Q^*$ function, or a optimal $Q$ function\n",
    "It reads \n",
    "$\n",
    "Q(s,a) = \\text{Return}\n",
    "$\n",
    "if \n",
    "- start in state $s$\n",
    "- take action $a$ (once)\n",
    "- then behave _optimally_ after that\n",
    "\n",
    "\n",
    "#### Example: Rover with states \n",
    "| States | 1 | 2 | 3 | 4 | 5 | 6 |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| Rewards | 100 | 0 | 0 | 0 | 0 | 40 |\n",
    "\n",
    "Example: $Q(2,\\rightarrow) = 12.5$ returns are $0$, $0$, $100$ with rewards $0.5^1$, $0.5^2$, $0.5^3$  \n",
    "For $Q(2,\\leftarrow) = 50$ the return is the return is $0+0.5*100$  \n",
    "For $Q(4,\\leftarrow) = 1$ as $0+0.5\\times 0 + (0.5)^2\\times 0 + (0.5)^3\\times 100$\n",
    "\n",
    "Computing this for every state and ever possible initial action $a$, we get \n",
    "\n",
    "| States | 1 | 2 | 3 | 4 | 5 | 6 |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| Rewards | 100 | 0 | 0 | 0 | 0 | 40 |\n",
    "| Returns | 100, 100 | 50, 12.5 | 25, 6.25 | 12.5, 10| 6.25, 20 | 40, 40 |\n",
    "\n",
    "The $Q$ function says how good are the state $s$ and $a$ if you behave optimally after that. \n",
    "\n",
    "> The best possible return from state $s$ is $\\max_a(Q(s,a))$\n",
    "\n",
    "> Tne best possible action in state $s$ is the action $a$ that gives $\\max_a(Q(s,a))$\n",
    "\n",
    "#### State-action value function example\n",
    "\n",
    "N/A\n",
    "\n",
    "#### Belman Equation\n",
    "\n",
    "Allows to compute the state-action value function $Q(s,a)$, where $s$ is the current state, $R(s)$ is the reward of a current state; $a$ is the current action, and $s'$ is the state you get after action $a$, and in the new state, the new action is labelled as $a'$. \n",
    "\n",
    "$$\n",
    "Q(s,a) = \\underbrace{R(s)}_{\\text{Immedeate reward}} + \\gamma \\times \\underbrace{ \\max_{a'}Q(s',a') }_{\\text{Return from behaving optimally from state }s'}\n",
    "$$\n",
    "\n",
    "For the previous rover example,  \n",
    "$Q(2,\\rightarrow) = R(s=2) + 0.5 \\max(Q(s'=3,a')) = 0 + 0.5 \\times 25 = 12.5$.  \n",
    "$Q(4,\\rightarrow) = R(s=4) + 0.5\\max(Q(s'=3,a')) = 0 + 0.5 \\times 25 = 12.5$\n",
    "\n",
    "The sequnce of rewards with discount function can be brockein into immedeate return $R_1$ and discounted return from optimal behaviour. \n",
    "\n",
    "#### Random (stocahstic) environment \n",
    "\n",
    "If there is a percantage of the actition to be not optimal.  \n",
    "Than the actual sequince of states is random.  \n",
    "Here we _maximize_ the a_average_ of the discounted returns.  \n",
    "\n",
    "Previously the retun was a sum of discounted rewards, and we tried to maximize it. \n",
    "For a stocahstic problem there is no single sequence of rewards, here we try to maximize the _average value_ of the sum of discounted rewards. The **Expected return** is the average of all returns for all possible trajectories. \n",
    "$$\n",
    "\\text{Expected Return} = \\text{average}(R_1 + \\sum_{i=2}^{n} \\gamma^i R_i) = \\mathbf{E}[R_1 + \\sum_{i=2}^{n} \\gamma^i R_i]\n",
    "$$\n",
    "\n",
    "This is _stocahstic Markov decision process_. \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
