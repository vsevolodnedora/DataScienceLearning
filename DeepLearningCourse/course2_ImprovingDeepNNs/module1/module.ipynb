{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / Dev / test sets\n",
    "\n",
    "> Building a NN is an iterative process\n",
    "\n",
    "Stractured data is used for Ads, speach, sequirity, logistics.  \n",
    "Thus coming from NLP, or vision, it is not strigtforward to go to analyzing stractured data. \n",
    "\n",
    "Data us usually split into \n",
    "- Test set \n",
    "- cross-validation set\n",
    "- test set\n",
    "\n",
    "Common rule is 70/30 or 60,20,10 for small data sets.  \n",
    "In case of huge datasets: just a few percent for test sets are sufficient.  \n",
    "\n",
    "> data from test and cv sets should be coming from **the same distribution**. \n",
    "\n",
    "Sometimes only test and cv sets are needed. So overall, generally, 2 datasets are the bare minimum needed. \n",
    "\n",
    "#### Bias vs Variance (underfitting vs overfitting)\n",
    "Consider Train set error and dev set error. (assume that test error __should__ be very low i.e., base error is low)\n",
    "- If they are $1$ and $11$, this is **high variance**. \n",
    "- If they are $11$ and $16$, this is **high bias**. \n",
    "- If they are $15$ and $30$, this is **high bias & varaince**. \n",
    "- If they are $.5$ and $1$, this is **low bias & varaince**. \n",
    "\n",
    "#### Improving high bias / varaince problem\n",
    "\n",
    "Check the bias (looking for training set).  If bias is high, try other network configurations, ($L$, $\\alpha$, ..., architecture)  \n",
    "**Note** Getting more data here usually _does not_ help\n",
    "\n",
    "After bias is solved, the high variance problem can be solved by \n",
    "(e.g., getting more data, regulariation, more appropriate NN achitecture). \n",
    "\n",
    "> Bias/variance tradeoff. Old proplem. Can be solved with Bigger NN (reduces bias) and getting more data (reduces variance)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recularzation\n",
    "\n",
    "> Regularization prevents overfitting.\n",
    "\n",
    "In, for example, logisitc regression, we are trying to minimze the cost function. In **Regularization** we add the regularization term to the cost\n",
    "$$\n",
    "J(w,b) += \\frac{\\lambda}{2m}b^2\n",
    "$$\n",
    "where the notm of $w^2$, the **$L_2$ regularization** term reads\n",
    "$$\n",
    "||w^{[l]}||^2 = \\sum_{i=1}^{n^{[l]}}\\sum_{j=1}^{n^{[l-1]}}( w_{i,j}^{[l]} )^2\n",
    "$$\n",
    "One can also consider $L_1$ regularization term $\\lambda/m ||w||_1$\n",
    "Here $w$ is sparse (has too many zeros), and can help with **compressing** the model. \n",
    "\n",
    "> $\\lambda$, the regularization term, is set using CV set.\n",
    "\n",
    "In NN, the regularization is added as ter to the cost function where one has to loop over all training examples and units\n",
    "$$\n",
    "||w^{[l]}||^2 = \\sum_{i=1}^{n^{[l]}}\\sum_{j=1}^{n^{[l-1]}}( w_{i,j}^{[l]} )^2\n",
    "$$\n",
    "> THis is so-called **Frobinious norm** of the matrix\n",
    "\n",
    "In the past $dw$ was computed using backprop. Now the update is:\n",
    "$$\n",
    "dw^{[l]} = (\\text{from backprop}) + \\frac{\\lambda}{m}w^{[l]};\\,\\, \\frac{\\partial J}{\\partial w^{[l]}} = dw^{[l]}\n",
    "$$\n",
    "> Sometimes $L2$ regularization is called ''weight decay''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why regularization reduces overfitting\n",
    "\n",
    "Regularization term penalizes large wight terms. Thus it reduces the size of the NN (zero out effecto of many hidden units). This reduces the stregth of overfitting. When $\\lambda$ is too large, many hidden units effect $\\rightarrow 0$. \n",
    "For example, for $\\tanh()$ becomes linear for small $w$ and the NN becomes linear at large $\\alpha$.\n",
    "\n",
    "### Dropout regularization\n",
    "\n",
    "> Drop-out reduces overftting\n",
    "\n",
    "At each training pass, a random set of neurons in layers are turned off. There exist many implemnetations. Consider \"Inverted dropout\".\n",
    "For 3rd layer it reads \n",
    "\n",
    "$$\n",
    "\\texttt{\\text{d3 = np.random.rand(a3.shapep[0],a3.shapep[1])}} \\\\\n",
    "a3 = np.multiply(a3,d3) \\\\\n",
    "a3 =/ 0.8 = \\text{keep probability}\n",
    "$$\n",
    "\n",
    "Note that because we shut off some units, we need to reduce the layer output by the \"keep probability\" factor. \n",
    "This assures that the expected value remains the same. \n",
    "\n",
    "At test time, no drop-out used. \n",
    "\n",
    "### Understanding Drop-out\n",
    "\n",
    "This is th idea to randomly elliminate the inputs for a given layer. This makes NN less reliant on any particular input. This reduces the size of weights realated to thes einputs and overall shrinks the NN. \n",
    "\n",
    "> Drop-out has a similar effect to $L2$ regularization.  \n",
    "\n",
    "As each layer has diffeent size, for small layers, keep-prop should be small, as too many units can be reduced. For input layer keep-prop is usually 100\\%. Overall, this is different  \n",
    "\n",
    "> Cost function $J$ is no longr _well defined_ when drop-our is used\n",
    "\n",
    "### Other regularization methods:\n",
    "\n",
    "- Data augmentation (creating 'new' data by flipping/ropping/resizing/filtering old data)\n",
    "- Eary stopping (step after a smaller number of iteration utill cost fuction starts rising) This prevents overfitting.\n",
    "\n",
    "> **Ortogonalization**  When training a NN, we otimize for cost function $J$. Then, there is a separate task of not overfitting, _reducing variance_. \n",
    "\n",
    "Note, that early stopping mixes the decreaing $J$ and reducing variacne. \n",
    "\n",
    "## Normalazing inputs for faster training\n",
    "\n",
    "Normalization:\n",
    "1. Substract the mean $\\mu=\\frac{1}{m}\\sum_{i=1}^m x^{(i)}$: $x:=x-\\mu$; \n",
    "2. Normalize variance $\\sigma^2 = \\frac{1}{m}\\sum_{i=1}^m (x^{(i)}**2)$, where $**$ is element wise $x^2$; $x /= \\sigma$\n",
    "\n",
    "This makes variance for all inputs $x$ t be $\\sigma=1$. \n",
    "\n",
    "**Note** test and train sets are normalized with the same $\\mu$ and $\\sigma$. \n",
    "\n",
    "**Normalization** helps speeding up the gradient descend, and it allows to set larger learning rate. \n",
    "\n",
    "### Vanishing / Exploding gradients\n",
    "\n",
    "When training a _very deep_ NN, it is possible that gradients become too small.  \n",
    "**Solution** carefull choice of random weight initializaton.  \n",
    "\n",
    "Consider a $L$ NN. and $g(z)$ activation function.  \n",
    "\n",
    "$y=[...w^{[l]}...] \\times x$. \n",
    "\n",
    "If each of the $w^{[l]}$ is a bit larger than identity matrix. \n",
    "Than \n",
    "$$\n",
    "\\hat{y} \\propto \n",
    "\\begin{bmatrix}\n",
    "1.5 & 0 \\\\\n",
    "0, & 1.5\n",
    "\\end{bmatrix}^{[L-1]} \\times X\n",
    "$$\n",
    "So, if $L$ is large, the $w$ matrix may `explode` due to exponential dependence.   \n",
    "If weghts are small, however, then the power $L-1$ can make the matrix zero. \n",
    "\n",
    "> In a deep NN, activations make explode or get to zero if weights are large or small\n",
    "\n",
    "### Solving exploding gradient problem\n",
    "\n",
    "Consider a single Nueron of a NN. $a = g(z)$ and $z=\\sum w_i x_i$.  \n",
    "Note: for larger $n$, the $w_i$ should be smaller to _avoid_ the gradient problem.  \n",
    "We can set the weights such that the variance of them $\\text{var}(w)=1/n$, where $n$ is the number of features that goes into the neuron.  \n",
    "$$\n",
    "w^{[l]} = \\texttt{\\text{np.random.rand(..shape..)}}\\times\\texttt{\\text{np.sqrt}(1/n{[l-1]})}\n",
    "$$\n",
    "For $ReLU$ activation function it is a bit different.  \n",
    "\n",
    "This helps reduce the exploding gradient problem.  \n",
    "For $\\tanh$ activation function, the initialization is different.  \n",
    "See `Xavier initialization` paper. \n",
    "\n",
    "This can eventually be wrapped into a hyperparameter -- how to set variance to 0 for various activation functions. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical approximation of gradients\n",
    "\n",
    "When creating a NN it is usfull to implement *gradient checking*.  \n",
    "The difrst step in it is to implement *numerically approximate gradients*.  \n",
    "\n",
    "The numerical approximatrion of gradients is essentially _finite differencing_. Specifically, the two-sided stencil of the finite differencing. \n",
    "$$\n",
    "g(\\epsilon) = \\lim_{\\epsilon\\rightarrow0}\\frac{f(\\theta+\\epsilon) - f(\\theta-\\epsilon)}{2\\epsilon} + \\mathcal{O}(\\epsilon^2)\n",
    "$$\n",
    "where the last term is the error. \n",
    "\n",
    "For one-sided stencil, usually used in NNs, the error is of the order of $\\mathcal{O}(\\epsilon)$.  \n",
    "\n",
    "### Gradient checking\n",
    "\n",
    "A tool to veryfy the correct implementation of backprop.  \n",
    "\n",
    "1. Concatenate all $W^{[l]}$ and $b^{[l]}$ after reshaping into a single vector $\\theta$. \n",
    "2. Do the same for all $dW^{[l]}$ and $W^{[l]}$ to create vector $d\\theta$\n",
    "3. check if $d\\theta$ is the slope of the cost function $J(\\theta) = J(\\theta_1....\\theta_n)$. \n",
    "This is done via two-sided stencil, i.e.,\n",
    "$$\n",
    "d\\theta[i]_{\\rm approx} = \\frac{J(\\theta_1...\\theta_{i}+\\epsilon,...)-J(\\theta_1...\\theta_{i}-\\epsilon,...)}{2\\epsilon} = d\\theta[i] = \\frac{\\partial J}{\\partial \\theta_i}\n",
    "$$\n",
    "_for every value of $i$_.  \n",
    "4. Compare $d\\theta_{\\rm approx}$ and $d\\theta$ via computing _eucledian distance as \n",
    "$$\n",
    "\\frac{ || d\\theta_{\\rm approx} - d\\theta||_2 }{||d\\theta_{\\rm approx}||_2 + ||d\\theta||_2} \\lesssim 10^{-7} \n",
    "$$\n",
    "\n",
    "where in the last term, it is a criterion. It should not be larger than $10^{-5-7}$. \n",
    "\n",
    "### Implementing gradient checking\n",
    "\n",
    "- This is only for debug. Calculations are very slow.  \n",
    "- Investigate which component of $d\\theta_{\\rm approx}[i]$ is far from $d\\theta[i]$ \n",
    "- Remember regularization (include the regularization term)\n",
    "- Does not work with dropout (the cost faction $J$ is complex) \n",
    "- Check the grad check ar rundom initialization (perhaps again after some training) This is because gradnetns might be incorrectly imlemented but it shows only for very small $W$ $b$.\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyBlastAfterglow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
