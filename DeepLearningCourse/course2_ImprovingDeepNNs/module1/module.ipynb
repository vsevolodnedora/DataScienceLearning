{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / Dev / test sets\n",
    "\n",
    "> Building a NN is an iterative process\n",
    "\n",
    "Stractured data is used for Ads, speach, sequirity, logistics.  \n",
    "Thus coming from NLP, or vision, it is not strigtforward to go to analyzing stractured data. \n",
    "\n",
    "Data us usually split into \n",
    "- Test set \n",
    "- cross-validation set\n",
    "- test set\n",
    "\n",
    "Common rule is 70/30 or 60,20,10 for small data sets.  \n",
    "In case of huge datasets: just a few percent for test sets are sufficient.  \n",
    "\n",
    "> data from test and cv sets should be coming from **the same distribution**. \n",
    "\n",
    "Sometimes only test and cv sets are needed. So overall, generally, 2 datasets are the bare minimum needed. \n",
    "\n",
    "#### Bias vs Variance (underfitting vs overfitting)\n",
    "Consider Train set error and dev set error. (assume that test error __should__ be very low i.e., base error is low)\n",
    "- If they are $1$ and $11$, this is **high variance**. \n",
    "- If they are $11$ and $16$, this is **high bias**. \n",
    "- If they are $15$ and $30$, this is **high bias & varaince**. \n",
    "- If they are $.5$ and $1$, this is **low bias & varaince**. \n",
    "\n",
    "#### Improving high bias / varaince problem\n",
    "\n",
    "Check the bias (looking for training set).  If bias is high, try other network configurations, ($L$, $\\alpha$, ..., architecture)  \n",
    "**Note** Getting more data here usually _does not_ help\n",
    "\n",
    "After bias is solved, the high variance problem can be solved by \n",
    "(e.g., getting more data, regulariation, more appropriate NN achitecture). \n",
    "\n",
    "> Bias/variance tradeoff. Old proplem. Can be solved with Bigger NN (reduces bias) and getting more data (reduces variance)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recularzation\n",
    "\n",
    "> Regularization prevents overfitting.\n",
    "\n",
    "In, for example, logisitc regression, we are trying to minimze the cost function. In **Regularization** we add the regularization term to the cost\n",
    "$$\n",
    "J(w,b) += \\frac{\\lambda}{2m}b^2\n",
    "$$\n",
    "where the notm of $w^2$, the **$L_2$ regularization** term reads\n",
    "$$\n",
    "||w^{[l]}||^2 = \\sum_{i=1}^{n^{[l]}}\\sum_{j=1}^{n^{[l-1]}}( w_{i,j}^{[l]} )^2\n",
    "$$\n",
    "One can also consider $L_1$ regularization term $\\lambda/m ||w||_1$\n",
    "Here $w$ is sparse (has too many zeros), and can help with **compressing** the model. \n",
    "\n",
    "> $\\lambda$, the regularization term, is set using CV set.\n",
    "\n",
    "In NN, the regularization is added as ter to the cost function where one has to loop over all training examples and units\n",
    "$$\n",
    "||w^{[l]}||^2 = \\sum_{i=1}^{n^{[l]}}\\sum_{j=1}^{n^{[l-1]}}( w_{i,j}^{[l]} )^2\n",
    "$$\n",
    "> THis is so-called **Frobinious norm** of the matrix\n",
    "\n",
    "In the past $dw$ was computed using backprop. Now the update is:\n",
    "$$\n",
    "dw^{[l]} = (\\text{from backprop}) + \\frac{\\lambda}{m}w^{[l]};\\,\\, \\frac{\\partial J}{\\partial w^{[l]}} = dw^{[l]}\n",
    "$$\n",
    "> Sometimes $L2$ regularization is called ''weight decay''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyBlastAfterglow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
