{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-batch Gradient Descent\n",
    "\n",
    "Training on a large data sets is slow. \n",
    "\n",
    "Optimization algorithms are needed. \n",
    "\n",
    "Vecotrization is one of the optimization tools. \n",
    "\n",
    "Processing the entire data sets for each pass of gradient descent is slow for very large training sets. The training sets can be split in into `mini-batches` of $X^{\\{i\\}}$ training data and $Y^{\\{i\\}}$ labels.  \n",
    "Recall\n",
    "- $(i)$ is for training example \n",
    "- $[i]$ is for layer\n",
    "- $\\{i\\}$ is for mini-batch \n",
    "\n",
    "**Batch gradient descent** processing the enitre datasets\n",
    "**Mini-batch grad. descent** processing mini-batche. \n",
    "\n",
    "In the algorithm, we loop over each mini-batch and in each of them, we loop over the data. The main loop can be parallelized. For eacj batch there is a cost function $J$ with regularization. Then compute backprob for each minibatch. This constitute one `epoch` of training. \n",
    "\n",
    "**Note** as the dataset was split, one epoch now constitutes $N$ epoches as each minibatch was processed independently and simultaneously. \n",
    "\n",
    "In _batch grad. descent_ the cost, the overall cost, is expected to go down on every iteration. \n",
    "In _mini-batch gard. descent_ for a given batch, each time you are training on a **different dataset** and the cost may increase or decrease. But it **should tend downwards**. \n",
    "\n",
    "### Choosing mini-batch size\n",
    "\n",
    "If size = 1 (one example) this is `stochastic gradient descent`. \n",
    "The path for it is very noisy and almost never converge. \n",
    "\n",
    "In practice the size is $>1$ and $<m$, with $m$ being the data set size. \n",
    "\n",
    "In stochastic gradient descent the vectorization is very bad. \n",
    "\n",
    "For small training set, there is no need to use mini-batch gradient descent.  \n",
    "\n",
    "For large training set, it is good to use powers of two, e.g., 64,128, 512... to get possible speed up due to memory layout. \n",
    "\n",
    "Make sure, that all mini-batch fits inside the CPU/GPU memory. This affects performance drastically. \n",
    "\n",
    "### Exponentially Weighted (moving) Averages \n",
    "\n",
    "Optimization algorithm. \n",
    "\n",
    "Consider a time-series data $\\theta_t$. The average, $v_t$ is given by \n",
    "\n",
    "$$\n",
    "v_t = \\beta v_{t-1} + (1-\\beta)\\theta_t\n",
    "$$\n",
    "\n",
    "where $\\beta\\sim0.9$. \n",
    "\n",
    "The interpretation here is that $V_t$ averages over $1/(1-\\beta)$ values of $\\theta$. So for $0.9$ it averages over the last $10$ values.\n",
    "\n",
    "The closer the $\\beta$, to $1$, the larger the $n$ for averaging and the more shifted the resulted curve.   It intorudces __latency__. \n",
    "\n",
    "### Understanding exponentially Weighted (moving) Averages \n",
    "\n",
    "Note that in order to get $v_{m}$ for the last element, this recursively depends on each previous value. This is a weighted sum of $v_{i}$ in powers. This is _Expenentially decaying function_. The $n-1$ term is the most importnat and other terms have decreasing importance. _Exponentially decreasing_. The decay time is given by $(1-\\epsilon)^{1/\\epsilon} = 1/e$ where $\\epsilon=1-\\beta$. This is where the **exponential** part comes from. After $1/(1-\\beta)$ timesteps, the weight decays by _one fold_. \n",
    "\n",
    "The implementation is **very** computationally simple, as the same value can be overritten. This is very memory efficient. Otherwise, with explicit window averaging, the memory requirements are higher. \n",
    "\n",
    "### Bias correction in EWA \n",
    "\n",
    "Bias is introduced by averaging over large values (due to weighted moving average) implementation (this is not a window average). This can be addressed. \n",
    "\n",
    "Solution: noramize $v_t/(1-\\beta^t)$. \n",
    "\n",
    "> Bias correction is especially important for early-time data (when weighted average didn't have time to 'warm up')\n",
    "\n",
    "It is rarely implemneted in practice as after several itrations, the algorithm has enough data to avoid the bias\n",
    "\n",
    "### Gradient descent with momentum\n",
    "\n",
    "> Idea: compute the exponentially weighted average of gradients and use it to update the weights instead. \n",
    "\n",
    "The cost function topology may favour a motion into a specific direction. To smooth-out possible oscillations. \n",
    "\n",
    "For a `mini-batch` grad. descten, for each batch it looks like:\n",
    "$$\n",
    "V_{dw} = \\underbrace{\\beta}_{\\text{friction}} \\underbrace{V_{dw}}_{\\text{velocity}} + (1-\\beta) \\underbrace{dw}_{\\text{acceleration}} \\\\\n",
    "V_{db} = \\beta V_{db} + (1-\\beta) db \\\\\n",
    "w := w - \\alpha V_{dw} \\\\\n",
    "b := b-\\alpha V_{db}\n",
    "$$\n",
    "\n",
    "This algorithm allos to follow a _more stgithforward_ path. \n",
    "\n",
    "The derivativs here _provide accelereation_ and the $\\beta$ terms are similar to _velocity_. \n",
    "\n",
    "> Consider an analogy of a ball rolling down the hill with acceleration and momentum and friction. \n",
    "\n",
    "Two hyperparameters are introduced $\\beta=.9$ and $\\alpha$  \n",
    "Sometimes $\\alpha$ absorbs $1-\\beta$ from $V_{db} = \\beta V_{db} + (1-\\beta) db$ so that $V_{db} = \\beta V_{db} + db$ \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root mean squared prop\n",
    "\n",
    "This is another algorithm to optimize the gradient descent. It allows to otpimize the descent along the direction in which there are less oscillations and that lead quicker to the global minimum. \n",
    "\n",
    "In this algorithm, \n",
    "\n",
    "$$\n",
    "S_{dw} = \\beta_2 S_{dw} + (1-\\beta_2) dw^2 \\\\\n",
    "S_{db} = \\beta_2 S_{db} + (1-\\beta_2) db^2 \\\\\n",
    "w := w - \\alpha \\frac{dw}{\\sqrt{S_{dw}}+\\epsilon} \\\\\n",
    "b := b- \\alpha \\frac{S_{db}}{\\sqrt{S_{db}}+\\epsilon}\n",
    "$$\n",
    "\n",
    "where $^2$ is done element-wise and $\\epsilon$ is added for numerical stability.  \n",
    "Here $\\sqrt{S_{dw}}$ is expected to be small, while $\\sqrt{S_{db}}$ is big and will **slow-down** the algorithm. This expectation comes from the fact that derivatives are larger in the direction of a larger slope. So the algorithm **slows down** in the direction of a larger slope. \n",
    "\n",
    "This also allows to use _larger learning rate_. \n",
    "\n",
    "### Adaptive moment estimation (ADAM) optimization algorithm\n",
    "\n",
    "Adam is a combination of RMS prop and Grad. Descent. with momentum\n",
    "\n",
    "Initialization includes $V_{dw}=0$, $V_{db}=0$, $S_{dw}=0$ and $S_{db}=0$.\n",
    "\n",
    "At each iteration, compute the $dw$, $db$ with __mini-batch gradient descent__, and then do the momentum-like update with $\\beta_1$ and RMS-like update with $\\beta_2$, **icluding** the bias correction as $V_{dw} = V_{dw} / (1-\\beta_1^t)$. The full set of equations looks like:\n",
    "\n",
    "$$\n",
    "V_{dw} = \\beta_1 V_{dw} + (1-\\beta_1) dw \\\\\n",
    "V_{db} = \\beta_1 V_{db} + (1-\\beta_1) db \\\\\n",
    "V_{dw} = V_{dw} / (1-\\beta_1^t) \\\\\n",
    "V_{dw} = V_{dw} / (1-\\beta_1^t) \\\\\n",
    "$$\n",
    "\n",
    "RMS-like update with $\\beta_2$:\n",
    "$$\n",
    "S_{dw} = \\beta_2 S_{dw} + (1-\\beta_2) dw^2 \\\\\n",
    "S_{db} = \\beta_2 S_{db} + (1-\\beta_2) db^2 \\\\\n",
    "S_{dw} = S_{dw} / (1-\\beta_2^t) \\\\\n",
    "S_{dw} = S_{dw} / (1-\\beta_2^t) \\\\\n",
    "$$\n",
    "\n",
    "And the final update is\n",
    "$$\n",
    "w := w - \\alpha \\frac{V_{dw}^{\\rm corrected}}{\\sqrt{S_{dw}^{\\rm corrected}}+\\epsilon} \\\\\n",
    "b := b- \\alpha \\frac{V_{db}^{\\rm corrected}}{\\sqrt{S_{db}^{\\rm corrected}}+\\epsilon}\n",
    "$$\n",
    "\n",
    "The hyperparameters: $\\alpha$ is free, $\\beta_1\\approx0.9$, $\\beta_2\\approx0.999$ and $\\epsilon\\approx10^{-8}$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning-rate decay\n",
    "\n",
    "Especially usefull with mini-batch learning where the convergence can oscillate \n",
    "\n",
    "$$ \\alpha = 1/(1+\\text{decayRate}\\times\\text{epochNumber}) $$\n",
    "\n",
    "Other option: \n",
    "- Exponential decay\n",
    "- power-law decay\n",
    "- Mini-batch dependent decay\n",
    "- Manual decay\n",
    "\n",
    "## The problem of local optima\n",
    "\n",
    "Most points with zero-gradient are saddle points. For a high-dimensional space especially. \n",
    "\n",
    "Plateoes can slow down the learning significantly. \n",
    "\n",
    "Adam can help in moving fast out of the plateue region\n",
    "\n",
    "(see exercise for implementaiton of these methods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
