{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Process\n",
    "\n",
    " There are many hyperparameters in training NN. \n",
    " Examples: \n",
    " - $\\alpha$\n",
    " - $\\beta$ momentum term\n",
    " - $\\beta_1$ and $\\beta_2$, $\\epsilon$ from ADAM algorithm\n",
    " - $N$ of layers\n",
    " - $M$ of hidden units\n",
    " - learning rate decay \n",
    " - ... \n",
    " - mini-batch size\n",
    "\n",
    " The most important here are $\\alpha$ and then $\\beta$ and mini-batch size, and hidden units. \n",
    "\n",
    " #### Setting values for hyperparameters.  \n",
    " Eary times the uniform sampling of a parameter space was used.  \n",
    " Nowardays, a random sampling of a hyperparameter space is adviced.  \n",
    " Then select a smaller region and sample it again. \n",
    "\n",
    "## Appropritate scale to chose hyperparams \n",
    "\n",
    "Small range - uniform sampleing can do  \n",
    "Large valeus, e.g., $\\alpha\\in(10^{-5},1)$. There logarithmic scaling is better.  \n",
    "\n",
    "#### Hyperparameters for exponentially weighted averages\n",
    "$\\beta \\in (0.9...0.99999)$. Consider $1-\\beta$ and logarithmic random uniform gridng $a=$rand  and $10^a$ is log random. \n",
    "> Note alrge sensitity to $\\beta$ when $\\beta\\rightarrow1$, so more dens sempling there is preferred\n",
    "\n",
    "#### Re-test hyperparameters occasionally\n",
    "\n",
    "- Babysitting one model (cahnge parameters after each epoch/day)\n",
    "- Training many models in parallel\n",
    "\n",
    "This depends on the computational resourses \n",
    "\n",
    "\n",
    "## Batch normalization \n",
    "\n",
    "Recall that when learning, normalization of the input features can speed-up the process\n",
    "\n",
    "$X=X/\\sigma$\n",
    "\n",
    "For deep NN, there are intermedeate activation function with $a^{[i]}$ which can also be normalized for faster training. \n",
    "\n",
    "The `batch normalization` is $Z^{[i]}$ normalization of **some** units in **some** hidden layers \n",
    "\n",
    "$$\n",
    "z_{\\rm norm}^{(i)} = \\frac{z^{(i)-\\mu}}{\\sqrt{\\sigma^2+\\epsilon}}\n",
    "$$\n",
    "\n",
    "This leads to mean $\\mu=0$ and standard unit variance $\\sigma=1$. \n",
    "> However not olways the hiddle units should have mean 0 and variance 1. \n",
    "For some activation functions it is better to have varaince $\\neq1$\n",
    "\n",
    "Different distribution is desirable \n",
    "$$\n",
    "\\tilde{z}^{(i)} = \\gamma z_{\\rm norm}^{(i)} + \\beta\n",
    "$$\n",
    "where $\\gamma$ and $\\beta$ are learniable parameters of the model via e.g., gradient descent.  \n",
    "These parameters allow to have $\\mu$ and $\\tilde{z}$ to be any value. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Batch Norm into a NN\n",
    "\n",
    "Recall. Each Unit of a NN computes first $z_{i}^{[l]}$ and then applies the activation function $a_{i}^{[l]}$.  \n",
    "\n",
    "Appling batch norm (BN) we get now normalized\n",
    "\n",
    "$$\n",
    "X \\overbrace{\\rightarrow}^{w^{[1]},b^{[1]}} Z^{[1]} \\underbrace{\\overbrace{\\rightarrow}^{\\beta^{[1]},\\gamma^{[1]}}}_{\\text{BN}} \\tilde{Z}^{[1]} \\rightarrow a^{[1]}=g^{[1]}(\\tilde{z}^{[1]}) \\\\\\overbrace{\\rightarrow}^{w^{[2]},b^{[2]}} Z^{1} \\underbrace{\\overbrace{\\rightarrow}^{\\beta^{[2]},\\gamma^{[2]}}}_{\\text{BN}} \\tilde{Z}^{[2]} \\rightarrow a^{[2]}=g^{[2]}(\\tilde{z}^{[2]})\n",
    "$$\n",
    "\n",
    "using normalized values.  \n",
    "The parameters of the NNs are $W^{[i]}$, $b^{[i]}$, $\\beta^{[i]}$, $\\gamma^{[i]}$, that are learned using gradient descent.  \n",
    "\n",
    "Usaally batch-norm is applied using mini-batches, as shown above, but for each mini-batch separaterly.\n",
    "\n",
    "Note that in this algorithm, as we compute means for every $\\tilde{z}$, the constant, $b^{[l]}$ will be removied automatically. So the actual list of parameters is \n",
    "$W^{[i]}$, $\\beta^{[i]}$, $\\gamma^{[i]}$\n",
    "\n",
    "Dimenstions of these parameters are $\\beta^{[l]} = (n^{[l]},1)$, $\\gamma^{[l]} = (n^{[l]},1)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
