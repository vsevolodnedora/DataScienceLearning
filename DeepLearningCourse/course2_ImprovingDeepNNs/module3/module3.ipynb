{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Process\n",
    "\n",
    " There are many hyperparameters in training NN. \n",
    " Examples: \n",
    " - $\\alpha$\n",
    " - $\\beta$ momentum term\n",
    " - $\\beta_1$ and $\\beta_2$, $\\epsilon$ from ADAM algorithm\n",
    " - $N$ of layers\n",
    " - $M$ of hidden units\n",
    " - learning rate decay \n",
    " - ... \n",
    " - mini-batch size\n",
    "\n",
    " The most important here are $\\alpha$ and then $\\beta$ and mini-batch size, and hidden units. \n",
    "\n",
    " #### Setting values for hyperparameters.  \n",
    " Eary times the uniform sampling of a parameter space was used.  \n",
    " Nowardays, a random sampling of a hyperparameter space is adviced.  \n",
    " Then select a smaller region and sample it again. \n",
    "\n",
    "## Appropritate scale to chose hyperparams \n",
    "\n",
    "Small range - uniform sampleing can do  \n",
    "Large valeus, e.g., $\\alpha\\in(10^{-5},1)$. There logarithmic scaling is better.  \n",
    "\n",
    "#### Hyperparameters for exponentially weighted averages\n",
    "$\\beta \\in (0.9...0.99999)$. Consider $1-\\beta$ and logarithmic random uniform gridng $a=$rand  and $10^a$ is log random. \n",
    "> Note alrge sensitity to $\\beta$ when $\\beta\\rightarrow1$, so more dens sempling there is preferred\n",
    "\n",
    "#### Re-test hyperparameters occasionally\n",
    "\n",
    "- Babysitting one model (cahnge parameters after each epoch/day)\n",
    "- Training many models in parallel\n",
    "\n",
    "This depends on the computational resourses \n",
    "\n",
    "\n",
    "## Batch normalization \n",
    "\n",
    "Recall that when learning, normalization of the input features can speed-up the process\n",
    "\n",
    "$X=X/\\sigma$\n",
    "\n",
    "For deep NN, there are intermedeate activation function with $a^{[i]}$ which can also be normalized for faster training. \n",
    "\n",
    "The `batch normalization` is $Z^{[i]}$ normalization of **some** units in **some** hidden layers \n",
    "\n",
    "$$\n",
    "z_{\\rm norm}^{(i)} = \\frac{z^{(i)-\\mu}}{\\sqrt{\\sigma^2+\\epsilon}}\n",
    "$$\n",
    "\n",
    "This leads to mean $\\mu=0$ and standard unit variance $\\sigma=1$. \n",
    "> However not olways the hiddle units should have mean 0 and variance 1. \n",
    "For some activation functions it is better to have varaince $\\neq1$\n",
    "\n",
    "Different distribution is desirable \n",
    "$$\n",
    "\\tilde{z}^{(i)} = \\gamma z_{\\rm norm}^{(i)} + \\beta\n",
    "$$\n",
    "where $\\gamma$ and $\\beta$ are learniable parameters of the model via e.g., gradient descent.  \n",
    "These parameters allow to have $\\mu$ and $\\tilde{z}$ to be any value. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Batch Norm into a NN\n",
    "\n",
    "Recall. Each Unit of a NN computes first $z_{i}^{[l]}$ and then applies the activation function $a_{i}^{[l]}$.  \n",
    "\n",
    "Appling batch norm (BN) we get now normalized\n",
    "\n",
    "$$\n",
    "X \\overbrace{\\rightarrow}^{w^{[1]},b^{[1]}} Z^{[1]} \\underbrace{\\overbrace{\\rightarrow}^{\\beta^{[1]},\\gamma^{[1]}}}_{\\text{BN}} \\tilde{Z}^{[1]} \\rightarrow a^{[1]}=g^{[1]}(\\tilde{z}^{[1]}) \\\\\\overbrace{\\rightarrow}^{w^{[2]},b^{[2]}} Z^{1} \\underbrace{\\overbrace{\\rightarrow}^{\\beta^{[2]},\\gamma^{[2]}}}_{\\text{BN}} \\tilde{Z}^{[2]} \\rightarrow a^{[2]}=g^{[2]}(\\tilde{z}^{[2]})\n",
    "$$\n",
    "\n",
    "using normalized values.  \n",
    "The parameters of the NNs are $W^{[i]}$, $b^{[i]}$, $\\beta^{[i]}$, $\\gamma^{[i]}$, that are learned using gradient descent.  \n",
    "\n",
    "Usaally batch-norm is applied using mini-batches, as shown above, but for each mini-batch separaterly, the $\\tilde{z}^{i}$ are computed independently.\n",
    "\n",
    "Note that in this algorithm, as we compute means for every $\\tilde{z}$, the constant, $b^{[l]}$ will be removied automatically (as mean and variance are normalized). So the actual list of parameters is \n",
    "$W^{[i]}$, $\\beta^{[i]}$, $\\gamma^{[i]}$\n",
    "\n",
    "Dimenstions of these parameters are $\\beta^{[l]} = (n^{[l]},1)$, $\\gamma^{[l]} = (n^{[l]},1)$\n",
    "\n",
    "### WHy does Batch norm works\n",
    "\n",
    "Batch norm is the normalization of inputs for hidden units.  \n",
    "This makes performace of deeper layers to be more robust with respect to changes in previous layers.  \n",
    "\n",
    "> `Covariant shift` - changes in the distribution in the training data (it requires retraining of the model)\n",
    "\n",
    "A given hidden layer gets $a^{[i]}$ as input. However, contrary to the first layer of the NN, that takes $X$ as an input which is **constant**, the $a^{[i]}$ values **always cahnge** during training. This introduces **covariant shift**. \n",
    "\n",
    "> _Batch norm_ helps reducing the covariant shift for deeper layers of a NN, the distribution of hidden values does not change as much. \n",
    "\n",
    "**Overall**, NN becomes more stable. Each layer can learn more idependently from other layers. \n",
    "\n",
    "> Batch norm has a small regularization effect\n",
    "\n",
    "This is because each mini-batch is scaled by the mean/varaince, computed on just that mini-batch. \n",
    "THis adds some noise to the values $z^{[l]}$ within that minibatch. So, similar to dropout, it adds some noise to each hidden layer activation. This leads to regularization effect. \n",
    "\n",
    "**However** it is not a regulizer!\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch norm at a test time\n",
    "\n",
    "**Note** Batch norm processes train data __one minibatch at a time__. At a test time we process sometimes __one example at a time__. \n",
    "\n",
    "For computing $\\mu$ and $\\sigma$, all train examples in a given mini-batch are used. THis is, however, not possible for test time, when only one example enters. \n",
    "\n",
    "Other estimations for $\\sigma$ and $\\mu$ are needed.  \n",
    "One option is the **exponentially weighted avegae** - avegare is across all mini-batches. There exists many *training* $\\mu$ for each minibatch. From these we compute $\\mu$ for train set. As running average for each layer. \n",
    "\n",
    "\n",
    "# Multiclass classification\n",
    "\n",
    "### Soft-max regression\n",
    "\n",
    "If there exist **many possible classes** that we need to identify, there is **soft-max regression**.\n",
    "\n",
    "> SOftmax is a generalization of the logistic regression to more than two classes case\n",
    "\n",
    "Let $C$ be the n of classes. The output layer than has the dimension of the number of classes. \n",
    "\n",
    "In this final layer we compute the usual $z^{[L]} = w^{[L]} a^{[L-1]} + b^{[L]}$.  \n",
    "\n",
    "The activation function looks like \n",
    "\n",
    "$a^{[L]}=\\frac{e^{Z^{[L]}}}{\\sum_{i=1}^{4}t_i}$\n",
    "\n",
    "where $t_i = e^{Z^{[L]}}$.\n",
    "\n",
    "This is essencially normalization across possible outcomes. It takes $[c,1]$ shape vector and returns $[C,1]$ shape vector. \n",
    "\n",
    "### Training a softmax classifier\n",
    "\n",
    "**Note** there is a _hard max_ function, which just like one-hot encoding, given for 4 classes $[1,0,0,0]^T$ vector. The _soft max_ instead allow values $\\in(0,1)$, the probabilties, for each entry. \n",
    "\n",
    "> If $C=2$ the softmax reduces to logistic regression\n",
    "\n",
    "**Loss function** for soft-max is \n",
    "\n",
    "$$\n",
    "\\mathcal{L(\\hat{y},y)} = -\\sum_{j=1}^4 y_j\\log(\\hat{y}_j)\n",
    "$$\n",
    "\n",
    "It looks at the ground trooth in your dataset and tries to make corresponding probabilities as large as possible.  \n",
    "This is equivalent to maximum **liklihood estimateion**.  \n",
    "\n",
    "The cost function as just a normalized sum of loss function as before.  \n",
    "\n",
    "Gradient descent in this layer is done by appliying the activation function in a forward step.  \n",
    "In the back prop, the \n",
    "\n",
    "$$\n",
    "dz^{[L]} = \\hat{y} - y\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning frameworks\n",
    "\n",
    "Currently, the following frameworks exist:\n",
    "- Caffe/Caffe2\n",
    "- CNTK\n",
    "- DL4J\n",
    "- Keras\n",
    "- Lasagne\n",
    "- mxnet\n",
    "- PaddlePaddle\n",
    "- TensorFlow\n",
    "- Theano\n",
    "- Torch\n",
    "\n",
    "### TensorFlow\n",
    "\n",
    "TensorFlow takes the cost function and constructs a **computation graph** for forward prop. Thus, it automacally generates the backprop steps.\n",
    "\n",
    "Finished exercise\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
