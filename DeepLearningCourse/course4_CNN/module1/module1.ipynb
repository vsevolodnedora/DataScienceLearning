{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "\n",
    "Used in `computer vision`, object detection, style transfer...\n",
    "\n",
    "Challenge: large inputs (images have large resolution) -> extremely high dimension for input layer of a NN. And for **fully connected NN**, the matrix of weights is extremely large.   \n",
    "\n",
    "Solution: convolution operation\n",
    "\n",
    "### Convolution operation\n",
    "\n",
    "Consider an edge detection in an image (gray scale).  \n",
    "Given an $N\\times N$ matrix of the image, constract a __kernal__ or __filter__ of the image $F\\times F$ where $F<N$ and `convolve` it wih the original matrix via __convolution operation__ '$*$'. The oputput of the convolution operation, _filter_ is a matrix $P\\times P$ where $N<P<F$. \n",
    "\n",
    "The first element of this matrix is computed by overlaying the $F\\times F$ matrix with the equally sized region of the $N\\times N$ matrix in the upper left corner and performing **element-wise** multiplication and **summing** the result.\n",
    "\n",
    "Next, shift the convolution layer one element and repeat the convolution to get the second number in $P\\times P$ matrix. \n",
    "\n",
    "In python comvolution is done with $\\texttt{\\text{tf.nn.conv2d}}$. \n",
    "\n",
    "If the _filter_ is made to detect vertical edges, i.e., \n",
    "bright pixes on the left and dark pixels on the right: \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & -1 \\\\\n",
    "1 & 0 & -1 \\\\\n",
    "1 & 0 & -1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Convolving it with the image that has an edge, it will give a lighter (higher values) for the region where there is such a structure, an _edge structure_. \n",
    "\n",
    "### Types of edge detections\n",
    "\n",
    "For horizontal edges, the filter would be \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "-1 & -1 & -1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "High values in the resulted matrix would indicate the presence of an edge. Note that if exact transition orientation is not important, an absolute value should be taken. \n",
    "\n",
    "\n",
    "#### Vertical edge detection examples:\n",
    "\n",
    "**Sobel filter** \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & -1 \\\\\n",
    "2 & 0 & -2 \\\\\n",
    "1 & 0 & -1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Scharr filter**\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "3 & 0 & -3 \\\\\n",
    "10 & 0 & -10 \\\\\n",
    "3 & 0 & -3 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "These filters can be learned by a NN. Back-prop can learn these values in matrixes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "\n",
    "Convolving $6\\times 6$ matrix with $3\\times 3$ filter results in a $4\\times 4$ matrix. \n",
    "\n",
    "Convolving $n\\times n$ matrix with $f\\times f$ filter results in a $(n-f+1)\\times (n-f+1)$ matrix. \n",
    "\n",
    "> Every time conv. oper. is applied, the image shrinks. \n",
    "\n",
    "Corner pixels are used only in one operation, while middle pixes are used multiple times. \n",
    "\n",
    "`Padding` -- adding addition 'border' image to preserve the original size after convolving with the filter.  \n",
    "\n",
    "Padding can also be done with two or more pixels. \n",
    "\n",
    "- `Valid convolution` - no padding\n",
    "- `Same convolution` - oputput size is then equal to the original size. Padding size is then set by the filter size as $p = (f-1)/2$. \n",
    "\n",
    "Usually $f$ is an odd number (symmetric padding). It also gives a central position.  \n",
    "\n",
    "### Strided Convolution\n",
    "\n",
    "Where the filter moves not by one pixel forward but by $s$ (with the same element-wise product). This gives smaller final image after the convolution. The output is \n",
    "$$\n",
    "(n + 2p - f)/s+1, (n + 2p - f)/s+1\n",
    "$$, \n",
    "where $n$ is the image, $p$ is the padding  and $f$ is the filter. \n",
    "\n",
    "#### _Cross correlation vs. convolution_\n",
    "\n",
    "Convolution in math: Filter is first (mirrored) along both axis  _transposed?_ before being applied to the original matrix. This allows to have the following prperties that used in signal __signal processing__ \n",
    "\n",
    "$$\n",
    "(A*B)*C = A*(B*C)\n",
    "$$\n",
    "\n",
    "But for deep NNs it is not that important. \n",
    "\n",
    "What we do here is more _cross-correlation_ but commonly used as __convolution__.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Over Volume \n",
    "\n",
    "For colored images. $3$ rd dimension is the number of channels. \n",
    "\n",
    "Filter should have __the smae__ number of channels. \n",
    "\n",
    "Output there is computed by __piece-wise__ multiplication for each channel for each pixel and then sum the enitre thing to get the output pixel value. \n",
    "\n",
    "Then the edges in a red for example, would have the standaed edge filter for red and all zeroes for other channels. \n",
    "\n",
    "This is feature detection for various cahnnels. \n",
    "\n",
    "### Multiple filters\n",
    "\n",
    "Applyting multiple filters to the same image, we get different iputps and __stack__ the results into a new multid-dimensional outputs. \n",
    "\n",
    "$$\n",
    "(n\\times n\\times n_c)*(f\\times f\\times n_c)\\rightarrow ((n - f + 1)\\times (n - f + 1) \\times n_c')\n",
    "$$\n",
    "\n",
    "where $n_c$ is the number of filters\n",
    "\n",
    "### One Layer of a Convolutional Network\n",
    "\n",
    "One layer of a CNN is applyting multiple, $n_c$ filters toa given image with dimensions $n\\times n \\times n_c$ and then applyting a certain non-linear function, e.g., ReLU to the output and adding a bias value, and the stacking the outputs together with the thrird dimenstion being the number of filter $n_f$.  \n",
    "\n",
    "$Z^{[1]} = w a^{[0]} + b^{[1]}$, with $a^{[1]} = g(z^{[1]})$\n",
    "\n",
    "where $w a^{[0]}$ is the convolution operation on each filter. \n",
    "\n",
    "##### Example: \n",
    "Consider a 10 filters that are $3\\times3\\times3$ in one layer of a NN. How many parameters does the layer have? \n",
    "\n",
    "Answer: $3\\times 3\\times 3 = 27$ weights plus $1$ bias times $10$ filters. So it is $280$ parameters. \n",
    "Note that this value is **independent** of the image size.  \n",
    "Hence, this model is less prone to _overfitting_.  \n",
    "\n",
    "### Summary\n",
    "\n",
    "- $f^{[l]}$ - filter size in layer $l$\n",
    "- $p^{[l]}$ - padding size in layer $l$\n",
    "- $s^{[l]}$ - stride size in layer $l$\n",
    "- $n_c^{[l]}$ - number of filters in layer $l$\n",
    "\n",
    "Input for layer $[l]$ is: $n_H^{[l-1]} \\times n_W^{[l-1]} \\times n_c^{[l-1]}$ for hight and width (if different). \n",
    "\n",
    "Output $n_H^{[l]} \\times n_W^{[l]} \\times n_c^{[l]}$ with the $n^{[l]} = \\Big[ \\frac{n^{[l-1]} + 2p^{[l]} - f^{[l]}}{s} +1 \\Big]$. \n",
    "for hight and width in the same way.  \n",
    "\n",
    "**Note** output image depth is equal to the number of filters. \n",
    "\n",
    "Size of each filter for layer $l$: $f^{[l]}\\times f^{[l]}\\times n_c^{[l-1]}$ (with the number of channels being the same as in the image)\n",
    "\n",
    "Activations: $a^{[l]}\\rightarrow n_H^{[l]}\\times n_W^{[l]}\\times n_c^{[l]}$.  \n",
    "For vectorized implementation $A^{[l]} \\rightarrow m\\times n_H^{[l]}\\times n_W^{[l]}\\times n_c^{[l]}$\n",
    "\n",
    "Weights $f^{[l]}\\times f^{[l]} \\times n_c^{[l]} \\times n_c^{[l]}$, with the last being the number of filters in the layer $l$.  \n",
    "\n",
    "Bias  $n_c^{[l]}$ is $(1,1,1,n_c^{[l]})$. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Convolutional Network Example\n",
    "\n",
    "Consider an image $39\\times 39\\times 3$.  \n",
    "There $n_H^{[0]}=n_w^{[0]}=39$ and $n_c^{[0]}=3$.  \n",
    "Assume that we use a _filter_ with $f^{[1]}=3$, $s^{[1]}=1$ and $p^{[1]}=0$.  \n",
    "The result is $37\\times 37\\times 10$ where the last number is the result of 10 filters application.  \n",
    "The dimension of the _activation_ in the first layer than $n_H^{[1]}=n_w^{[1]}=37$ and $n_c^{[0]}=10$ (number of filters). \n",
    "\n",
    "next, assume that the _filter_ is:\n",
    "$f^{[2]}=5$ and $s^{[2]}=2$ and $p^{[2]}=0$ and 20 of them.  \n",
    "The ouput is the $17\\times 17\\times 20$ with 20 being the number of filters.  \n",
    "\n",
    "At the end, we flatten the result into a vector and feed it into a ligistic or softmax layer to get the prediction.  \n",
    "\n",
    "Hyperparamters are difficult to select. \n",
    "\n",
    "It is common that the size, $W\\times$ H will decrease while the number of channels increases. \n",
    "\n",
    "**Types of layers in CNN**: \n",
    "- Convolutiopn (CONV)\n",
    "- Pooling (POOL)\n",
    "- Fully connected (FC)\n",
    "\n",
    "### Pooling layers\n",
    "\n",
    "- Reduct size, speed-up, make robust calcs. \n",
    "\n",
    "#### Max pooling: \n",
    "Split image into $x$ regions and output would have the number of those regions where values are the $max$ over the region in the image.  \n",
    "\n",
    "This layer highlights the feature that exists -- make it preserved in the output layer. \n",
    "\n",
    "**Note** once $f$ and $s$ are fixed there are no hyperparameters to learn for this layer (as it is an empty filter essentially)\n",
    "\n",
    "So, essentially, it is like a standard convolution, but instead of summing all the results after the convolution, we just take an ''empty filter'', move it over the image and take a maximum value _inside this window_ for an output image pixel\n",
    "\n",
    "For each channel the max pooling is done idependently. \n",
    "\n",
    "#### Average pooling\n",
    "\n",
    "In a very deep in the NN\n",
    "\n",
    "Overall:  \n",
    "Hyperparameters $f$- filter size $n$- stride. Common $f=2$ $s=2$ that shrink the image by $2$.  \n",
    "\n",
    "But there are __No parameters__ to learn for a backprop.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN exmaple\n",
    "\n",
    "Image recogintion from colored images\n",
    "\n",
    "$[32\\times 32\\times 3]\\rightarrow[28\\times28\\times6]\\rightarrow[14\\times 14\\times6]$\n",
    "after Convolution and Pool layers.  \n",
    "\n",
    "Number of layers -- number of parameters.  \n",
    "Pooling layer does not have parameters and is generally treated as a part of 'conv' layer. \n",
    "\n",
    "$[14\\times 14\\times6]\\rightarrow[10\\times10\\times10]\\rightarrow[5\\times5\\times10]$\n",
    "\n",
    "after another 'conv' plus 'pool' combo.  \n",
    "Recall that maxpul with $f=2$ $s=2$ halves the $H$ and $W$ of the image.  \n",
    "\n",
    "Then we flatten the result after two layers $400\\times1$ vector.  \n",
    "Consider next lyaer with $120\\times1$ units. This is `fully-connected` layer. Standard layer with all connections. \n",
    "Than another layer with $84$ parameters and than final output layer with \\eg softmax activation function.  \n",
    "\n",
    "It is common to have the following parrten:  \n",
    "CONV-POOL-CONV-POOL-FC-FC-FC-SOFTMAX \n",
    "\n",
    "Activation size genrerally __slowly__ decreases with NN depth. \n",
    "\n",
    "\n",
    "### Why Convolution\n",
    "\n",
    "Reasons for Conv\n",
    "- Spacity of connections\n",
    "For FC layers, if the input is e.g., image, the amount of connections, e.g., parameters is just enormous. CONV allows to decrease the number of connections and as such, the number of parameters.  \n",
    "\n",
    "- Parameter sharing\n",
    "Parameter sharing allows different parts of the NN use the low-level 'feature' detection. Also a given image might have the same feature multiple times, again, there is no need to learn it every time.  \n",
    "(translation invaraicne can also be utilized)\n",
    "\n",
    "Cost functions are generally in FC layers. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
