{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Localization\n",
    "\n",
    "### Object localization\n",
    "\n",
    "- Classification with localization problem\n",
    "- Detection problem (for multiple objects)\n",
    "\n",
    "In **Image Classification**, the image is passed through *ConvNet* and *softmax* to extract the class.  \n",
    "\n",
    "For localization, we need to change the NN to get the bounding box. $b_x$, $b_y$, $b_h$, $b_w$. \n",
    "\n",
    "If training set contains the bounding box data, the bounding box can be learned and found in new data. \n",
    "\n",
    "How to define the target label $y$?  \n",
    "Assume that $y$ is a vector $[p_c, b_x, b_y, b_h, b_w, c_1, c_2, c_3]^T$,  where $p_c$ probability that the object is there. And $c_i$ are probabilities for different classes of the object (1 or 0).  \n",
    "\n",
    "Loss function $\\mathcal{L}(\\hat{y},y)$ is sum of squared residuals for if $y_1 = 1$ (if there is an image). Otherwise we do not care about other labels, so we get \n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\hat{y},y) = \n",
    "\\begin{cases}\n",
    "\\sum(\\hat{y}_i - y_i)^2 \\text{ if } y_1 = 1 \\\\\n",
    "(\\hat{y}_1-y_1)^2 \\text{ if } y_1 = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### Landmark detection \n",
    "\n",
    "Outpot an important points of the image (coordinates)\n",
    "\n",
    "An image can have many landmarks of interest (facial features). These can be extracted via *ConvNet*. This is used to extract emoptions, etc.  \n",
    "Computer graphics.  \n",
    "\n",
    "The labels in the train set are usually prepared _manually_. \n",
    "\n",
    "Labells have to be consistent between different images.  \n",
    "\n",
    "### Object detection\n",
    "\n",
    "Using _sliding window_ dectection algorithm and _ConvNet_.  \n",
    "- Create training set with __closely cropped__ image of an object (e.g., car)\n",
    "- and images without a car.  \n",
    "- Create a ConvNet to classifiy these crops if it is a car or not.  \n",
    "- Create a slide-window detection. For atest image, pick the window size and scroll through the entire image with the \"window\" (with a small stride). Classify each part of the image whether it is a car not. \n",
    "    - Resize the window and repeat the process\n",
    "    - Resize again and repeat \n",
    "\n",
    "> So, if there is a car in the image, the ConvNet should give positive at least once in this search. \n",
    "\n",
    "**Distadvantage** Very computatiaonlly expensive\n",
    "\n",
    "**Solution** Convolutional implementation of a sliding-window\n",
    "\n",
    "### Convolutional Implementation of Sliding Windows\n",
    "\n",
    "Turning FC layers into ConvLayers.  \n",
    "\n",
    "Consider an image $(14\\times14\\times3)$ then $5\\times5$ layers to make $(10\\times10\\times16)$ and then $2\\times2$ MAXPOOL to make $(5\\times 5\\times 16)$ ... then FC, FC, softmax with say $4$ labels for 4 varius objects present in the image.  \n",
    "\n",
    "In order to convert the last FC layers to ConvLayers, we repalce $1$ FC laer with $400$ neurons with $400$ ConvLayers with $5\\times 5$ so that ouput is $1\\times1\\times400$, then you need another $400$ of $1\\times 1$ filters. \n",
    "\n",
    "Implementation is from OverFeat paper: by Sermanet et al 2014. \n",
    "\n",
    "Consider imput of $(14\\times14\\times3)$ image. \n",
    "\n",
    "$$\n",
    "\\Big[ 14\\times14\\times3 \\Big]\n",
    "\\underbrace{\\rightarrow}_{5\\times 5}\n",
    "\\Big[ 10\\times 10\\times 16 \\Big]\n",
    "\\overbrace{\\underbrace{\\rightarrow}_{2\\times 2}}^{\\text{MAX POOL}}\n",
    "\\Big[ 5\\times 5\\times 16 \\Big]\n",
    "\\overbrace{\\underbrace{\\rightarrow}_{5\\times 5}}^{\\text{FC}}\n",
    "\\Big[ 1\\times 1\\times 400 \\Big]\n",
    "\\overbrace{\\underbrace{\\rightarrow}_{1\\times 1}}^{\\text{FC}}\n",
    "\\Big[ 1\\times 1\\times 400 \\Big]\n",
    "\\overbrace{\\underbrace{\\rightarrow}_{1\\times 1}}^{\\text{FC}}\n",
    "\\Big[ 1\\times 1\\times 4 \\Big]\n",
    "$$\n",
    "\n",
    "Consider a larger input image: \n",
    "\n",
    "$$\n",
    "\\Big[ 16\\times 16\\times 3 \\Big]\n",
    "\\underbrace{\\rightarrow}_{5\\times 5}\n",
    "\\Big[ 12\\times 12\\times 16 \\Big]\n",
    "\\overbrace{\\underbrace{\\rightarrow}_{2\\times 2}}^{\\text{MAX POOL}}\n",
    "\\Big[ 6\\times 6\\times 16 \\Big]\n",
    "\\overbrace{\\underbrace{\\rightarrow}_{5\\times 5}}^{\\text{FC}}\n",
    "\\Big[ 2\\times 2\\times 400 \\Big]\n",
    "\\overbrace{\\underbrace{\\rightarrow}_{1\\times 1}}^{\\text{FC}}\n",
    "\\Big[ 2\\times 2\\times 400 \\Big]\n",
    "\\overbrace{\\underbrace{\\rightarrow}_{1\\times 1}}^{\\text{FC}}\n",
    "\\Big[ 2\\times 2\\times 4 \\Big]\n",
    "$$\n",
    "\n",
    "The resulting $4$ in the output volume represents the results if we took the original image and movied it within the bigger 16 by 16 image, rerunning the net each time. But, but doing it like this we duplicate a lot of calcualtions!\n",
    "In the above implementaion, many calculations are shared. \n",
    "The `MAX POOL` layer guverns the stride with which we \"slide the window\" and the output density.  \n",
    "\n",
    "> Convolutional implementation allows to process the entire image without manually sliding the window and _rerunning_ the net every time!\n",
    "\n",
    "**Shortcoming** position of the bounding boxes is not accurate \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounding Box Predictions\n",
    "\n",
    "It is possible that non of the boxes matches the position of the car.  \n",
    "\n",
    "Especially if bounding box may not be perfectly squred. \n",
    "\n",
    "Solution: `YOLO` algorithm (you only look once). See Redmon et al 2015 paper. \n",
    "\n",
    "Idea: Devide a given image into _cells_. And process each cell of the grid spearately through the ConvNet discussed above. For each grid cell there exists a label $y=[p_c,b_x, b_y, b_h, b_w, c_1, c_2, c_3]^T$.  \n",
    "Assign object to the center of a given cell, if the __object center__ lies within the boundaries of this grid cell.  \n",
    "\n",
    "__Target output volume__ is $3\\times 3\\times 8$  for $8$ labels in the $Y$ and $3\\times 3$ cell devision. \n",
    "\n",
    "On the other side, Use the __ConvNet__ to process the image and adjust the layers such, that the output shape is equal to the shape of the _target volume_. \n",
    "\n",
    "use backprop to train the NN to output the target volume.  \n",
    "\n",
    "Advantage: faster.  \n",
    "**Note** there should be one object of interest in a gird cell.  \n",
    "**Single Conv. implementation**  \n",
    "\n",
    "#### Specifying the boundary boxes\n",
    "\n",
    "In YOLO algorithm in each cell, the opposite corners have $(1,1)$ and $(0,0)$. Then, the coordinates are specified within these bounds. $x_{i}\\int(0,1)$ however width and hight can be larger than $1$.  \n",
    "\n",
    "### Intesection Over Union (IoU)\n",
    "\n",
    "Consider if there are multiple boxes around an object (ground truth and model prediction). IoU computes the area where the boundary boxes intersect. \n",
    "\n",
    "$$\n",
    "\\text{IoU} = \\frac{\\text{size of overlapping region}}{\\text{size of the summed ara (all boxes)}}\n",
    "$$\n",
    "\n",
    "If predicted box is equal to the recovered, then IoU = 1. Otherwise, the result is considered good if $\\text{IoU} > 0.5$. \n",
    "\n",
    "### Non-max Supression\n",
    "\n",
    "Detecting a given object only once.  \n",
    "If you use grid overlay on a image. Than an object would be found by multiple cells if the object is big and lay in various cells. \n",
    "In the end an algorithm gives _multiple_ detections for each object.  Non-max supressiion, looks at probabilities of all these detections. If other boxes overlap with the one with the maximum probability (with IoU as a criteria) they are supressed. So each possible box is either highlighted or darkened (in terms of probability to be the best, final prediction).  \n",
    "\n",
    "1. Discard all prediciton, all boxs, with $p_c < 0.5$ or $0.6$. \n",
    "2. Pick the box with highest $p_c$ and output as a prediction. \n",
    "3. Discard any remaining box with high $\\text{IoU}$ with the box selected in $2.$. (overlapping box)\n",
    "4. Continue to do output the best and discard overlaps untill there are no remining boxes. \n",
    "\n",
    "If there are multiple objects that algorithm is trying to detect, the supression should be exercised independently for each class. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anchor Boxes\n",
    "\n",
    "Detecting multiple objects in one grid cell. \n",
    "\n",
    "Anchro boxes have predefine shape. The $y$ vector than contains $N$ segments associated with different _anchor boxes_. There will be $N$ probabilities $p_c$ for different acnhor boxes, and differnt cooridantes and classes.  \n",
    "This makes the output vector $N$ times larger (for each anchor box).   \n",
    "\n",
    "Each detected object is assigned to 1) grid cell 2) anchor box.  \n",
    "\n",
    "### YOLO Algorith (one of the most successful algorithms)\n",
    "\n",
    "**Training set construction** Consider three anchor boxes.  \n",
    "Go through ceach of the gird cells. \n",
    "Train a **ConvNet**, that ouputs the volume with $M\\times M\\times N$, where $M$ is the nmber of **cells** in the image and $M$ is the $8$ values of the $Y$ times the number of **anchor boxes**. \n",
    "Funally, run output through _Non-Max supression_ for each of the class separately.  \n",
    "\n",
    "### Region proposals R-CNN\n",
    "\n",
    "Run the segmentation agorithm to identify the \"blobls\" where the might be something interesting that is worh focuing the ConvNet on. This may give smaller number of positions. For each region, output the label.  \n",
    "Slow. \n",
    "\n",
    "Exits a fast R-CNN.  \n",
    "\n",
    "Future --  simultanous region identification and analysis.\n",
    "\n",
    "### Semantic Segmentation with U-Net\n",
    "\n",
    "Draw a carefull line around an object to know which pixels belong to an object.  \n",
    "\n",
    "The algorithm tries to label every pixel of the image based on its category: road, wall, car...  \n",
    "\n",
    "See Novivoc er al 2017 Fully Convolutional Achitecctures...  \n",
    "and Dong et al 2017 Automatic Brain Tumor Detection... \n",
    "\n",
    "Per-pixel class labels. \n",
    "\n",
    "Deep learning for semantic segmentation. \n",
    "\n",
    "Replace the last layers of ConvNet with \"mirror\" of the first layer (like encoder-decoder system) to get an image-size output in the end.  \n",
    "\n",
    "The second stage makes use of `transpose convolution`. \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transpose Convolutions\n",
    "\n",
    "Consider a filter that is larger than the image, with padding $p=1$ and stride $s=2$.  \n",
    "Then, you mechanically step over the __output__ image and _paste_ it. \n",
    "\n",
    "Consider $I_{i,j}$ of the input image. Multiple it by each element of the filter $F_{i,j}$. The result has the dimensions of the filter. Past it onto the output with the outermost elements landing into a _padded_ region, so they can be neglected.   \n",
    "Then, repeat with the second element of the input matrix, but move the result in the output one with a given _stride_. Then, also recal that the top row of elements may fall into _padding_ and should be neglected. \n",
    "Note that when the outputs overlap, the values must be added together!\n",
    "\n",
    "This process is effective. \n",
    "\n",
    "### U-Net Architecture Intuition\n",
    "\n",
    "Achitecture:  \n",
    "Normal convolution for the first part of the NN, and the second, increases the size of the net to get the image.  \n",
    "Net also emplys the _skip connection_ from the __frist__ to the __last__ elevel.  \n",
    "This is done to pass the high resolution low level spatial information (it was lost in deeper levels)\n",
    "\n",
    "### U-Net Architecture\n",
    "\n",
    "See Ronnenberger et al. 2015, U-Net \n",
    "\n",
    "There, the first part is composed of Conv + ReLU.  \n",
    "after the lowest point, the first Trans Conv is used, _with skip connection_ from the corresponding Conv layer. And the skip connection is used after each Trans Conv operation. \n",
    "At the end there are several $1\\times1$ layers"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
