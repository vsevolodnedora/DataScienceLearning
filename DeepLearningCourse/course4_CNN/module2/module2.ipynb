{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Studies\n",
    "\n",
    "- LeNet-5\n",
    "- AlexNet\n",
    "- VGG\n",
    "\n",
    "Residual Networks (152 layer net)\n",
    "\n",
    "Inception\n",
    "\n",
    "### Classic Netowrk achitectture\n",
    "\n",
    "#### LeNet-5 \n",
    "was made to analyze gray-scale images on  numbers. \n",
    "Did not use padding. Each layer was schrinking the image.  \n",
    "Used average pooling.  \n",
    "Number of channels increases as you goo deeper. \n",
    "Usually CONV layer is followed by the POOL layer. \n",
    "\n",
    "`LeCun et al 1998 Grad.Based learning`\n",
    "\n",
    "Graph transfprming Network\n",
    "\n",
    "#### AlexNet \n",
    "was a bigger net with 227by 227 image. \n",
    "Image->Conv->MaxPool->Conv->MaxPool->...->FC->SoftMax (for classification)  \n",
    "Deeper Net with ReLU activation.  \n",
    "Trained on multiple GPUs.  \n",
    "LocalResponseNormaliation layer (largely unused)  \n",
    "\n",
    "#### VGG-16 Net. \n",
    "Was a simpler NER with CONV was 3 by 3 (_same_) with s=1 and MAX-POOL was 2 by 2. \n",
    "There first 2 layers were convolutions, then POOL, than again two convolutions than POOL than again two CONV and ... FC and softmax for 1/1000 classes\n",
    "It was 138 million parameters neural net.  \n",
    "Relatively uniform in terms of achitecture.  \n",
    "\n",
    "### ResNets \n",
    "Very deep NNs are built using so-called __skip connections__ to avoid problems of vanishing or exploding gradients.  \n",
    "ResNets are built using __residual blocks__.\n",
    "In a standard net, there is a _main path_ where information has to flow through linear operations (z=wa+b) than activation function and than again linear operation and ... etc (for forward prop).  \n",
    "In ReNet the change in layer $l$ is copied to $l+1$ avoiding applying the linear operation and non-linearity of activation function.  This is __shortcut__. After the activation is $a^{[l+2]} = g(z^{[l+2]}+a^{[l]})$, where $a^{[l]}$ represents the __residual block__.   \n",
    "**Note** Injection is done after the linear part but __before__ the activation functon part.  \n",
    "\n",
    "From 'plane-net' to 'res-net' the way is to add 'skipped connection'.  \n",
    "\n",
    "The training error in 'plane net' at some point reaches minimum and then goes up. \n",
    "\n",
    "In 'res-net' the trainerror __always__ goes down as NN has easier time training and avoids loss in performance. \n",
    "\n",
    "### Why do ResNEts work so well?\n",
    "\n",
    "In ResNet we have $a^{[l+2]} = g(z^{[l+2]}+a^{[l]})$, so when $z^{[l+2]}$ is very close to zero, the g(...) will be very small as well and consequent gradient will be close to zero. However, adding the residual block, we 'offset' the value allowing the activation function to give value that is easier to gradients to be evaluated. Roughly speaking. \n",
    "\n",
    "The identity function is easier for residual block to learn.  \n",
    "\n",
    "Adding a residual block does not hirt but helps performance.  \n",
    "\n",
    "In deep net it is difficult for NN to learn the identnty matrix. ResNet helps. \n",
    "\n",
    "In reals NNs the 'same' convolution commonly used in short cut. \n",
    "\n",
    "It is commot to  add a matrix in front of the residual block that can be learned.  \n",
    "\n",
    "### 1 by 1 convolution\n",
    "\n",
    "Consider a $1\\times1$ filter. The convolution with this filter. It might have _many channels_ as the _volume_ than you do the _sum_ over all channels for a given _filter_. \n",
    "Convolving an input volume with a $1\\times1$ filter gives an array for each value of the 3rd dimension. This is similar to this depth-slice being a layer with each Neuron then being subjectied to a ReLU function. \n",
    "\n",
    "Thus, it is essentially a __fully connected__ layer of NN that is applied to _each_ of the pixel independently to prodice an output image. Different for each filter. \n",
    "\n",
    "`Network in Network` \n",
    "\n",
    "This is used in many NN to, e.g., schring the volume of the NN. If we use $M$ filters shich are all $1\\times1$, then we schrink the colume to this $M$. \n",
    "\n",
    "Also this $1\\times1$ comvolution __adds non-linearity__ and allows to learn a more complex structures. \n",
    "\n",
    "### Inception Network\n",
    "\n",
    "Consider an input image and:\n",
    "- Use $1\\times1$ CONV layer to get output1\n",
    "- Use $M_1\\times M_1$ _same_ CONV layer to get output2\n",
    "- Use $M_2\\times M_2 $ _same_ CONV layer to get output2\n",
    "...\n",
    "- Stack all the output volumes along the 3rd dimension. This __increases__ the final output compare to the original image along 3rd dimension.  \n",
    "There you stuck _many possibilities_ and you do not mage to pick one.  \n",
    "**Note** This increases the _computational cost_ of the model.  \n",
    "\n",
    "**Solution**  Use $1\\times 1$ comvolution to _reduce_ the __initial image__ to a smaller volume. This is called `bottleneck layer`. \n",
    "Shcrinking down does not hurt the NN acuracy much if the layer is chosen carefully. \n",
    "\n",
    "`Inception module` Takes a matrix and applies several operations at the same time. First it applies $1\\times1$ CONV and then a series of $M\\times M$ CONV and at the same time a POOL layer with the _same_ type. This allows to concatanate the outputs. In the case of POOL, to reduce the number of cahnnels, we add another $1\\times1$ layer to reduce the number of channels (the third dimension) -- \n",
    "`Channel concat`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Inception Network` puts many of the _inception blocks_ together.  It may also include intermediate side branches to investigate the features at the intermediate levels. This has an regularizing effect. \n",
    "\n",
    "#### MobileNet\n",
    "\n",
    "Deploy NN that work in a low-compute environment. \n",
    "\n",
    "__Depthwise-separable convolution__ is the solution. \n",
    "\n",
    "Consider an image ahd a filter with $n_c$ channels and the CONV operation. Assume that no padding is used so the image is smaller. \n",
    "Now assume that number of filters is larger $n_c'$. \n",
    "\n",
    "Generally, the computational cost is given by:\n",
    "$$\n",
    "\\text{Computational cost} = \\text{\\# filter params}  \\times \\text{\\# filter positions} \\times \\text{\\# of filters}\n",
    "$$\n",
    "\n",
    "Example:\n",
    "\n",
    "$$\n",
    "\\text{Computational cost} = (3\\times3\\times3)  \\times (4\\times4) \\times 5 = 2160 \n",
    "$$\n",
    "\n",
    "Depth-wise separaragle convolution is a sequience of convolutions: \n",
    "- Depthwise\n",
    "- Pointwise\n",
    "\n",
    "__Depthwise convolution__: given an image with $n_c$ channels and filters with $n_c$ filters, you apply one filter to __one__ channel of the image. This returns an image with $n_c$ depth.  \n",
    "\n",
    "The computational cost is \n",
    "\n",
    "$$\n",
    "\\text{Computational cost} = (3\\times3)  \\times (4\\times4) \\times 3 = 2160 \n",
    "$$\n",
    "\n",
    "__Pointwise Convolution__: convolving the previous result with with $1\\times1$ filter to change the dpeth of the image to what is expected. **NOTE** that we use the $n_c$ for the filter different from the $n_c$ of the image. \n",
    "The computational cost here is \n",
    "\n",
    "$$\n",
    "\\text{Computational cost} = (1\\times1\\times3)  \\times (4\\times4) \\times 5 = 240 \n",
    "$$\n",
    "\n",
    "Note, in both cases we did the convolution that resulted in the same image. The difference is that we split the convolution into depthwise and pointwise steps. The number of caclulations that is needed, however, is $3x$ smaller. \n",
    "Computational saving is \n",
    "$1/n_c' + 1/f^2$\n",
    "where $b_c'$ is the number of channels in the output. \n",
    "\n",
    "> `Depthwise conv` allows to have much more efficient inferences\n",
    "\n",
    "#### Achitecture \n",
    "\n",
    "MobilNetv1 used 13 of the Depthwise CONV and then, POOL, FC, softpax.\n",
    "\n",
    "MobilNetv2 used Residual connections (skip connections) for more efficient gradient propagation. It also  used an expansion layer. The block with these two main stages was repeated 17 times, before passing to POOL, FC. The block was called `bottleneck block`. \n",
    "\n",
    "The `bottleneck block` has the following structure:  \n",
    "- Residual part, where the input is being passed forward to the outpu\n",
    "- Main part: 1) expansion operator ($1\\times1$ with large $n_c$) (usually factor of exapnsion is 6). Then, the depthwise separable convolution with the output being with the same dimension. then, the point-wise convolution with the $n_c'$ being smaller. This last step is called __projection__ step, as the larger representaion is being projected into a smaller one. So It is like \"expansion-convolution-projection\" sequence.  \n",
    "Here the _expansion_ operation allows the NN to learn a richer function by expanding the input and giving more degrees of freedom. Due to memory constraints then, the _depthwise\\&point-wise_ is used and the memory of the output is reduced by schrinking the output using _projection_\n",
    "\n",
    "### EfficientNet\n",
    "\n",
    "How to automatically scale a NN upo and down depending on the devise.  \n",
    "Adjustments\n",
    "- Image resolution $R$\n",
    "- Depth of the NN $D$\n",
    "- Width of the layers $W$\n",
    "\n",
    "Or compound scaling.\n",
    "\n",
    "##### Using open-source implementation\n",
    "\n",
    "Saee ResNets in GitHub...  \n",
    "See pre-train nets...  \n",
    "Use transfer learning...  Freeze some layers to keep them trained and reduce calc. time. \n",
    "The larger the dataset the more data can you learn.  \n",
    "Usually, the output layer has to be replaced.  \n",
    "\n",
    "Data augmentation for CNNs is pretty common but complicated.  \n",
    "Example: mirrorwing, random cropping, rotation, shearning, local warping, ... color shifting (using PCA, PCA color augmentation)\n",
    "\n",
    "\n",
    "### State:\n",
    "\n",
    "- Image recognition (less data)\n",
    "- Speach recognition (more data)\n",
    "- Object detection (not much data due to bounding boxes limits)\n",
    "\n",
    "Simpler algirhtms for a lot of data.  \n",
    "More hand-engineered data for small data  and transfer learning helps\n",
    "\n",
    "For benchmarks: \n",
    "- ensembling - averaging by hand outputs from multiple outputs\n",
    "\n",
    "Finished assignment 1  \n",
    "Finished assignemnt 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
