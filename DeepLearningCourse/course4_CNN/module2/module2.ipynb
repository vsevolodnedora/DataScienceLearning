{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Studies\n",
    "\n",
    "- LeNet-5\n",
    "- AlexNet\n",
    "- VGG\n",
    "\n",
    "Residual Networks (152 layer net)\n",
    "\n",
    "Inception\n",
    "\n",
    "### Classic Netowrk achitectture\n",
    "\n",
    "#### LeNet-5 \n",
    "was made to analyze gray-scale images on  numbers. \n",
    "Did not use padding. Each layer was schrinking the image.  \n",
    "Used average pooling.  \n",
    "Number of channels increases as you goo deeper. \n",
    "Usually CONV layer is followed by the POOL layer. \n",
    "\n",
    "`LeCun et al 1998 Grad.Based learning`\n",
    "\n",
    "Graph transfprming Network\n",
    "\n",
    "#### AlexNet \n",
    "was a bigger net with 227by 227 image. \n",
    "Image->Conv->MaxPool->Conv->MaxPool->...->FC->SoftMax (for classification)  \n",
    "Deeper Net with ReLU activation.  \n",
    "Trained on multiple GPUs.  \n",
    "LocalResponseNormaliation layer (largely unused)  \n",
    "\n",
    "#### VGG-16 Net. \n",
    "Was a simpler NER with CONV was 3 by 3 (_same_) with s=1 and MAX-POOL was 2 by 2. \n",
    "There first 2 layers were convolutions, then POOL, than again two convolutions than POOL than again two CONV and ... FC and softmax for 1/1000 classes\n",
    "It was 138 million parameters neural net.  \n",
    "Relatively uniform in terms of achitecture.  \n",
    "\n",
    "### ResNets \n",
    "Very deep NNs are built using so-called __skip connections__ to avoid problems of vanishing or exploding gradients.  \n",
    "ResNets are built using __residual blocks__.\n",
    "In a standard net, there is a _main path_ where information has to flow through linear operations (z=wa+b) than activation function and than again linear operation and ... etc (for forward prop).  \n",
    "In ReNet the change in layer $l$ is copied to $l+1$ avoiding applying the linear operation and non-linearity of activation function.  This is __shortcut__. After the activation is $a^{[l+2]} = g(z^{[l+2]}+a^{[l]})$, where $a^{[l]}$ represents the __residual block__.   \n",
    "**Note** Injection is done after the linear part but __before__ the activation functon part.  \n",
    "\n",
    "From 'plane-net' to 'res-net' the way is to add 'skipped connection'.  \n",
    "\n",
    "The training error in 'plane net' at some point reaches minimum and then goes up. \n",
    "\n",
    "In 'res-net' the trainerror __always__ goes down as NN has easier time training and avoids loss in performance. \n",
    "\n",
    "### Why do ResNEts work so well?\n",
    "\n",
    "In ResNet we have $a^{[l+2]} = g(z^{[l+2]}+a^{[l]})$, so when $z^{[l+2]}$ is very close to zero, the g(...) will be very small as well and consequent gradient will be close to zero. However, adding the residual block, we 'offset' the value allowing the activation function to give value that is easier to gradients to be evaluated. Roughly speaking. \n",
    "\n",
    "The identity function is easier for residual block to learn.  \n",
    "\n",
    "Adding a residual block does not hirt but helps performance.  \n",
    "\n",
    "In deep net it is difficult for NN to learn the identnty matrix. ResNet helps. \n",
    "\n",
    "In reals NNs the 'same' convolution commonly used in short cut. \n",
    "\n",
    "It is commot to  add a matrix in front of the residual block that can be learned.  \n",
    "\n",
    "### 1 by 1 convolution\n",
    "\n",
    "Consider a $1\\times1$ filter. The convolution with this filter. It might have _many channels_ as the _volume_ than you do the _sum_ over all channels for a given _filter_. \n",
    "Convolving an input volume with a $1\\times1$ filter gives an array for each value of the 3rd dimension. This is similar to this depth-slice being a layer with each Neuron then being subjectied to a ReLU function. \n",
    "\n",
    "Thus, it is essentially a __fully connected__ layer of NN that is applied to _each_ of the pixel independently to prodice an output image. Different for each filter. \n",
    "\n",
    "`Network in Network` \n",
    "\n",
    "This is used in many NN to, e.g., schring the volume of the NN. If we use $M$ filters shich are all $1\\times1$, then we schrink the colume to this $M$. \n",
    "\n",
    "Also this $1\\times1$ comvolution __adds non-linearity__ and allows to learn a more complex structures. \n",
    "\n",
    "### Inception Network\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
