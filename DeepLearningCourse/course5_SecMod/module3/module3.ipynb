{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VARIOS SEQUENCE TO SEQUENCE ARCHITECTURES\n",
    "\n",
    "### Basic models\n",
    "\n",
    "- machine translation: Input $\\{x^{<1>}\\}$, $\\{x^{<2>}\\}$ and output are $\\{y^{<1>}\\}$ ... $\\{y^{<2>}\\}$.  \n",
    "\n",
    "Start by building an incput, `encoder network` (RNN (GRU or LSTM), that takes one word at a time). After that build a `decoder netword` that outputs a sequence $\\{y^{<1>}\\}$ ... $\\{y^{<2>}\\}$.\n",
    "\n",
    "This model words given enough pairs of sentences of different words.  This is `encoder-decoder` network.  \n",
    "\n",
    "A similar architecture words for image captioning. \n",
    "The netword takes image via CNN (pre-train net), an learn a set of feautres of an image. It returns a dense layer with a lot of units. This can be a _encoder netword_. This can then be fed into _RNN_, a _decoder network_ to generate a caption. This works reasonably well.  \n",
    "\n",
    "These are basic seq-to-seq.  but it gives a randomly chosen output, not _the most likely output_.  \n",
    "\n",
    "### Picking the most likely sequence\n",
    "\n",
    "Machine trasnlation as a conditional language model.  \n",
    "\n",
    "A language model returns a probability of a given output.  \n",
    "\n",
    "- Conditional language model. In a classical language model the inputs $a^{<0>}$ are set as zero. Here we use the output of the _encoder NN_ to _condition_ the language model. Thus, machine translation becomes _encoder and decoder_ NNs. The _decoder_ NN is has an output similar to the _language model_ but the _encoder_ is used to condition it. \n",
    "\n",
    "The model gives probabilities of possible translation sentences.  It is important to _avoid sampling randomly_ from the distribution of outputs.  \n",
    "Find a sentence, output, that maximises the conditional ptobability $x$.  This is done via `beam search`. \n",
    "The `gready search` _is not generally used_ (where word by word are picked based on their probabilities). Instead, _joint probabilities_ of _all words in the sentence_ is used. \n",
    "\n",
    "Also, _approximate search_ is used as the sample of search is too large.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam search\n",
    "\n",
    "Choosing the best ouput from LLMs.  \n",
    "\n",
    "Beam search picks first the first most likely word.  \n",
    "Chose the probability of various outputs given an input. The Search outputs _several_ likely outputs. The number is given by $B$ variable.  \n",
    "\n",
    "Next, for each of the _selected_ outputs, consider the probabilities for the next word (remember we feed the previous word into the next layer of the _decoder_, so for each previous output, the next one will change).  \n",
    "\n",
    "Now we collect pairs of first-second words that are most likely. The selsection is done as \n",
    "$$\n",
    "P(y^{<1>},y^{<2>}|x) = P(y^{<1>}|x)P(y^{<2>}|x,y^{<1>})\n",
    "$$\n",
    "The same is done for the third etc word. This allows to select the _most probable setnece_ rather than word by word, contrary to the _greedy search_ (Note, at $B=1$ it is a greedy search)\n",
    "\n",
    "Beam search aims to maximize \n",
    "\n",
    "$$\n",
    "\\text{arg max} \\,\\Pi_{t=1}^{T_y} P(y^{<t>}|x,y^{<1>}...y^{<t-1>})\n",
    "$$\n",
    "\n",
    "### Refinments to beam search \n",
    "- Length normalization\n",
    "\n",
    "More numerically stable algorithm can be obtained by considering $\\sum\\log(P(...))$ instead of product of probabilities.  \n",
    "\n",
    "This function however, prefers short sentences, as it is simpler to optimize for them.  \n",
    "A common approach is to add \n",
    "$$\n",
    "\\text{arg max} \\frac{1}{T_y^{\\alpha}} \\,\\sum_{t=1}^{T_y} \\log P(y^{<t>}|x,y^{<1>}...y^{<t-1>})\n",
    "$$\n",
    "\n",
    "where the first time here accounts for the equation bias for short sentences. \n",
    "\n",
    "This a _heuristic approach_. \n",
    "\n",
    "IF $B$ is large -- it is slower, more memory needed, but it is more accurate. \n",
    "If $B$ is small, -- less accurate\n",
    "\n",
    "In practice, $B\\in(10-100)$\n",
    "\n",
    "Beam search is _not exact search_ lke BFS or DFS (search algorithms)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis in Beam Search\n",
    "\n",
    "A model has two components: RNN and BeamSearch algorithm  \n",
    "The error in translation can be attributed to either RNN or the beam search algorithm.  \n",
    "\n",
    "Increasing the Beam Width may not increase the performance, as inclreasing the training dataset may not achieve this.  \n",
    "\n",
    "RNN computes $P(y|x)$.  \n",
    "Compare the result of RNN with human translation.  \n",
    "Beam search choses $\\hat{y}$. But $y^*$ gives higher $P(y|x)$, $P(y^*|x) > P(\\hat{y}|x)$ Then, _beacm search fails_ as it fails to give the highest probability.  \n",
    "If on the other hand $P(y^*|x) < P(\\hat{y}|x)$, then RNN is the problem. \n",
    "\n",
    "Go through the dev.set and find the mistackes that algorithm made. \n",
    "\n",
    "Compare the probabiltiies for $*$ and $\\hat{}$ and what gives more error, beam search or RNN.  \n",
    "Then the error analysis is what fraction attributed to different algorithm.  \n",
    "\n",
    "The beam search is worth checking only if it is responsible for most of the errors. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bleu Score (Bilingual evaluation)\n",
    "\n",
    "- Single number evaluation metric\n",
    "- System to evalaute the text\n",
    "- _Not_ for speach recognition\n",
    "\n",
    "Evaluating the machine translation when there are many valid answers.\n",
    "\n",
    "Bleu score is the score that assess how close they are to human level.  \n",
    "\n",
    "__PRecision__: how close each word to the expcted. Not usefull. \n",
    "__Modified Precision__: a word has a maximum number of times to be there. So, the word gives a cridit $2/7$, where number of appearances/number of words in the sentence.  \n",
    "\n",
    "`Bigrams` - pairs of words appearining. \n",
    "\n",
    "Aan algorithm can compare these pairs in mahine outputs\n",
    "\n",
    "$$\n",
    "P_1 = \\frac{\\sum\\text{count(unigram)}}{\\sum\\text{count(unigram)}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_n = \\frac{\\sum_{n-gram}\\text{count(n-gram)}}{\\sum_{n-gram}\\text{count(n-gram)}}\n",
    "$$\n",
    "\n",
    "Bleu $p_n$ is the Bleu score on $n$-grams only.  \n",
    "\n",
    "$$\n",
    "BP\\exp( .25 \\sum_{n=1}^4 p_n )\n",
    "$$\n",
    "BP -s the gravity penalty to penalize the outputs that are too short.  \n",
    "\n",
    "\n",
    "### Attention model intution\n",
    "\n",
    "In the usual NN, the enire sentence is read, stored and translated.  \n",
    "A human, however, read the text part by part.  \n",
    "\n",
    "In encoder-decoder the problems thus begin when setneces are too large. Hard to translate them.  \n",
    "\n",
    "Attention model performs part-by-part translation/analysis of the text.  \n",
    "\n",
    "Developed in 2014 and is now used in many applications.  \n",
    "\n",
    "Consider a _bidirectional RNN_.  \n",
    "The translation is generated by another RNN with a hidden state $S^{<0>}$. The question is, in order to generate the first output, _what part of the sentence to consider_.  \n",
    "The model takes `attention weights` that assess how much _attention_ is payed to a given part of the input setnece. \n",
    "\n",
    "For a new part of the sentence, $S^{<2>}$, there is a _new set of weights_ and is also an input from the previous output.  \n",
    "\n",
    "Similar for the next step.  \n",
    "\n",
    "The __context__, what part of the sentence to consider.  \n",
    "So inputs now are: activations, weights, previous input. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Model\n",
    "\n",
    "Consider an input sentence and a _bidirectional RNN_.  \n",
    "In each hidden state (for each timestep) there are activations, features $a^{\\rightarrow <i>}$ and $a^{\\leftarrow <i>}$, \n",
    "where the first timestep takes $a^{\\rightarrow <0>} = \\vec{0}$.  \n",
    "Assume $a^{<t'>} = (a^{\\rightarrow <i>},a^{\\rightarrow <i>})$, the _feature vector for timestep $t'$ in the original sequence.  \n",
    "\n",
    "Then consider a _forward only RNN_ for translation, that \n",
    "takes as input $s^{<0>}$, and context $c$ has a hidden state $s^{<i>}$ and outputs $y^{<i>}$. \n",
    "\n",
    "The $c$ depends on the attention parameters $\\alpha^{o,i}$, where $o$ is the current $s$ and $i$ is the input (from all states of the previos, bidirectional NN)\n",
    "\n",
    "The `context` then is the _weighted sum_ of features, by attention weights. \n",
    "Normalization:  \n",
    "$$\n",
    "\\sum\\alpha^{<1,t'>} = 1\n",
    "$$\n",
    "\n",
    "And the `context vectors` read  \n",
    "$$\n",
    "c^{<1>} = \\sum_{t'}\\alpha^{<1,t'>}a^{<t'>}\n",
    "$$\n",
    "\n",
    "where $\\alpha^{<1,t'>}$ is the amount of _attention_ that $y^{<t>}$ should pay to $a^{<t'>}$.  \n",
    "\n",
    "At the next timestep, the output is generated similarly, but taking the output from the previous one as an input.  \n",
    "This part of _one-directional_ RNN. \n",
    "\n",
    "So, the $s$ network is similar to classical RNN. \n",
    "\n",
    "\n",
    "#### Calculation of the attention $a^{<t,t'>}$  \n",
    "\n",
    "Recall that $\\alpha^{<1,t'>}$ is the amount of _attention_ that $y^{<t>}$ should pay to $a^{<t'>}$.  \n",
    "\n",
    "$$\n",
    "a^{<t,t'>} = \\frac{\\exp{e^{<t,t'>}}}{\\sum_{t'=1}^{T_x}\\exp e^{<t,t'>}}\n",
    "$$\n",
    "\n",
    "where $e$ are the weights, that sum-up to one ober $t'$.  \n",
    "Factors $e$ are computed using small NN by passing $s^{<t-1>}$ and $a^{<t'>}$ into a _one-hidden-layer_ NN to get $e^{<t,t'>}$ (it approximates the function that we do not know).  \n",
    "\n",
    "Here $ee^{<t,t'>}$ are the $\\alpha^{<t,t'>}$.  \n",
    "\n",
    "_Disadvantages_:  \n",
    "The cost of algorithm is __quadaratic__. \n",
    "The __total number of parameters__ is $T_x\\times T_y$.  \n",
    "\n",
    "In machine translation it is generally acceptable.  \n",
    "\n",
    "_Other application_:  \n",
    "- Image caption  \n",
    "\n",
    "Visualization of the $\\alpha$ can help to find where the attention is high.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speach Recognition  \n",
    "\n",
    "Given an audio clip $x$ and create a transcript $y$.  \n",
    "The audio clip is the pressure versus time.  \n",
    "\n",
    "Spectrogram is the way to examine the audio.  \n",
    "\n",
    "_Old approach_: create uints of sound, `phones`, and discretize the sound into them  \n",
    "_New approach_: Deep learning.  \n",
    "\n",
    "Usual datasets $\\sim300$-hours or $100.000$ hours for large industry systems.  \n",
    "\n",
    "Approaches: \n",
    "- attention + LSTM model.  \n",
    "- Connectionist temporal calssification (CTC)  \n",
    "\n",
    "The idea is:  \n",
    "Consider a bi-directional LSTM with equal amount of inputs $x^{<N>}$ and outputs $y^{<N>}$.  \n",
    "\n",
    "Usually, the amount of intput data, the frequency, is _very large_.  The CTC allows to generate the sequence in a form  \n",
    "$\\texttt{ttt\\_h\\_eee\\_\\_\\_ \\_\\_\\_qqq\\_\\_}$.  \n",
    "\n",
    "This is considered to be the correct ouputput of the first part, for a word $\\texttt{the}$.  \n",
    "The basic rule is to _collapse the repeated caracters_ that are __not separated__ by the _empty space_.  \n",
    "This allows to separate words and have a _shorted output_.  \n",
    "\n",
    "### Trigger word detection  \n",
    "\n",
    "This can be accomplished with even a small dataset (contrary to the large speach recognition).  \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finished exercsie 1,2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
