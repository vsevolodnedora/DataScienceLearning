{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VARIOS SEQUENCE TO SEQUENCE ARCHITECTURES\n",
    "\n",
    "### Basic models\n",
    "\n",
    "- machine translation: Input $\\{x^{<1>}\\}$, $\\{x^{<2>}\\}$ and output are $\\{y^{<1>}\\}$ ... $\\{y^{<2>}\\}$.  \n",
    "\n",
    "Start by building an incput, `encoder network` (RNN (GRU or LSTM), that takes one word at a time). After that build a `decoder netword` that outputs a sequence $\\{y^{<1>}\\}$ ... $\\{y^{<2>}\\}$.\n",
    "\n",
    "This model words given enough pairs of sentences of different words.  This is `encoder-decoder` network.  \n",
    "\n",
    "A similar architecture words for image captioning. \n",
    "The netword takes image via CNN (pre-train net), an learn a set of feautres of an image. It returns a dense layer with a lot of units. This can be a _encoder netword_. This can then be fed into _RNN_, a _decoder network_ to generate a caption. This works reasonably well.  \n",
    "\n",
    "These are basic seq-to-seq.  but it gives a randomly chosen output, not _the most likely output_.  \n",
    "\n",
    "### Picking the most likely sequence\n",
    "\n",
    "Machine trasnlation as a conditional language model.  \n",
    "\n",
    "A language model returns a probability of a given output.  \n",
    "\n",
    "- Conditional language model. In a classical language model the inputs $a^{<0>}$ are set as zero. Here we use the output of the _encoder NN_ to _condition_ the language model. Thus, machine translation becomes _encoder and decoder_ NNs. The _decoder_ NN is has an output similar to the _language model_ but the _encoder_ is used to condition it. \n",
    "\n",
    "The model gives probabilities of possible translation sentences.  It is important to _avoid sampling randomly_ from the distribution of outputs.  \n",
    "Find a sentence, output, that maximises the conditional ptobability $x$.  This is done via `beam search`. \n",
    "The `gready search` _is not generally used_ (where word by word are picked based on their probabilities). Instead, _joint probabilities_ of _all words in the sentence_ is used. \n",
    "\n",
    "Also, _approximate search_ is used as the sample of search is too large.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam search\n",
    "\n",
    "Choosing the best ouput from LLMs.  \n",
    "\n",
    "Beam search picks first the first most likely word.  \n",
    "Chose the probability of various outputs given an input. The Search outputs _several_ likely outputs. The number is given by $B$ variable.  \n",
    "\n",
    "Next, for each of the _selected_ outputs, consider the probabilities for the next word (remember we feed the previous word into the next layer of the _decoder_, so for each previous output, the next one will change).  \n",
    "\n",
    "Now we collect pairs of first-second words that are most likely. The selsection is done as \n",
    "$$\n",
    "P(y^{<1>},y^{<2>}|x) = P(y^{<1>}|x)P(y^{<2>}|x,y^{<1>})\n",
    "$$\n",
    "The same is done for the third etc word. This allows to select the _most probable setnece_ rather than word by word, contrary to the _greedy search_ (Note, at $B=1$ it is a greedy search)\n",
    "\n",
    "Beam search aims to maximize \n",
    "\n",
    "$$\n",
    "\\text{arg max} \\,\\Pi_{t=1}^{T_y} P(y^{<t>}|x,y^{<1>}...y^{<t-1>})\n",
    "$$\n",
    "\n",
    "### Refinments to beam search \n",
    "- Length normalization\n",
    "\n",
    "More numerically stable algorithm can be obtained by considering $\\sum\\log(P(...))$ instead of product of probabilities.  \n",
    "\n",
    "This function however, prefers short sentences, as it is simpler to optimize for them.  \n",
    "A common approach is to add \n",
    "$$\n",
    "\\text{arg max} \\frac{1}{T_y^{\\alpha}} \\,\\sum_{t=1}^{T_y} \\log P(y^{<t>}|x,y^{<1>}...y^{<t-1>})\n",
    "$$\n",
    "\n",
    "where the first time here accounts for the equation bias for short sentences. \n",
    "\n",
    "This a _heuristic approach_. \n",
    "\n",
    "IF $B$ is large -- it is slower, more memory needed, but it is more accurate. \n",
    "If $B$ is small, -- less accurate\n",
    "\n",
    "In practice, $B\\in(10-100)$\n",
    "\n",
    "Beam search is _not exact search_ lke BFS or DFS (search algorithms)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis in Beam Search\n",
    "\n",
    "A model has two components: RNN and BeamSearch algorithm  \n",
    "The error in translation can be attributed to either RNN or the beam search algorithm.  \n",
    "\n",
    "Increasing the Beam Width may not increase the performance, as inclreasing the training dataset may not achieve this.  \n",
    "\n",
    "RNN computes $P(y|x)$.  \n",
    "Compare the result of RNN with human translation.  \n",
    "Beam search choses $\\hat{y}$. But $y^*$ gives higher $P(y|x)$, $P(y^*|x) > P(\\hat{y}|x)$ Then, _beacm search fails_ as it fails to give the highest probability.  \n",
    "If on the other hand $P(y^*|x) < P(\\hat{y}|x)$, then RNN is the problem. \n",
    "\n",
    "Go through the dev.set and find the mistackes that algorithm made. \n",
    "\n",
    "Compare the probabiltiies for $*$ and $\\hat{}$ and what gives more error, beam search or RNN.  \n",
    "Then the error analysis is what fraction attributed to different algorithm.  \n",
    "\n",
    "The beam search is worth checking only if it is responsible for most of the errors. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bleu Score (Bilingual evaluation)\n",
    "\n",
    "Evaluating the machine translation when there are many valid answers.\n",
    "\n",
    "Bleu score is the score that assess how close they are to human level.  \n",
    "\n",
    "__PRecision__: how close each word to the expcted. Not usefull. \n",
    "__Modified Precision__: a word has a maximum number of times to be there. So, the word gives a cridit $2/7$, where number of appearances/number of words in the sentence.  \n",
    "\n",
    "`Bigrams` - pairs of words appearining. \n",
    "\n",
    "Aan algorithm can compare these pairs in mahine outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
