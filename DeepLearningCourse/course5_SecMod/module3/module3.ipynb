{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VARIOS SEQUENCE TO SEQUENCE ARCHITECTURES\n",
    "\n",
    "### Basic models\n",
    "\n",
    "- machine translation: Input $\\{x^{<1>}\\}$, $\\{x^{<2>}\\}$ and output are $\\{y^{<1>}\\}$ ... $\\{y^{<2>}\\}$.  \n",
    "\n",
    "Start by building an incput, `encoder network` (RNN (GRU or LSTM), that takes one word at a time). After that build a `decoder netword` that outputs a sequence $\\{y^{<1>}\\}$ ... $\\{y^{<2>}\\}$.\n",
    "\n",
    "This model words given enough pairs of sentences of different words.  This is `encoder-decoder` network.  \n",
    "\n",
    "A similar architecture words for image captioning. \n",
    "The netword takes image via CNN (pre-train net), an learn a set of feautres of an image. It returns a dense layer with a lot of units. This can be a _encoder netword_. This can then be fed into _RNN_, a _decoder network_ to generate a caption. This works reasonably well.  \n",
    "\n",
    "These are basic seq-to-seq.  but it gives a randomly chosen output, not _the most likely output_.  \n",
    "\n",
    "### Picking the most likely sequence\n",
    "\n",
    "Machine trasnlation as a conditional language model.  \n",
    "\n",
    "A language model returns a probability of a given output. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
