{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Network\n",
    "\n",
    "We started with __RNN__ that had problems with vanishing gradients.  \n",
    "Solutions were __GRU__ and __LSTM__ that included gates, that accounted for the flow of information.  \n",
    "This led to the increase in complexity.  \n",
    "However, each of these models are _sequential_. The information flows from one unit to another.  \n",
    "Output of the next element requires the result from the previous.  \n",
    "\n",
    "Transformer network was published in 2017 in paper _attention is all you need_.  \n",
    "> Key: use combiniation of attention and CNNs  \n",
    "\n",
    "In RNN we output $y^{<0>}$ _sequentionally_.  In CNN, the putput is done simultaneously for many units.  \n",
    "\n",
    "- Self-Attention (If there are many tokens, and many represenations are computed, this is atention based wave)\n",
    "- Multi-Head Attention (several versions of representations)\n",
    "\n",
    "### Self-Attention\n",
    "\n",
    "> Attention mechanisms in CNNs requires _self-attention_.\n",
    "\n",
    "This is _attention-based_ representation of each word: \n",
    "$A(q,K,V)$.  \n",
    "Such represeantion is computed for each word.  \n",
    "Word embeddings cannot be used for represenation due to different context of the word usage. So the most appropriate _represeantion_ has to be determined from the _context_.\n",
    "\n",
    "Transofrmer Attention \n",
    "\n",
    "$$\n",
    "A(q,K,V) = \\sum_i \\frac{\\exp(q\\cdot k^{<i>})}{\\sum_j\\exp(q\\cdot k^{<j>})}v^{<i>}\n",
    "$$\n",
    "\n",
    "where $v$ is the value for a given word.  \n",
    "first part is the _soft-max_ over the $q\\cdot k$ showing the importa part of the context.  \n",
    "\n",
    "For each word there are three _vectors_ to compute attention:\n",
    "- $q^{<i>}$ - _Query_ - $q^{<3>} = w^{\\theta} \\cdot x^{<i>}$. Represents the question 'what's happend', the dot product $q^{<i>} \\cdot k^{<i>}$ gives the answer. This establishes a _connection_ between words in _context_, how related they are. \n",
    "- $k^{<i>}$ - _Key_ - $k^{<3>} = w^{K} \\cdot x^{<i>}$\n",
    "- $v^{<i>}$ - _Value_ - $v^{<3>} = w^{V} \\cdot x^{<i>}$\n",
    "\n",
    "They are named in analogy _databases_.  \n",
    "\n",
    "Computing inner products between $q^{<i>}$ and $k^{<j>}$, _connects_ parts of context, e.g., _how good/relevant/connected/attributed to a given word to another_. \n",
    "\n",
    "Building these inner products between a word and all the others allows to establish all relevant connects.\n",
    "\n",
    "The largest value among all these gives _what to pay attention to_. _Most relevant context_ is obtained via __Soft-max__ in the _denominator_ of the formula. \n",
    "\n",
    "Finally, we sum all soft-max outputs to get $A^{<i>} = A(q^{<i>},k,v)$.  \n",
    "\n",
    "> Achieved representaion is different from _fixed word-embedding_. The _represenation depends on the context_. \n",
    "\n",
    "Literature representaiton: \n",
    "$$\n",
    "\\text{Attention}(Q,K,V) = softmax\\Big(\\frac{QT^{T}}{\\sqrt{d_k}}\\Big)V\n",
    "$$\n",
    "\n",
    "where $d_k$ is the scaling factor for the dot-product. \n",
    "\n",
    "> Original Name: scaled dot-product attention \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
