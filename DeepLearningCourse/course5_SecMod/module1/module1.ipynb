{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Seq. Modelling\n",
    "\n",
    "Examples:\n",
    "- Speach recognition\n",
    "- Music generation\n",
    "- Sentiment classification\n",
    "- DNA sequence analysis\n",
    "- Machine translation\n",
    "- Video activity recognition\n",
    "- Name entity recognition \n",
    "\n",
    "Output or input may be sequence only\n",
    "\n",
    "### Notation\n",
    "\n",
    "Consider a sentnece (seq. of words)  \n",
    "$x$ is a sentence. \n",
    "Find where are people's names in the problem: \n",
    "`Named-entity recognition` used by search engines (indexing the words for fast search).  \n",
    "\n",
    "Do one-hot encoding for names and not names (labels are 1 or 0, $y$-vector)\n",
    "\n",
    "Indexes are $x^{<i>}$ or $x^{<t>}$ with length $T_x$ for temporal index  \n",
    "and outputs are $y^{<t>}$ with length $T_y$.  \n",
    "\n",
    "For various sets the length can be different. \n",
    "In particular $T_x^{(i)}$ and $T_y^{(i)}$\n",
    "\n",
    "Representing words is done making a dictionary or vocabluary. \n",
    "\n",
    "Dictionaries are set of indexed words with large number of them. Then, each word is a vector the length of the whole vocabluary with one-hot 1 for this word and 0 for the rest. These are `one-hot` vectors. \n",
    "\n",
    "The goal is to learn mapping the mapping to the target output $Y$ using this one-hot vector via `supervised learning`. \n",
    "\n",
    "If the word is not in the vocabluary, use a  `UNK` (not in the vocabluary workd)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network Model\n",
    "\n",
    "One approach iis to use standard NN approach, taking a set of one-hot vectors, building a FC layers and predicting output. \n",
    "Problems:\n",
    "- With various length for different words/sentences\n",
    "- Features are not shared across different positions in the text (i.e, repeating words) this is similar to CNN\n",
    "- input layers are extemely large with very large weight matrixes\n",
    "\n",
    "Proper approach:\n",
    "\n",
    "Consider $x^{<1>}$ input vector, and predict $y^{<1>}$.  \n",
    "In RecNN, for the second input word, $^{<2>}$, the information from previous $x^{<1>}$ is used to predict $^y{<2>}$. So in the RecNet, the timesteps are \"chained\" via passing the information from previous step to the next.  \n",
    "Also, there is $x^{<0>}$ is added which is initialized as zeros.  \n",
    "$x^{<t>} \\rightarrow y^{<t>}$.  \n",
    "RecNet scans the data from left to right. The parameters are shared across timesteps. \n",
    "So $w_{ax}$, $w_{aa}$ and $w_{ay}$, controlling the input to the layer from $x$, previous layer, and output respectively are __sahred__.  \n",
    "\n",
    "When making a porediction for $y^{<3>}$, the information from $x^{<1>}$, $x^{<2>}$, $x^{<3>}$ is used.  \n",
    "\n",
    "_Limitaiton_: In this RNN only the info from earlier in the sequence can be used to make a prediction.  \n",
    "\n",
    "_Solution_: bidiractional (BRNN)\n",
    "\n",
    "#### Example of the RNN calculation\n",
    "\n",
    "$$\n",
    "a^{<0>}\\rightarrow\n",
    "\\overbrace{\\underbrace{\n",
    "\\begin{bmatrix}\n",
    "\\cdot \\\\\n",
    "\\cdot \\\\\n",
    "\\cdot \\\\\n",
    "\\end{bmatrix}}_{x^{<1>}}}^{\\hat{y}^{<1>}}\n",
    "\\overbrace{\\rightarrow}^{a^{<1>}}\n",
    "\\overbrace{\\underbrace{\n",
    "\\begin{bmatrix}\n",
    "\\cdot \\\\\n",
    "\\cdot \\\\\n",
    "\\cdot \\\\\n",
    "\\end{bmatrix}}_{x^{<2>}}}^{\\hat{y}^{<2>}}\n",
    "\\overbrace{\\rightarrow}^{a^{<2>}}\n",
    "\\overbrace{\\underbrace{\n",
    "\\begin{bmatrix}\n",
    "\\cdot \\\\\n",
    "\\cdot \\\\\n",
    "\\cdot \\\\\n",
    "\\end{bmatrix}}_{x^{<3>}}}^{\\hat{y}^{<3>}}\n",
    "\\overbrace{\\rightarrow}^{a^{<3>}}\n",
    "\\cdots\n",
    "\\overbrace{\\rightarrow}^{a^{<T_x-1>}}\n",
    "\\overbrace{\\underbrace{\n",
    "\\begin{bmatrix}\n",
    "\\cdot \\\\\n",
    "\\cdot \\\\\n",
    "\\cdot \\\\\n",
    "\\end{bmatrix}}_{x^{<T_x>}}}^{\\hat{y}^{<T_y>}}\n",
    "$$\n",
    "\n",
    "__Forward prop__:  \n",
    "$$\n",
    "a^{<1>} = g(w_{aa} a^{<0>} + w_{ax} x^{<1>} + b_a)\n",
    "$$\n",
    "and the prediction then \n",
    "$$\n",
    "\\hat{y}^{<1>} = g(w_{ya} a^{<1>} + b_y)\n",
    "$$\n",
    "\n",
    "where the activation function can be different!\n",
    "\n",
    "_Notations_: $w_{ax}$ means that it will be multipled by $x$-like and be used to output $a$-like quantity.  \n",
    "\n",
    "Activations: $\\tan{(z)}$ or rarely ReLU.  \n",
    "\n",
    "Here there are other ways to prevent vanishing gradient.  \n",
    "The chose of final activation function is determined by the task.  \n",
    "\n",
    "At time $t$\n",
    "\n",
    "$$\n",
    "a^{<t>} = g(w_{aa} a^{<t-1>} + w_{ax} x^{<t>} + b_a) \\\\\n",
    "\\hat{y}^{<t>} = g(w_{ya} a^{<t>} + b_y)\n",
    "$$\n",
    "\n",
    "Let us re-express is as\n",
    "\n",
    "$$\n",
    "a^{<t>} = g(w_a[a^{<t-1>},x^{<t>}] + b_a) \\\\ \n",
    "\\hat{y}^{<t>} = g(w_{ya} a^{<t>} + b_y)\n",
    "$$\n",
    "\n",
    "where the matrix $w_a$ is constructed by _horizontally stacking_ [$w_{aa} | w_{ax}$]. \n",
    "I.e, if $a$ is 100 dim, and $x$ is 10000 dim, than  \n",
    "$w_{aa}$ is [100,100] and $w_{ax}$ is[100,10000],  \n",
    "so that stacking them along the _common axis_ we get [100, 10100] matrix. \n",
    "\n",
    "This $[a^{<t-1>},x^{<t>}]$ implies a __stacking__ operation \n",
    "$$\n",
    "[a^{<t-1>},x^{<t>}] = \n",
    "\\begin{bmatrix}\n",
    "a^{<t-1>} \\\\\n",
    "x^{<t>}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where the \"hight\" of the final vector is given by the sum of the their hight. \n",
    "\n",
    "Then the original quantity is recomvered by multiplied the \n",
    "$$\n",
    "[w_{aa} | w_{ax}] \\times \n",
    "\\begin{bmatrix}\n",
    "a^{<t-1>} \\\\\n",
    "x^{<t>}\n",
    "\\end{bmatrix}\n",
    "=w_{aa} a^{<t-1>} + w_{ax} x^{<t>}\n",
    "$$\n",
    "\n",
    "This allows to compress into one parameter matrix $w_a$. \n",
    "\n",
    "Finally, the forward prop reduces to \n",
    "\n",
    "$$\n",
    "\\hat{y}^{<t>} = g(w_{y} a^{<t>} + b_y)\n",
    "$$\n",
    "\n",
    "where $w_{ay}\\rightarrow w_{y}$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Prop. \n",
    "\n",
    "Calculations in the oposite direction.  \n",
    "\n",
    "Define loss function for a given prediction, an element in the sequence, to be the wright or wrong. Consider stanrd _logist regression_ loss (aka cross-entropy  loss). \n",
    "$$\n",
    "\\mathcal{L}^{<t>}(\\hat{y}^{<t>},y^{<t>}) = -y^{<t>} \\log(\\hat{y}^{<t>}) - (1-y^{<t>}) \\log(1-y^{<t>})\n",
    "$$\n",
    "\n",
    "The loss for the entire sequence is a sum of the losses: \n",
    "$$\n",
    "\\mathcal{L} = \\sum_{t=1}^{T_x=T_y} \\mathcal{L}^{<t>}(\\hat{y}^{<t>},y^{<t>}) \n",
    "$$\n",
    "\n",
    "Backprop: calculation in the opposite direction, taking gradients and updated parameters.  \n",
    "\n",
    "As the direction is backwards in $t$. It is  called _backpropagation through time_.  \n",
    "\n",
    "(P.s. See the drawing from \"backpropagation through time\" video). \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different types of RNNs\n",
    "\n",
    "Previously we considered the so-called __many-to-many__ architecture (input and output have many units). \n",
    "\n",
    "If on the other hand input is the text and output is the number or a single value. \n",
    "Here the output is is only considered at the end, (at the $t_x$ unit) after the entire sentece has been read. This is __many-to-one__ achitecture. \n",
    "\n",
    "There is also __one-to-one__ (stanrad NN, FC)\n",
    "\n",
    "There is also __one-to-many__ (example is music generation) Here, the there is only one input but the same overall structure, but the output of the previous layer being fed as an input to the next. \n",
    "\n",
    "There is also __many-to-many__ where $T_x \\neq T_y$. There some layers get input as output from the previos, while others get input as an actial input data. Such network has __two__ dinstinct parts: _encoder_ and _decoder_. \n",
    "The _encoder_ takes the input data $x^{<1..T_x>}$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language modelling and sequence generation\n",
    "\n",
    "Speach recognition: selecting the closly sounding workds based on the context. \n",
    "\n",
    "Language model estimates the probability of a particlar sentence, sequence of words.  \n",
    "\n",
    "Building a RNN requires \n",
    "- a large _corpus_ of english text.  \n",
    "(_corpus_ is a large quantity/set). \n",
    "- perform _tokanization step_: tokenizing the text (building dictionary as discussed) and map each of those words to _one-hot_ vectors. \n",
    "    - Add extra token, EOS (end-of-setnece) can be appended to the end of each sentence in the training set.  \n",
    "    Punctuation can also be a token.  \n",
    "    If a word does not exist in the token, than replace the unknown work wth $<UNK>$ (unknown word, _unique token_). \n",
    "- Buld an RNN to model the chance of different sequences:\n",
    "\n",
    "#### RNN achitecture:\n",
    "At time $0$ compute activation $a^{<1>}$ as a function of $x^{<1>}=\\vec{0}$ (recall that $a^{<0>}=\\vec{0}$ by convection). The $a^{<1>}$ will make a __soft-max__ prediciton  tryin to figure out, what is the prob. of the first word to be $\\hat{y}^{<1>}$ -- i.e., what is the chance of the first word to be... any of the word in the dictionary (each gets a probability).  \n",
    "\n",
    "At the next step, the the same happens but also the _correct word_ from the previous output is fed as an input.  $x^{<2>} = y^{<1>}$. \n",
    "\n",
    "At the next step the same happens as at the previos step., but now __the first $2$__ outputs are fed in as an input. $x^{<3>} = y^{<2>}$. \n",
    "\n",
    "At the last timestep, the $x^{<n>} = y^{<n-1>}$. and hopefully there is a large chance of the EOS token. \n",
    "\n",
    "Define the cost function at a certain time $t$\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{i} (y_i^{<t>}\\log(\\hat{y}_i^{<t>})) \n",
    "$$\n",
    "this is __soft-max__ loss function. \n",
    "\n",
    "For a _sufficiently large training set_ the NN can, given an initial set of words, _redict_ the chance of the _last words_. \n",
    "For example. For a sentence of a three: each __soft-max__ predixts: \n",
    "\n",
    "$$\n",
    "P(y^{<1>},y^{<2>},y^{<3>}) = \\\\\n",
    "P(y^{<1>})\\times\n",
    "P(y^{<2>}|y^{<1>}) \\times\n",
    "P(y^{<3>}|y^{<2>},y^{<1>})\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling sequences from the model\n",
    "\n",
    "Sampling is done as\n",
    "- What is the first word you want to generate. \n",
    "- Randomly sample according to the soft-max distribution\n",
    "    - As the output of the soft max is a arange of probabilitites for each word in the dictionary. \n",
    "    - Use np.random.choise() to sample according to the distribution. This gives the $\\hat{y}^{<1>}$\n",
    "- in the second timestep uses the $\\hat{y}^{<1>}$ as an input, as $x$ for the input to predict $\\hat{y}^{<2>}$ again via __soft-max__. \n",
    "- Continue untill the EOS. Or just sample a finite amount of words. \n",
    "- Reject any sample if UNK is generated. \n",
    "\n",
    "It is also possible to build a _character-based dictionary_. This is character-level language model. \n",
    "It has pros and cons.  \n",
    "Main disadvantage: Long sequences. Not as good at capturing long-term dependencies. \n",
    "Used in more specialized applications. \n",
    "\n",
    "### Vanishing Gradient Problem\n",
    "\n",
    "Basic RNNs  are not very good at capturing long-term dependencies\n",
    "\n",
    "In a classical NN gradients had  hard time propagating through all layers, affecting the weghts in the deeper layers. \n",
    "\n",
    "Similar problem is in the RNNs.  \n",
    "In RNNs working with sentences it can be especially bad.  \n",
    "\n",
    "Inputs affect only close layers. Error does not propagate all the way through the system. \n",
    "\n",
    "Another problem is the problem of __exploding gradients__.   \n",
    "__Gradient clipping__ is a possible solution to that\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated Recurrent Unit\n",
    "\n",
    "Modification to RNN hedden layer.  \n",
    "\n",
    "The standard form is \n",
    "\n",
    "$$\n",
    "a^{<t>} = g(w_a[a^{<t-1>},x^{<t>}] + b_a) \\\\ \n",
    "\\hat{y}^{<t>} = g(w_{ya} a^{<t>} + b_y)\n",
    "$$\n",
    "\n",
    "where $g(...)$ is the activation function. \n",
    "\n",
    "The _unit of RNN_ takes intput:\n",
    "- $a^{<t-1>}$ from the previous layer\n",
    "- $x^{<t>}$ from the input data \n",
    "\n",
    "and it outputs: \n",
    "- $a^{<t>}$ the current layer activations (for next layer)\n",
    "- $\\hat{y}$ for a given input via, e.g., _soft-max_\n",
    "\n",
    "Consider $c$, a __memory cell__, that remembers a certain piece of information that can be used by later units. \n",
    "\n",
    "At time $t$, the memory cell will have value $c^{<t>}$. The output of GRU is the $c^{<t>}=a^{<t>}$.  \n",
    "_Memory cell value equals to the activation value_ (they are qual in GRU but not in general)\n",
    "\n",
    "At every timestep, we overwire the \n",
    "$$\n",
    "\\tilde{c}^{<t>} = \\tanh(w_c[c^{<t-1>},x^{<t>}] + b_a)\n",
    "$$\n",
    "which is a __candidate__ in replacing $c^{<t>}$.  \n",
    "\n",
    "Then we introduce __Gate__ $\\Gamma_u$ (update gate) $\\Gamma_u\\in[0,1]$. In reality it is computed by applying sigmoid function \n",
    "$$\n",
    "\\Gamma_u = \\sigma(w_u[c^{<t-1>},x^{<t>}] + b_a)\n",
    "$$\n",
    "(so for most of the inputs it is either 0 or 1).  \n",
    "\n",
    "> The _gate_ controls when to update the $c^{<t>}$ with $\\tilde{c}^{<t>}$. \n",
    "\n",
    "The actual equations for the update is\n",
    "$$\n",
    "c^{<t>} = \\Gamma_u * \\tilde{c}^{<t>} + (1 - \\Gamma_u) * c^{<t-1>}\n",
    "$$\n",
    "\n",
    "where $*$ is the elementwise multiplication. \n",
    "\n",
    "This makes the cell _hold on to the information_ (keeping $\\Gamma_u=0$) untill it is needed (and $\\Gamma_u=1$).  \n",
    "\n",
    "So, in summation:\n",
    "\n",
    "The _unit of GRU_ takes intput:\n",
    "- $a^{<t-1>}=c^{<t-1>}$ from the previous layer\n",
    "- $x^{<t>}$ from the input data \n",
    "\n",
    "and it outputs: \n",
    "- $\\tilde{c}^{<t>}$ the current layer candidate (via $\\tan{()}$ activation)\n",
    "- $\\Gamma_u$ the current layer update gate (via sigmoid activation) \n",
    "\n",
    "Final output is the combination for the last two.  \n",
    "\n",
    "_The GRU is good at keeping the cell without updates untill it is needed_   \n",
    "\n",
    "Note, that here $\\tilde{c}^{<t>}$, $\\Gamma_u$ and $\\tilde{c}^{<t>}$ are of the same dimesion!  \n",
    "\n",
    "In the __full GRU unit__, there is also _relvence_ as:\n",
    "\n",
    "$$\n",
    "\\tilde{c}^{<t>} = \\tanh(w_c[\\Gamma_r * c^{<t-1>},x^{<t>}] + b_a) \\\\\n",
    "\\Gamma_u = \\sigma(w_u[c^{<t-1>},x^{<t>}] + b_a) \\\\\n",
    "\\Gamma_r = \\sigma(w_r[c^{<t-1>},x^{<t>}] + b_r) \\\\ \n",
    "c^{<t>} = \\Gamma_u * \\tilde{c}^{<t>} + (1 - \\Gamma_u) * c^{<t-1>}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
