{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Representation\n",
    "\n",
    "Previously we were representing words with on-hot vector using vocabluary.\n",
    "\n",
    "Weackness is that the word is treated as independent thing. Hurts generalization. No relationship between words of similar meaning is here.\n",
    "\n",
    "Intesd it is better to learned `feauturized represenation`. Fow words, features like age, royalty, gender... \n",
    "\n",
    "Each feature vector in this represenation is denoted as $e_{N}$, where $N$ is the number of features for a given work represenation. Similarity in feature vectors between words would allow to better _generalized_ for algorithm.  \n",
    "\n",
    "It is also common to embede the _visualize word embedding_ using `t-SNE` algorithm.  High dimension embedding visualized in 2D space.  \n",
    "\n",
    "The featureized represenations are called `embeddings`.  \n",
    "\n",
    "### Using word embeddings\n",
    "\n",
    "Embeddings help generalizing.  \n",
    "\n",
    "Examining large amounts of unlabeled text allows to learn embeddings that allow to group similar words together.  \n",
    "\n",
    "This is done via __transfer learning__. Using generalized learned represenations to do tasks with _small train_ set. \n",
    "\n",
    "_The exact algorithm is the mollowing_:  \n",
    "\n",
    "- Learn word embeddings from large text corpus.  (use pre-trained embeddings)\n",
    "- Transfer embeddings for a task with small train set (lower dimension feature wectors)\n",
    "- Continue to finetune the word embedding with new data\n",
    "\n",
    "This is less usefull for tasks where there are a lot of train data, e.g., machine translation. \n",
    "\n",
    "\n",
    "Word embeddings have similarity with face recognition in CNNs.  The difference is that in CNNs the unseen picture can be used, while in word embeddings, words should have known feature vector. \n",
    "\n",
    "> Encoding $\\approx$ Embedding\n",
    "\n",
    "### Properties of word embeddings\n",
    "\n",
    "This helps in analogy reasonigs.  \n",
    "#e_1 - e_2 \\approx e_{i}-?# the algorithm is trying to find the $?$, -- which embeddings are as similar as those that are asked. Man - Woman, King - ? (Queen). \n",
    "\n",
    "Each word in the _embedding space_ is represented as a point. This is a high dimensional space. Goal is find a point that _maximises the similarity_ between a required $e_1-e_2+e_{i}$ and $e_{?}$  where $e_{?}$ is the one to be found.  \n",
    "\n",
    "Visualization for embedding psaces, aka, t-SNE, performs a non-linear mapping from $N-D$ embedding space to 2D planne. _After mapping this similarity no longer holds_. \n",
    "\n",
    "The most commonly used is `cosine similarity` as \n",
    "$$\n",
    "sim(u,v) = \\frac{u^T v}{||u||_2 ||v||_2}\n",
    "$$\n",
    "-- cosine of the inner product between two vectors; \n",
    "which is just an inner product.  \n",
    "\n",
    "Also `eucleadian distance` is used \n",
    "$$\n",
    "||u-v||^2\n",
    "$$\n",
    "\n",
    "the main difference is the normalization for the length of vectors.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Matrix\n",
    "\n",
    "Consider a $V$-word wocabluary.  \n",
    "Learn an embedding matrix $E$ that is $F$-features by $V$-vocabluary size matrix.  \n",
    "\n",
    "Multiplying the embedding matrix by the one-hot vector gives the vector with the higher of the feature vector $F$.  \n",
    "\n",
    "This multiplication selects a specific word, by zeroes out everything else.  \n",
    "\n",
    "### Learning word Embeddings. \n",
    "\n",
    "There are many algorithms for word embeddings.  \n",
    "\n",
    "Consider a sesntence and a NN to complete it. Each word has an index in the vocab.  \n",
    "For each word, we constract a _one-hot_ vector.  \n",
    "Constract the matrix of arameters $E$, and \n",
    "$$\n",
    "o_{i} \\rightarrow E \\rightarrow e_{i}\n",
    "$$\n",
    "And the same for all remaining vectors.  \n",
    "\n",
    "- Now we have a set of features-size-$D$ embedding vectors.  \n",
    "- Feed them into a NN with a hidden lyaer, with set of parameters $W^{[1]}$ $b^{[1]}$ with the size of $N$ of words $\\times$  size of the feature vector. Otherwise `fixed historical window` is used to limit how long is the sequence of words that is used for prediction (`context window?`) \n",
    "- Feed the result into _soft-max_  $W^{[2]}$ $b^{[2]}$.  \n",
    "that classifies the output from the vocab.  \n",
    "\n",
    "Here the _same_ matrix $E$ is used for _all_ words.  \n",
    "\n",
    "The parameters, weights and biases, are optimized using the _backprop_. \n",
    "\n",
    "This algorithm words well. But there are fancier/or/simpler versions.  \n",
    "\n",
    "\n",
    "#### Other context/target pairs\n",
    "\n",
    "`Target word` is the one to predict.  \n",
    "`Context` are words that go before it.  \n",
    "\n",
    "We can learn by settings context both in the _left_ and on the _right_ from the target. \n",
    "\n",
    "Or consider a nearby _one_ word, which is a `skip gram` model.  \n",
    "\n",
    "This simpler approach allows to learn _word embedding_.  \n",
    "\n",
    "### Word2Vec model\n",
    "\n",
    "THis is simpler an computationally more efficinet way.  \n",
    "\n",
    "In the classical _skip-gram_ model, we consider pairs \"context-target\" to perform supervised learning.  \n",
    "\n",
    "Randomly pick a word for context and randomly pick the target and form pairs this way.  \n",
    "Now, isntead of supervised learning, consider which of the +/- 10 words from it in the context is a good target.  \n",
    "\n",
    "##### Word2Vec: Skip-Gram Model\n",
    "\n",
    "Consider a vocab of $V$ words.  \n",
    "The supervised problem we have is to learn mapping from context $X\\rightarrow Y$ target.   \n",
    "Create a _one-hot_ vector $o_i$m take the embedding matrix $E$ and infer the embedding $e_c$. This gives an embedding for a context word.  \n",
    "Than use the $e_c$ feed it into a softmax and it returns the _probability_ of different contect words \n",
    "\n",
    "$$\n",
    "p(t,c) = \\frac{e^{\\theta_t^T}e_c}{\\sum_{j=1}^V e^{\\theta_j^T}e_c}\n",
    "$$\n",
    "\n",
    "where $\\theta_t$ is the parameter $\\sim$ output $t$ (chance of the word $t$ to be the label). \n",
    "\n",
    "Loss function for the _soft-max_ is __negative log likelihood__:  \n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\hat{y},y) = -\\sum_{i=1}y_i\\log\\hat{y}_i\n",
    "$$\n",
    " \n",
    "with $y$ being a one-hot vector.  \n",
    "and $\\hat{y}$ is the vector of $V$ probabilities for each word in the vocab. to be a good label.  \n",
    "\n",
    "This is a _little NN_ \n",
    "\n",
    "$$\n",
    "o_c \\rightarrow E \\rightarrow e_c \\rightarrow [softmax] \\rightarrow \\hat{y}\n",
    "$$\n",
    "\n",
    "that has parameters in embedding $E$ (for each embedding vector $e_c$) and softmax with parameters $\\theta_c$.  \n",
    "If we optimize this mddel, we arrive at good parameters.  \n",
    "\n",
    "__Problems__:  Computational speed. _softmax is expensive to compute_ The probability evaluation requires the sum of all $V$ entries in the vocab.  \n",
    "\n",
    "__SOlution__: `hierachical soft-max`. (classifier splits the vocab into sections and first finds the section and then the word, like a _tree of clasifiers_). The computational cost here scales as $\\log(V)$ rather than linear. Where the trees are not perfectly balanced.  \n",
    "\n",
    "Sampling the context $c$. The context is chosen either by uniformly randomly, but this leads to words like 'the' 'a' are found extrmeely frequently. This biases the updates.  The sampling are done with a certain _heirsitcs_ to account for common/uncommon words. \n",
    "\n",
    "\n",
    "\n",
    "#### Negative sampling\n",
    "\n",
    "Sample context and target words.  \n",
    "And generate positive examples (as before, within the context window). \n",
    "Generate negative examples by randomly picking the word from the dictionary.  Most likely it should not be associated. Than the labels would $0$.  \n",
    "\n",
    "Given the pair or words, what is the probability of them appearing together?\n",
    "\n",
    "So the training set consists of one positive, $y=1$ example (from context window) and $k$ negative $y=0$ randomly chosen from the vocab. $k$ is _small_ for large dataset and _large_ for smaller datasets.  \n",
    "\n",
    "- With K to 1 ration of examples.\n",
    "\n",
    "Than create a supervised NN.  \n",
    "\n",
    "Again, define a logistic regression model, a sigmoid\n",
    "\n",
    "$$\n",
    "P(y=1|c,t) = \\sigma(\\theta_{c}^T,e_c)\n",
    "$$\n",
    "\n",
    "$$\n",
    "o_{n+p} \\rightarrow E \\rightarrow [... binary\\: classifiers ...]\n",
    "$$\n",
    "\n",
    "Here, instead of giant soft-max, we consider giant number of binary classifications, that are __much cheaper__. \n",
    "\n",
    "How to chose negative examples? \n",
    "- Random sampling of words \n",
    "- Sampling according with  empiricalfrequency (of the word appearance )\n",
    "- Middle point $P(w_i) = f(\\omega_i)^{3/4} / \\sum(f(w_j)^{3/4})$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe algorithm - global vectors for word represenation\n",
    "\n",
    "A very simple algorithm.  \n",
    "\n",
    "We sample words that appear in a close proximity, pairs $c,t$ as  \n",
    "$X_{ij}$ = \\# of times $j$ appears in context of $i$,  \n",
    "whre $j=t$ and $i=c$.  \n",
    "$X_{i,j}$ counrs how ofter a given pair appears.  \n",
    "The algorithm is to\n",
    "\n",
    "$$\n",
    "\\min \\sum_{i=1}\\sum_{j=1} f(X_{ij})(\\theta^T_j e_j + b_i + b_j' - \\log(X_{ij}))^2\n",
    "$$\n",
    "\n",
    "and solve fot parameters $\\theta$ and $e$, to minimize. \n",
    "\n",
    "where again $i=t$ and $j=c$, $f(X_{ij})$ is the weighting term, that is $0$ if $i=j$ to avoid the problem with log.   \n",
    "THis equations says how related the words, $c,t$ are.  \n",
    "Weighting factor also allows to contol for heuristics (frequency of the word appearance).  \n",
    "\n",
    "Here the rols of $\\theta$ and $e$ are completely symmeric. So they can be \n",
    "- initialized with random numbers\n",
    "- aget training compute $e_{w}^{final} = (e_w + \\theta_w)/2$.  \n",
    "\n",
    "**Notably** individaul components of the embedding are not generally interpretable. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment classification\n",
    "\n",
    "Determination of the tone/emotion/number behind the text. \n",
    "\n",
    "Generally, not much training set is available for it.  \n",
    "\n",
    "Word embeddings can help to do with this task.  \n",
    "\n",
    "Approaches:  \n",
    "- Use a classifier, one-hot vector, use emedding matrix from other task, extract embedding vector, and this gives encodings for new words using much larger dataset.  \n",
    "Then, average the embedded vector that can be passed into soft-max classifier with, e.g., 1-5 outputs. This allows for an argitrary sized text to extract one number/numbers to assess the setiment.  \n",
    "\n",
    "Negative: this approach ignores word order. \n",
    "\n",
    "Another approach is to use RNN for sentiment classification. Again, it starts with one-hot vector, that is fed into RNN, that allows to predict $\\hat{y}$ via _many-to-one_ RNN achitecture. \n",
    "\n",
    "### Bias in a word embeddings\n",
    "\n",
    "Ah, there is no bias :) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- finihsed exercises"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
