{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Representation\n",
    "\n",
    "Previously we were representing words with on-hot vector using vocabluary.\n",
    "\n",
    "Weackness is that the word is treated as independent thing. Hurts generalization. No relationship between words of similar meaning is here.\n",
    "\n",
    "Intesd it is better to learned `feauturized represenation`. Fow words, features like age, royalty, gender... \n",
    "\n",
    "Each feature vector in this represenation is denoted as $e_{N}$, where $N$ is the number of features for a given work represenation. Similarity in feature vectors between words would allow to better _generalized_ for algorithm.  \n",
    "\n",
    "It is also common to embede the _visualize word embedding_ using `t-SNE` algorithm.  High dimension embedding visualized in 2D space.  \n",
    "\n",
    "The featureized represenations are called `embeddings`.  \n",
    "\n",
    "### Using word embeddings\n",
    "\n",
    "Embeddings help generalizing.  \n",
    "\n",
    "Examining large amounts of unlabeled text allows to learn embeddings that allow to group similar words together.  \n",
    "\n",
    "This is done via __transfer learning__. Using generalized learned represenations to do tasks with _small train_ set. \n",
    "\n",
    "_The exact algorithm is the mollowing_:  \n",
    "\n",
    "- Learn word embeddings from large text corpus.  (use pre-trained embeddings)\n",
    "- Transfer embeddings for a task with small train set (lower dimension feature wectors)\n",
    "- Continue to finetune the word embedding with new data\n",
    "\n",
    "This is less usefull for tasks where there are a lot of train data, e.g., machine translation. \n",
    "\n",
    "\n",
    "Word embeddings have similarity with face recognition in CNNs.  The difference is that in CNNs the unseen picture can be used, while in word embeddings, words should have known feature vector. \n",
    "\n",
    "> Encoding $\\approx$ Embedding\n",
    "\n",
    "### Properties of word embeddings\n",
    "\n",
    "This helps in analogy reasonigs.  \n",
    "#e_1 - e_2 \\approx e_{i}-?# the algorithm is trying to find the $?$, -- which embeddings are as similar as those that are asked. Man - Woman, King - ? (Queen). \n",
    "\n",
    "Each word in the _embedding space_ is represented as a point. This is a high dimensional space. Goal is find a point that _maximises the similarity_ between a required $e_1-e_2+e_{i}$ and $e_{?}$  where $e_{?}$ is the one to be found.  \n",
    "\n",
    "Visualization for embedding psaces, aka, t-SNE, performs a non-linear mapping from $N-D$ embedding space to 2D planne. _After mapping this similarity no longer holds_. \n",
    "\n",
    "The most commonly used is `cosine similarity` as \n",
    "$$\n",
    "sim(u,v) = \\frac{u^T v}{||u||_2 ||v||_2}\n",
    "$$\n",
    "-- cosine of the inner product between two vectors; \n",
    "which is just an inner product.  \n",
    "\n",
    "Also `eucleadian distance` is used \n",
    "$$\n",
    "||u-v||^2\n",
    "$$\n",
    "\n",
    "the main difference is the normalization for the length of vectors.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Matrix\n",
    "\n",
    "Consider a $V$-word wocabluary.  \n",
    "Learn an embedding matrix $E$ that is $F$-features by $V$-vocabluary size matrix.  \n",
    "\n",
    "Multiplying the embedding matrix by the one-hot vector gives the vector with the higher of the feature vector $F$.  \n",
    "\n",
    "This multiplication selects a specific word, by zeroes out everything else.  \n",
    "\n",
    "### Learning word Embeddings. \n",
    "\n",
    "There are many algorithms for word embeddings.  \n",
    "\n",
    "Consider a sesntence and a NN to complete it. Each word has an index in the vocab.  \n",
    "For each word, we constract a _one-hot_ vector.  \n",
    "Constract the matrix of arameters $E$, and \n",
    "$$\n",
    "o_{i} \\rightarrow E \\rightarrow e_{i}\n",
    "$$\n",
    "And the same for all remaining vectors.  \n",
    "\n",
    "- Now we have a set of features-size-$D$ embedding vectors.  \n",
    "- Feed them into a NN with a hidden lyaer, with set of parameters $W^{[1]}$ $b^{[1]}$ with the size of $N$ of words $\\times$  size of the feature vector. Otherwise `fixed historical window` is used to limit how long is the sequence of words that is used for prediction (`context window?`) \n",
    "- Feed the result into _soft-max_  $W^{[2]}$ $b^{[2]}$.  \n",
    "that classifies the output from the vocab.  \n",
    "\n",
    "Here the _same_ matrix $E$ is used for _all_ words.  \n",
    "\n",
    "The parameters, weights and biases, are optimized using the _backprop_. \n",
    "\n",
    "This algorithm words well. But there are fancier/or/simpler versions.  \n",
    "\n",
    "\n",
    "#### Other context/target pairs\n",
    "\n",
    "`Target word` is the one to predict.  \n",
    "`Context` are words that go before it.  \n",
    "\n",
    "We can learn by settings context both in the _left_ and on the _right_ from the target. \n",
    "\n",
    "Or consider a nearby _one_ word, which is a `skip gram` model.  \n",
    "\n",
    "This simpler approach allows to learn _word embedding_.  \n",
    "\n",
    "### Word2Vec model\n",
    "\n",
    "THis is simpler an computationally more efficinet way.  \n",
    "\n",
    "In the classical _skip-gram_ model, we consider pairs \"context-target\" to perform supervised learning.  \n",
    "\n",
    "Randomly pick a word for context and randomly pick the target and form pairs this way.  \n",
    "Now, isntead of supervised learning, consider which of the +/- 10 words from it in the context is a good target.  \n",
    "\n",
    "##### Word2Vec: Skip-Gram Model\n",
    "\n",
    "Consider a vocab of $V$ words.  \n",
    "The supervised problem we have is to learn mapping from context $X\\rightarrow Y$ target.   \n",
    "Create a _one-hot_ vector $o_i$m take the embedding matrix $E$ and infer the embedding $e_c$. This gives an embedding for a context word.  \n",
    "Than use the $e_c$ feed it into a softmax and it returns the _probability_ of different contect words \n",
    "\n",
    "$$\n",
    "p(t,c) = \\frac{e^{\\theta_t^T}e_c}{\\sum_{j=1}^V e^{\\theta_j^T}e_c}\n",
    "$$\n",
    "\n",
    "where $\\theta_t$ is the parameter $\\sim$ output $t$ (chance of the word $t$ to be the label). \n",
    "\n",
    "Loss function for the _soft-max_ is __negative log likelihood__:  \n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\hat{y},y) = -\\sum_{i=1}y_i\\log\\hat{y}_i\n",
    "$$\n",
    " \n",
    "with $y$ being a one-hot vector.  \n",
    "and $\\hat{y}$ is the vector of $V$ probabilities for each word in the vocab. to be a good label.  \n",
    "\n",
    "This is a _little NN_ \n",
    "\n",
    "$$\n",
    "o_c \\rightarrow E \\rightarrow e_c \\rightarrow [softmax] \\rightarrow \\hat{y}\n",
    "$$\n",
    "\n",
    "that has parameters in embedding $E$ (for each embedding vector $e_c$) and softmax with parameters $\\theta_c$.  \n",
    "If we optimize this mddel, we arrive at good parameters.  \n",
    "\n",
    "__Problems__:  Computational speed. The probability evaluation requires the sum of all $V$ entries in the vocab.  \n",
    "\n",
    "__SOlution__: `hierachical soft-max`. (classifier splits the vocab into sections and first finds the section and then the word, like a _tree of clasifiers_). The computational cost here scales as $\\log(V)$ rather than linear. Where the trees are not perfectly balanced.  \n",
    "\n",
    "Sampling the context $c$. The context is chosen either by uniformly randomly, but this leads to words like 'the' 'a' are found extrmeely frequently. This biases the updates.  The sampling are done with a certain _heirsitcs_ to account for common/uncommon words. \n",
    "\n",
    "\n",
    "\n",
    "#### Negative sampling\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
