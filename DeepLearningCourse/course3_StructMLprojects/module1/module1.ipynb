{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why ML strategy\n",
    "\n",
    "If accuracy is low: \n",
    "- Collect more / more diverse data\n",
    "- Try ADAM, biggenr, smaller, deeper NN\n",
    "- Add L2 regularization, dopput, other activation hunctions\n",
    "- ...\n",
    "\n",
    "There needs to be a way to quickly check what is needed to impove the NN. \n",
    "\n",
    "### Orthogonalization\n",
    "\n",
    "A process to know what to tune to achive desirable outcome.  \n",
    "E.g., one parameter controls a spectific aspect of the model performance. Example: TV set with knobs for color/saturation/gradient controls. \n",
    "\n",
    "For NN. First look at the performance on the training set.  \n",
    "Then, check the performance on the dev. set and then on the test set.  \n",
    "There shoud be 1 or a specific set for adjasting training on the train set.  \n",
    "Separate set of knobs should be made for NN on the test set.  \n",
    "\n",
    "\n",
    "### Single Number Evaluation metric\n",
    "\n",
    "One number to check whether model performs better or worse\n",
    "Recall confusion matrx\n",
    "\n",
    "| actual | values |\n",
    "| --- | --- |\n",
    "| TP  | FP  |\n",
    "| FN  | TN  |\n",
    "\n",
    "`Recall RecieverOperator Characterisitcs` (ROC) graph within: true positive rate (sensitivity) vs false pasitive rate (1-specificity).  \n",
    "Area\n",
    "\n",
    "`True positive rate` = `sensitivity` = $\\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$\n",
    "\n",
    "`Precision` $\\%$ of examples the method classifies correctly of all examples\n",
    "\n",
    "`Recall` $\\%$ of actual...\n",
    "\n",
    "The problem is that some classifiers are better at precision others are at recall. \n",
    "\n",
    "The better choice is $F1$ score. This is an average of precision $p$ arnd precision $r$. This is `harmonic mean`. \n",
    "\n",
    "Having a single metric, the $F1$ score speeds up the optimization process. \n",
    "\n",
    "If there are several datasets on which model is to be evaluated. Consider computing `average` of the $F1$ score. \n",
    "\n",
    "### Satisficing and Optimizing Metric\n",
    "\n",
    "Consider $F1$ score as an accurcay metric.  \n",
    "Assume there is also a running time metric, $t$.  \n",
    "Then we can creata a new metric $x = F1 - 0.5t$. \n",
    "\n",
    "If there is an additional requirement for $t<100$ ms, than  \n",
    "- Accuracy is the optimizing metric \n",
    "- Time is satisificing metric (just needs to be within limits)\n",
    "\n",
    "Usually, one metric is optimizing and other metrics are satisficing. \n",
    "\n",
    "### Setting up training/dev/test set distributions\n",
    "\n",
    "Dev/cv set. A set to check the performance.  \n",
    "\n",
    "If there are multiple sources of data, you should make sure that \n",
    "\n",
    "> train and dev sets should be from the same distribution\n",
    "\n",
    "### Setting size of Dev and Test sets \n",
    "\n",
    "- When dataset is very small (100) $60-30$ or $60-20-20$ rule usually worked\n",
    "\n",
    "- When dataset is large ($10^6$ examples) $98-1-1$ can work\n",
    "\n",
    "Size of the test set:  \n",
    "Assess system after the model is developed.  \n",
    "This is usually achieved with reasonably small dataset.  \n",
    "\n",
    "For some applications the test set may not be required. And dev-train split is acceptable\n",
    "\n",
    "### When change dev/test sets and metric\n",
    "\n",
    "If evaluation metric does no cpation desirable/undesirable aspect of the model, the metric has to be changed. This can be done by e.g., adding weights to the cost function that panishes certain picks \n",
    "\n",
    "1. Set the target\n",
    "2. How to aim accurately at the target (to achieve good results with the metric)\n",
    "\n",
    "> Bayes Optimal Error - the best possible error for a given algirithm (depends on the intrinsic problems in data), irreducable error\n",
    "\n",
    "Progress usually slows down after surpassing human level performance\n",
    "\n",
    "If perforamcne is worse than human\n",
    "- Ger more labeled data from pesky meat sacks \n",
    "- Get insight from manual error analysis (why did algorithm failed)\n",
    "- Do better analysis of bias/varaianc\n",
    "\n",
    "### Avoidable bias\n",
    "\n",
    "If there is a large difference between human and trainin ser error, than algorithm revision needed. So the train set performance needs to be imporved.  \n",
    "- Focus on `bias `\n",
    "\n",
    "On the other hand, when human and train set errors are close, than the perofrance on the dev set may be considered more.  \n",
    "- Focus on `varaince`\n",
    "\n",
    "`Avoidable bia`s` is a diference between human performance and train error that is accatable\n",
    "\n",
    "### Human-level performance\n",
    "\n",
    "Consider sever human errors. Several qualified humans have different errors. If we know the Bayes error, than the best human can be close to the bayer error.\n",
    "\n",
    "Always compare what is relative difference between human - train and train - dev. Whichever is begger -- solve for it. \n",
    "\n",
    "Correct estimation of the human, or avoidable bians is crucual.  \n",
    "This is also why it is difficult to emprove model far beyond human performance. \n",
    "\n",
    "### Surpassing Human-level performance. \n",
    "\n",
    "Humans are good at natural perception tasks. It is hard for a machine to surpase human in those. \n",
    "\n",
    "In other tasks, like advetising, recomendations, logistions, etc, machine can outperform human many times. \n",
    "\n",
    "### Improving model performance\n",
    "- For avoidable bias (from human to train error) \n",
    "    - Train bigger model\n",
    "    - Train longer (better optimization alg., ADAM, momentum)\n",
    "    - New NN achitecture / other model architectures\n",
    "- Varaince (train - dev error)\n",
    "    - More data\n",
    "    - More regularization\n",
    "    - Data augmentation\n",
    "    - Other NN achitecture\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
