{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why ML strategy\n",
    "\n",
    "If accuracy is low: \n",
    "- Collect more / more diverse data\n",
    "- Try ADAM, biggenr, smaller, deeper NN\n",
    "- Add L2 regularization, dopput, other activation hunctions\n",
    "- ...\n",
    "\n",
    "There needs to be a way to quickly check what is needed to impove the NN. \n",
    "\n",
    "### Orthogonalization\n",
    "\n",
    "A process to know what to tune to achive desirable outcome.  \n",
    "E.g., one parameter controls a spectific aspect of the model performance. Example: TV set with knobs for color/saturation/gradient controls. \n",
    "\n",
    "For NN. First look at the performance on the training set.  \n",
    "Then, check the performance on the dev. set and then on the test set.  \n",
    "There shoud be 1 or a specific set for adjasting training on the train set.  \n",
    "Separate set of knobs should be made for NN on the test set.  \n",
    "\n",
    "\n",
    "### Single Number Evaluation metric\n",
    "\n",
    "One number to check whether model performs better or worse\n",
    "Recall confusion matrx\n",
    "\n",
    "| actual | values |\n",
    "| --- | --- |\n",
    "| TP  | FP  |\n",
    "| FN  | TN  |\n",
    "\n",
    "`Recall RecieverOperator Characterisitcs` (ROC) graph within: true positive rate (sensitivity) vs false pasitive rate (1-specificity).  \n",
    "Area\n",
    "\n",
    "`True positive rate` = `sensitivity` = $\\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$\n",
    "\n",
    "`Precision` $\\%$ of examples the method classifies correctly of all examples\n",
    "\n",
    "`Recall` $\\%$ of actual...\n",
    "\n",
    "The problem is that some classifiers are better at precision others are at recall. \n",
    "\n",
    "The better choice is $F1$ score. This is an average of precision $p$ arnd precision $r$. This is `harmonic mean`. \n",
    "\n",
    "Having a single metric, the $F1$ score speeds up the optimization process. \n",
    "\n",
    "If there are several datasets on which model is to be evaluated. Consider computing `average` of the $F1$ score. \n",
    "\n",
    "### Satisficing and Optimizing Metric\n",
    "\n",
    "Consider $F1$ score as an accurcay metric.  \n",
    "Assume there is also a running time metric, $t$.  \n",
    "Then we can creata a new metric $x = F1 - 0.5t$. \n",
    "\n",
    "If there is an additional requirement for $t<100$ ms, than  \n",
    "- Accuracy is the optimizing metric \n",
    "- Time is satisificing metric (just needs to be within limits)\n",
    "\n",
    "Usually, one metric is optimizing and other metrics are satisficing. \n",
    "\n",
    "### Setting up training/dev/test set distributions\n",
    "\n",
    "Dev/cv set. A set to check the performance.  \n",
    "\n",
    "If there are multiple sources of data, you should make sure that \n",
    "\n",
    "> train and dev sets should be from the same distribution\n",
    "\n",
    "### Setting size of Dev and Test sets \n",
    "\n",
    "- When dataset is very small (100) $60-30$ or $60-20-20$ rule usually worked\n",
    "\n",
    "- When dataset is large ($10^6$ examples) $98-1-1$ can work\n",
    "\n",
    "Size of the test set:  \n",
    "Assess system after the model is developed.  \n",
    "This is usually achieved with reasonably small dataset.  \n",
    "\n",
    "For some applications the test set may not be required. And dev-train split is acceptable\n",
    "\n",
    "### When change dev/test sets and metric\n",
    "\n",
    "If evaluation metric does no cpation desirable/undesirable aspect of the model, the metric has to be changed. This can be done by e.g., adding weights to the cost function that panishes certain picks \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
