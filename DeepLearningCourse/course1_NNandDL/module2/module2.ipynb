{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification\n",
    "\n",
    "When there are two labels to assign, e.g., can/not cat for an image. \n",
    "\n",
    "An image is represented by three matrixes in $[x,y]$ for red, blye and green colors. \n",
    "\n",
    "Single training set $(x,y)$ where $x\\in\\mathcal{R}^3$ and $y\\in\\{o,1\\}$.  \n",
    "There exists $m$ training examples $\\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), ... ,(x^{(m)},y^{(m)})\\}$.  \n",
    "Given also $m=m_{\\rm train }$ and $m_{\\rm test}$. \n",
    "\n",
    "Data is represented as matrixes for all training examples as \n",
    "\n",
    "$$\n",
    "X=\n",
    "\\begin{bmatrix}\n",
    "\\vec{x}^{(1)},\\vec{x}^{(2)},...,\\vec{x}^{(m-1)},\\vec{x}^{(m)}\\\\\n",
    "\\end{bmatrix}\n",
    "\\in \\mathcal{R}^{n_x\\times m}\n",
    "$$\n",
    "\n",
    "where each $\\vec{x}^{(i)}$ is a vector with all features for a given training example. \n",
    "\n",
    "And labels are a given by a coumn-vector as \n",
    "\n",
    "$$\n",
    "Y = [y^{(1)},y^{(2)},...,y^{(m-1)},y^{(m)}] \\in \\mathcal{R}^{1\\times m}\n",
    "$$ \n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "Given features $X$, what is $\\hat{y}=P(y=1|X)$, i.e., a probability of $y$ being a e.g., cat.  \n",
    "Parameters $\\omega\\in\\mathcal{R}^{n_{x}}$, $b\\in\\mathcal{R}$.  \n",
    "Output $\\hat{y}=\\sigma(\\omega^T x + b)$, i.e., sigmoid function $\\sigma(z)=1/(1+e^{-z})\\in(0,1)$.  \n",
    "\n",
    "Goal is to learn $\\omega$ and $b$ parameters. \n",
    "\n",
    "#### Cost function \n",
    "\n",
    "Given a training set with $m$ training examples, we need to learn the parameters. \n",
    "We have to compute the cost function for each training example $x^{(i)}$, where the trye label is $y^{(i)}$ and the model prediction is $\\hat{y}^{(i)}=\\sigma(\\omega^T x^{(i)} + b)$ where $\\sigma(z^{(i)})=1/(1+e^{-z^{(i)}})$.  \n",
    "\n",
    "Consider a **Loss function** (**Error function**). \n",
    "Square error cannot work as the function becomes non-convex with many local minima. \n",
    "\n",
    "Consider \n",
    "$$\n",
    "\\mathcal{L}(\\hat{y},y)=-(y\\log(\\hat{y})+(1-y)\\log(1-\\hat{y}))\n",
    "$$.\n",
    "\n",
    "This is a **convex** function \n",
    "\n",
    "- if $y=1$, it will _try_ to make $\\hat{y}$ **large**  \n",
    "- if $y=0$, it will _try_ to make $\\hat{y}$ **small**\n",
    "\n",
    "> Loss funciton is for a single training example\n",
    "\n",
    "> Cost function is for the entire set\n",
    "\n",
    "$$\n",
    "J(\\omega,b) = \\frac{1}{m}\\sum_{i=1}^{m}\\mathcal{L}(\\hat{y}^{(i)},y^{(i)})\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descend algorithm\n",
    "\n",
    "For a convex funciton, it tries to move downhill to the global minimum. \n",
    "\n",
    "Repean: \n",
    "\n",
    "$$\n",
    "w:=w - \\alpha \\frac{dJ(u)}{dw} \\text{ the slope of the function } \\\\\n",
    "b := b - \\alpha \\frac{dJ(u)}{db} \\\\\n",
    "$$\n",
    "\n",
    "$\\alpha$ is a learning rate. \n",
    "\n",
    "## Computation Graph \n",
    "\n",
    "Computation on a NN is optimized in terms of a \n",
    "- _forward pass_ (compute the output of a NN) and a \n",
    "- _backward pass_ (in which we compute drivatives and update the model parameters).  \n",
    "\n",
    "Computation graph illustrates why it is made this way. Given a $3$ functions computation of which requires intermediate steps. The computatational graph illustrates these steps.   \n",
    "The backward propagation can be seen by increasing the values of the $X$ _slightly_. This slight increase propagates forward in a form that can be traced bak to compute the drivatives. \n",
    "If we change one of the values, we evaluate the net change and we get the derivatives $dv/da$ via _chain rule_. \n",
    "\n",
    "> When computing derivitiaves it is most efficient from right to left. \n",
    "This is the key point behind back propagation.  \n",
    "\n",
    "\n",
    "## Logistic regression gradient descent\n",
    "\n",
    "Consider the computation graph.  \n",
    "$z=w^Tx 9 b$  \n",
    "$\\hat{y} = a = \\sigma(z)$  \n",
    "$\\mathcal{L}(a,y) = -(y\\log(a)+(1-y)\\log(1-a))$  \n",
    "\n",
    "Consider the graph \n",
    "\n",
    "$[ x_1, w_1, x_2,w_2, b ]\\rightarrow z=w_1x_1+w_2x_2+b \\rightarrow a=\\sigma(z)\\rightarrow\\mathcal{L}(a,y)$\n",
    "\n",
    "Compute derivativers with respect to the loss. First we do $d\\mathcal{L}(a,y)/da$, then go further backwards and compute the $dz=d\\mathcal{L}/dz=d\\mathcal{L}(a,y)/dz$ that can be computed via _chain rule_. Finally, compute $d\\mathcal{L}/dx_1$ alongside other derivatives.\n",
    "\n",
    "## Gradient descent on $m$ examples\n",
    "\n",
    "Recall the definition of the **cost function**\n",
    "\n",
    "$$\n",
    "J(w,b) = \\frac{1}{m}\\sum_{i=0}^m\\Big( \\mathcal{L}(a^{(i)},y) \\Big)\n",
    "$$\n",
    "\n",
    "where $a^{(i)}=\\hat{y}^{(i)}=\\sigma(z^{(i)})=\\sigma(w^tx^{(i)}+b)$  \n",
    "\n",
    "and the derivative _is also an average of the derivatives_ with respect to loss terms as \n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_i}J(w,b)=\\frac{1}{m}\\sum_{i=1}^m\\frac{\\partial }{\\partial w_i}\\mathcal{L}(a^{(i)},y^{(i)})\n",
    "$$\n",
    "\n",
    "where the derivatives for each training example can be computed independently as before. \n",
    "\n",
    "Start with initialization $0$ for each variable and loop over each element in the training exampl,e compute the $z^{(i)}$, $a^{(i)}$, $J$, $dz^{(i)}$, $dw_{i}$, where $dw_i$, $db$ are **accumulators**. \n",
    "\n",
    "The limitation of this approach is that there are multiple $for$ loops. It is inefficinet to implement explicit $for$ loops. Solution: _vectorization_. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "Implement the training of a NN for ligistic regression without _any_ explicit $for$ loops. \n",
    "\n",
    "This is done using the $x\\cdot y$ dot product in $\\texttt{\\text{numpy.dot()}}$ and  $x^T$ transpose function. \n",
    "Then $z = w^t\\cdot x + b$, which is done in one line withour explicit $for$ loop.  \n",
    "**Note** that $b$ here is a number. However iti is __broadcasted__ as a vector for this operation. \n",
    "\n",
    "## Vectorizing the gradient computation. \n",
    "\n",
    "$dz^{(i)} = az^{(i)} - yz^{(i)}$ constuct a vector $dz = [...dz^{(i)}...]$ and $A = [... a^{(i)} ...]$ and $Y=[...y^{(i)}...]$. \n",
    "Then vectorized operation is $dz = A-Y$.  \n",
    "To compute the derivative $dw$ and $db$ we do the following \n",
    "$db = (1/m)\\sum_{i=1}^m dz^{(i)}$ and $dw = (1/m) X dz^T$, where $Xdz^{(i)}$ is the matrix multiplication. \n",
    "\n",
    "## Broadcasting\n",
    "\n",
    "simpl python...\n",
    "\n",
    "## Cost function for logistic regression\n",
    "\n",
    "$\\hat{y} = \\sigma(w^T x + b)$ where $\\sigma(z) = 1 / (1+e^{-z})$\n",
    "\n",
    "interprete $\\hat{y} = P(y=1|x)$, i.e., $y = 1 : P(y|x) = \\hat{y}$ and if $y=0$ $p(y|x) = 1-\\hat{y}$. These two equations can be summarized in $p(y|x) = \\hat{y}^y(1-\\hat{y})^{(1-y)}$ and \n",
    "$$\n",
    "\\log{(p(y|x))} = y\\log\\hat{y} + (1-y)\\log(1-\\hat{y})\n",
    "$$\n",
    "\n",
    ">Minimizing the loss leads to maximizing the log of the probability. \n",
    "\n",
    "Consider overall cost function for the entire training set:\n",
    "\n",
    "Consdier that the samples are drawn independently and that the distribution is normal. \n",
    "Then:\n",
    "> Probability of the sample is given by the product of probabilities\n",
    "\n",
    "$p(\\text{labels in training set}) = \\Pi_{i=1}^{m} p(y^{(i)}|x^{(i)})$\n",
    "\n",
    "To perfrom maximum likelihood estimation we need to maximize chance that with parameters chosen chance that observations correspond to training set \n",
    "\n",
    "$\\log p(...) = \\sum\\log p(y^{(i)}|x^{(i)} ) = -\\mathcal{L}(\\hat{y}^{(i)},y^{(i)})$\n",
    "\n",
    "> Maximum likelihood estimation: choose the parameters that opimise this $-\\sum_{i=1}^{m}\\mathcal{L}(\\hat{x}^{(i)},y^{(i)})$\n",
    "\n",
    "> Minimization of the cost function $J(w,b)$ is the maximum likelihood estimation with the logistic regression model under the assumption that our training examples are identically independetly distributed.\n",
    "\n",
    "This justifies the cost function for the logistic regression. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
