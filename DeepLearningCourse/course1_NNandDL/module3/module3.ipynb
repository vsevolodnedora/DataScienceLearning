{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Overview\n",
    "\n",
    "## Representation\n",
    "\n",
    "Consider a single hidden layer NN. \n",
    "1. Input lyaer, $a^{[0]}=x$ (not real layer)\n",
    "2. Hidden layer, $a^{[1]}$, parameters $w^{[1]}$, $b^{[1]}$, $w$ shape=$(n_{nodes},m_{train})$\n",
    "3. Output layer, $a^{[2]}=\\hat{y}$, parameters $w^{[2]}$, $b^{[2]}$, $w$ shape=$(n_{nodes}=1,m_{prev.layer})$\n",
    "\n",
    "Hidden layer -- true values are not observed in a training set. \n",
    "\n",
    "The activation values for the input layer are the training data: $a^{[0]}=x$  \n",
    "The activation values for the hidden layer are $a^{[1]}$\n",
    "Finally, $a^{[2]}=\\hat{y}$. \n",
    "\n",
    "This is $2$-layer NN (input layer does not layer). \n",
    "\n",
    "Hidden and output layers have parameters associated with them. \n",
    "\n",
    "## Compute a NN output\n",
    "\n",
    "For the first node of the first layer, NN computes \n",
    "\n",
    "From training set: $x_1...x_n$ inside of the first node of the first hidden layer, NN computes $z=w^Tx+b$ and evaluates the activaion function $a = \\sigma(z)$. This is the output of the node, $a=\\hat{y}$. \n",
    "\n",
    "With subsctipts it looks like $a_{i\\text{ - node in layer}}^{[j\\text{ - layer}]}$\n",
    "\n",
    "In vectorized from the node equation looks as \n",
    "\n",
    "$$\n",
    "W^{[1]T} X + B^{[1]} = \n",
    "\\begin{bmatrix}\n",
    "w_{1}^{[1]T} \\\\\n",
    "...\\\\\n",
    "w_{n}^{[1]T}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_{1} \\\\\n",
    "...\\\\\n",
    "x_{n}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b_{1}^{[1]} \\\\\n",
    "...\\\\\n",
    "b_{n}^{[1]}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "w_{1}^{[1]T} x + b_{1}^{[1]} \\\\\n",
    "...\\\\\n",
    "w_{n}^{[1]T} x + b_{1}^{[1]}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "z_{1}^{[1]} \\\\\n",
    "...\\\\\n",
    "z_{n}^{[1]}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "z^{[1]}\n",
    "$$\n",
    "\n",
    "> Nodes in the layer are stacked vertically\n",
    "\n",
    "Same equations are for the next hidden layer with $X$ being the $A$ from the previous layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize across multiple examples\n",
    "\n",
    "Each training example $(i)$ gives $x^{(1)} \\rightarrow \\hat{y}^{(1)}$. For the second layer of NN, this reads $x^{(i)} \\rightarrow \\hat{a}^{[2](i)}$.  \n",
    "Here $z^{[1][i]}=w^{[i]}x^{(i)}+b^{[i]}$ where $a^{[1](i)} = \\sigma(z^{[1][i]})$\n",
    "\n",
    "Shape of the data is $X=(n_x,m)$\n",
    "\n",
    "## Activation function \n",
    "\n",
    "There is a lot of freedom in  chosing such a function\n",
    "\n",
    "- Sigmoid is rarely used (except output layer)\n",
    "- $\\tanh(z)$ is a beter sigmoid function\n",
    "- ReLU (rectified linear unit)\n",
    "- Leaky ReLU (has a non-zero zerivative at $x < 0$)\n",
    "- Linear (used only in the output layer and rarely)\n",
    "\n",
    "> Non-linear activation function is needed as otherwise it reduces to a **linear** model and cannot model non-linearity in data\n",
    "\n",
    "## Gradient descend for NNs \n",
    "\n",
    "Given \n",
    "- parameters $w^{[i]}$\n",
    "- cost functon $J()$ sum of the loss functions\n",
    "- Initialize with rabom values \n",
    "\n",
    "Gradient descend: Repeat\n",
    "- Compute partial derivatives \n",
    "- update parameters \n",
    "- evaluate cost/loss \n",
    "- adjust parameters\n",
    "\n",
    "Forward propagation.\n",
    "1. $Z^{[1]} = W^{[1]} X + B^{[1]} $\n",
    "2. $A^{[1]} = g^{[1]}(Z^{[1]})$ \n",
    "3. $Z^{[2]} = W^{[2]} A^{[1]} + B^{[2]}$\n",
    "4. $A^{[2]} = g^{[2]}(z^{[2]}) = \\sigma(Z^{[2]})$\n",
    "\n",
    "Backward propagation\n",
    "1. $dz^{[2]} = A^{[2]} - Y$ where $Y=[y^{(1)},y^{(2)},...,y^{(m)}]$ \n",
    "2. $dw y^{[2]}=\\frac{1}{m} dz^{[2]} Ay^{[1]T}$\n",
    "3. $dby^{[2]} = \\frac{1}{m}\\texttt{\\text{np.sum}}(dz^{[2]}, axis=1, keepdims=True)$\n",
    "4. $ dz^{[1]} = w^{[2]T} dz^{[2]} $#$ g^{[1]'}(z^{[1]}) $ where # is the element wise product\n",
    "5. $dw^{[1]} = \\frac{1}{m} dz^{[1]}X^T$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - \n",
    "2 false\n",
    "3 $a_2^{[3](4)}$\n",
    "4 Sigmoid \n",
    "5 (1,2)\n",
    "6 True\n",
    "7 True\n",
    "8 This will casue the inputs of the tanh to be very large thus causing graients to be close to zero\n",
    "9 $b^{[1]} is (2,1)$ and $W^{[1]}$ is ... $W^{[1]}$ is $(2,4)$ -- the number of rows in $W^{[k]}$ is the number of neurons in the $k-th$ layer and the number of columns is the number of inputs of the layer\n",
    "10 3,m"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyBlastAfterglow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
