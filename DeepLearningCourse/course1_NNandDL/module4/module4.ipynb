{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep L-layer Neural Network\n",
    "\n",
    "Logisitc regressionm is a _shallow_ problem (one-layer)\n",
    "\n",
    "Consider $L=4$ NN $n^{[4]}$ with # number onits in this layer. \n",
    "\n",
    "### Forward propagation\n",
    "\n",
    "For a given training example. \n",
    "1. Compute $Z^{[1]}=w^{[1]} x + b^{[1]}$ for the first layer.\n",
    "2. Compute activations $a^{[1]}=g(z^{[1]})$  \n",
    "3. For layer $2$ now compute $z^{[2]} = w ^{[2]} a ^{[1]} + b^{[2]}$\n",
    "\n",
    "...\n",
    "\n",
    "4. Compute $z^{[l]}$ for the final layer and compute $a^{[l]}=\\hat{y}$\n",
    "\n",
    "For a vectorized implementation it reads\n",
    "for the first layer \n",
    "$$Z^{[1]} = W^{[1]} X + b^{[1]}$$\n",
    "and for $l$ith\n",
    "$$Z^{[l]} = W^{[l]} A^{[l-1]} + B^{[l]}$$\n",
    "and \n",
    "$$A^{[l]} = g^{[l]}(Z^{[l]})$$\n",
    "\n",
    "### Debugging\n",
    "\n",
    "Consider NN with $2$ inputs $x_{1,2}$, and the following arcitecture \n",
    "$$\n",
    "2\\rightarrow 3\\rightarrow 5 \\rightarrow 4 \\rightarrow 2 \\rightarrow 1\n",
    "$$\n",
    "with the last one being the output.  \n",
    "This reads\n",
    "$$\n",
    "n^{[0]} = n = 2 \\\\\n",
    "n^{[1]} = 3 \\\\\n",
    "n^{[2]} = 5 \\\\\n",
    "n^{[3]} = 4 \\\\\n",
    "n^{[4]} = 2 \\\\\n",
    "$$\n",
    "Consider the first layer: \n",
    "$$Z^{[1]} = W^{[1]} X + b^{[1]}$$\n",
    "Here the dimensions are:\n",
    "$$(3,1) = (3,2) (2,1) + (3,1)$$ \n",
    "$$(n^{[1]},1) = (n^{[1]},n^{[0]}) (n^{[0]},1) + (n^{[1]},1)$$\n",
    "\n",
    "> So wot _weights_ dimensions for layer $l$ is $(n^{[l]},n^{[l-1]})$ with $n^{[l]}$ being the **number of units in the layer**. \n",
    "\n",
    "> for _biases_ the dimensions for layer $l$ is $(n^{[l]},1)$ with $n^{[l]}$ being the **number of units in the layer**. \n",
    "\n",
    "#### For vectorized implementation:\n",
    "\n",
    "HEre imensions of $Z$ and $X$ are different. Recall\n",
    "$$\n",
    "Z^{[1]} = W^{[1]} x + b^{[1]} \\\\\n",
    "(n^{[1]},1) = (n^{[1]},n^{[0]})(n^{[0]},1) + (n^{[1]},1)\n",
    "$$\n",
    "\n",
    "In vectorized form wiwth $m$ examples, we have \n",
    "$$\n",
    "Z^{[1]} = W^{[1]} X + b^{[1]} \\\\\n",
    "(n^{[1]},m) = (n^{[1]},n^{[0]})(n^{[0]},m) + (n^{[1]},m)\n",
    "$$\n",
    "\n",
    "So the dimensions are $Z^{[l]},A^{[l]} : (n^{[l]},m)$\n",
    "\n",
    "### Deep representation\n",
    "\n",
    "Each layer lears a certain parttern type. From simple, like basic gemetric things, to complex, like faces and sound patterns. \n",
    "\n",
    "Circuit theory and deep learning.\n",
    "\n",
    "> There are matematical functions that are easier to compute with _deep NNs_ than with shallow ones, as the size of the layer may increase drastically\n",
    "\n",
    "### Building blocks of Deep Neural Networks\n",
    "\n",
    "Consider a layer $l$ with parameter $W^{[l]}$ and $b^{[l]}$. We compute $Z^{[l]}$ and $A^{[l]}$ and cash $Z^{[l]}$. Then in a backprop we compute $ dW^{[l]}$ and $db^{[l]}$. \n",
    "\n",
    "> See fancy graphics in the lecture with building blocks\n",
    "\n",
    "### Forward and backward propagation\n",
    "\n",
    "Consider Forward propagation for layer $l$ (vectorized). \n",
    "$$\n",
    "Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]} \\\\\n",
    "A^{[l]} = g^{[l]}(Z^{[l]}) \n",
    "$$\n",
    "Consider Backward propagation step, where we need $dW^{[l]}, db^{[l]}$ and $dA^{[l-1]}$\n",
    "$$\n",
    "dZ^{[l]} = dA^{[l]} * g^{[l]'}(Z^{[l]}) \\\\\n",
    "dW^{[l]} = \\frac{1}{m}dZ^{[l]} A^{[l-1]T} \\\\\n",
    "db^{[l]} = \\frac{1}{m} \\texttt{\\text{np.sum}}(dZ^{[l]})(dZ^{[l]}, \\text{axis}=1, \\text{keepdims=True}) \\\\\n",
    "dA^{[l-1]} = W^{[l]T} dZ^{[l]}\n",
    "$$\n",
    "where as before $*$ is the element-wise product, and $g^{[l]'}$ is the derivative of the activation function?\n",
    "\n",
    "### Implementation\n",
    "\n",
    "For the initialization of the baward step, the final layer $dA^{[l=L]}$ with respect to $\\hat{y}$ is needed. For **logistic regression** the derivative of the loss function $\\mathcal{L}(\\hat{y},y)$ are \n",
    "$$\n",
    "da^{[l]}=-\\frac{y}{a} + \\frac{1-y}{1-a}\n",
    "$$\n",
    "and in the vectorized form it is a sum over all training examples of the above equation. \n",
    "\n",
    "### Hyperparameters and parameters\n",
    "\n",
    "1. Learning rate: $\\alpha$\n",
    "\n",
    "> Training a NN is an iterative process, where parameters are twicked and model is re-evaluated\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See assignments..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
