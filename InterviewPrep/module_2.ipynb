{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q1 & Q2: What is Logistic Regression? Difference with linear regression?\n",
    "\n",
    "Logistic regrssion (or _logit_ model) is a type of statistical model, that estimates the probability of an event occuring. There, the dependenct variable is bound [0,1]. There, the _logit_ transforamtion is applied on the odds. The probability of success is devided by the probability of failer. This is called $\\log$ odds. \n",
    "\n",
    "$$\n",
    "logit(\\pi) = \\frac{1}{1+\\exp(-\\pi)} \\\\\n",
    "\\ln(\\pi/1-\\pi) = \\beta_0 + \\beta_1 x_1 + ... +\\beta_k x_k\n",
    "$$\n",
    "\n",
    "where $logit(\\pi)$ is the dependent or response variable and $x$ is the independent variable. The $\\beta$ parameters are estimated via _maximum likelihood estimation_ (MLE). \n",
    "\n",
    "The method iterates over combinations of $\\beta$(s) to find the best fit to log odds. In other words, the algorithm tries to maximize the log likelihood function. \n",
    "\n",
    "En interperting the log odds, it is common to transform them into odds ratio (OR), that represents the odds that an outcome will happen given a certain event comapred to the odds tha an outcome occure on the absnce of that event. \n",
    "\n",
    "The difference between linear and logistic regressions is the following:\n",
    "- Linear regression is applied to continous dependent varaibles. \n",
    "- logistic regression is applied to categorical variables (descrte). Categorical variables are [1,0], [\"T\", \"F\"]. Logistic regression prodced probability \n",
    "\n",
    "\n",
    "> Q3: Why can't we do classification using regression? \n",
    "\n",
    "Linear regression is unbounded, while logistic regression gives a probability between [0,1]. \n",
    "\n",
    "\n",
    "\n",
    "> Q4: What is the decision tree? \n",
    "\n",
    "See Articule by [IBM](https://www.ibm.com/topics/decision-trees)\n",
    "\n",
    "Decision tree is a _non-parameteric_ supervised learning algorithm, used for classifiction and regression. It builds a hierchical, tree-like, structure that consists of \n",
    "- root node\n",
    "- branches \n",
    "- internal nodels\n",
    "- leaf nodes (represent all possible outcomes)\n",
    "\n",
    "Decision tree utilises _divide and concure_ strategy and uses a _greedy search_ to identify opticam split points within a tree. The splitting is done _top-down_ in a recursive manner untill all (or most) outcomes have been classed with labels. \n",
    "\n",
    "A decision tree can overfit, when the deision tree becomes _impure_, i.e., when data points can no longer be classified as _homogenous_ sets. This is prone to occure as the desision tree complexity grows. Thus, decision trees try to build the smallest possible tree. \n",
    "\n",
    "_Prunning_ is commonly used to prevent overfitting in decesion trees, which is a process of removing branches that split on features with low importance. \n",
    "\n",
    "\n",
    "> Q5: What os entropy, information gain, ginin index, reducing impurity?\n",
    "\n",
    "Entropy, information gain and Gini impurity are the methods that used to chose the best attribute at each node. They are _splitting criteria_. \n",
    "\n",
    "- _Entropy_ comes from the information theory and is defined as \n",
    "$$\n",
    "Entropy(S) = -\\sum_{c\\in C} p(c)\\log_2(p(c)) \\in [0,1]\n",
    "$$\n",
    "where $S$ is the data set, $c$ is the class in set $S$, $p(c)$ is the proportion of data points that belong to the class to the total number of points in $S$.  \n",
    "Entropy is 0 if all points belong to a class and $1$ if 50\\% of points belong to a class.  \n",
    "The smaller the entropy the better this attribute for splitting. \n",
    "\n",
    "- _Information gain_ is the difference in entropy before and after splitting the tree on a given attribute. The attributre with the highest information gain is the best one to use for splitting. It is the best at classifying the data. \n",
    "$$\n",
    "InfoGain(S,\\alpha) - Entropy(S) - \\sum\\frac{|S_v|}{|S|}Entropy(S_v)\n",
    "$$\n",
    "where $\\alpha$ is a certain attribut or a class label, $|S_v|/|S|$ represents the proportion of the values in $S_v$ to the number of values in $S$. \n",
    "\n",
    "- _Gini Impurity_ is the probability of incorrectly classifying random data point in the dataset if it were labeled based on the class distribution of the dataset. Similar to entropy, if set, S, is pure-i.e. belonging to one class, then, its impurity is zero.\n",
    "$$\n",
    "GI = 1 - \\sum_i(p_i)^2\n",
    "$$\n",
    "\n",
    "\n",
    "> Q6: How to control leaf height and Prunning?\n",
    "\n",
    "- Use _Maximum Depth_ to control leaf hight\n",
    "- Use _Minimum split size_ to stop further slitting when a number of points at a node reaches a certain minimum value. This limits the tree growth. \n",
    "- Use _Minimum leaf size_ to limit split nodes when the number of points in one of the child nodes is lower than the minimum leaf size.\n",
    "\n",
    "Prunning is separated into:\n",
    "- Pre-prunning or _early stopping criteria_. Set before building a tree and it stops tree from growing when the criterion is met or when a pure class is discovered. \n",
    "- Post-prunning - grow the tree fully monitoring the _Complexity Parameter_ (CP) and than cutting the tree wit the optimal CP. \n",
    "\n",
    "\n",
    "> Q7: How to handle a decision tree for numerical and categorical data?\n",
    "\n",
    "For numerical data, the splitting of a tree is done using a threshold value.  \n",
    "\n",
    "At every split, the decision tree will take the best variable at that moment. This will be done according to an impurity measure with the split branches. And the fact that the variable used to do split is categorical or continuous is irrelevant (in fact, decision trees categorize continuous variables by creating binary regions with the threshold).  \n",
    "\n",
    "\n",
    "> Q8: What is the radnom forest algorithm? \n",
    "\n",
    "_Random Forest_ is an _ensemble machine learning algorithm_ that follows the _bagging technique_. The base estimators in the random forest are _decision trees_. Random forest randomly selects a set of features that are used to decide the best split at each node of the decision tree.\n",
    "\n",
    "Algorithm:\n",
    "1. Random subsets are created from the original dataset (bootstrapping).\n",
    "2. At each node in the decision tree, only a random set of features are considered to decide the\n",
    "best split.\n",
    "3. A decision tree model is fitted on each of the subsets.\n",
    "4. The final prediction is calculated by averaging the predictions from all decision trees.\n",
    "To sum up, the Random forest randomly selects data points and features and builds multiple trees\n",
    "(Forest).\n",
    "Random Forest is used for feature importance selection. The attribute (.feature_importances_) is\n",
    "used to find feature importance\n",
    "\n",
    "Some Important Parameters:\n",
    "1. __n_estimators:__  It defines the number of decision trees to be created in a random forest.\n",
    "2. __criterion__: \"Gini\" or \"Entropy.\"\n",
    "3. __min_samples_split__: Used to define the minimum number of samples required in a leaf\n",
    "node before a split is attempted\n",
    "4. __max_features__: It defines the maximum number of features allowed for the split in each\n",
    "decision tree.\n",
    "5. __n_jobs__: The number of jobs to run in parallel for both fit and predict. Always keep (-1) to\n",
    "use all the cores for parallel processing.\n",
    "\n",
    "\n",
    "> Q9: What is Variance and Bias tradeoff?\n",
    "\n",
    "It is a relationshipo between model's complexity, accuracy of the prediction and how well it generalizes to unseen data. In general, as we increase the model complexity the _bias_ reduces, and model fits the data better. However, after a certain point it tends to perform worse on previously unseen data, i.e., it has high _variance_. \n",
    "\n",
    "High bias is also referred to as _underfitting_ as model may miss the important trends in data. \n",
    "\n",
    "High variance is reffred to as _overfitting_ as model starts to fit the random fluctuations in traning data. \n",
    "\n",
    "The _bais-variance tradoff_ is a big problem in sueprvised learning. \n",
    "\n",
    "Note model complexity is not the number of parameters in the model. It is an indicator, but a poor one. \n",
    "\n",
    "The problem is similar to the _accuracy and precision_ relationship. \n",
    "\n",
    "- Accuracy is the description of the bias (improved by narrowing down dataset, where bias is low, which leads to _underfitting_).\n",
    "- Precision is the description of the variance (improved by enlarging the dataset). \n",
    "\n",
    "\n",
    "> Q10: What are ensemble methods?\n",
    "\n",
    "Using multiple learning algorithms and combining their predictions to obtain better result.  \n",
    "Particluar example is combining _decision trees_ into an esamble model.  \n",
    "Each decision tree uses its own feature selection and thresholds. \n",
    "\n",
    "Bsing examples: _Bagging_ and _Boosting_, ``decrease the variance of a single estimate as they combine several estimates from different models``. \n",
    "\n",
    "Types of Ensemble Methods:\n",
    "- Bagging = Bootstrap Aggregating\n",
    "    - Bagging is a combination of Bootstrap and Aggregation, where multiple bootstrapped samples are pulled from a dataset and a _decision tree_ is build on each bootstrapped subsample. After, an algorithm is used to aggregate over decision trees to form the most efficient prediction. \n",
    "    - Bagging employs _homogeneous_ weak lerners that learn independently, in parallel. \n",
    "    - __Example__: Random Forest: Similar to _bagging_ but feature selection for each tree is assigned randomly. This each tree has a statistically different subset of data to work with. This provides better ensemble to aggregate over. \n",
    "    I.e., the decision trees are split on different features. \n",
    "- Boosting:\n",
    "    - In boostong weak lerners, (aka, trees), learn sequentially and adaptively. \n",
    "    - Each new weak lerner is built to correct for the errors of the previos. The process continues untill either all data is predicted correctly or maximum number of weak lerners is reached. \n",
    "    - __Examples__: AdaBoost, Gradient Boosting, XGBoost\n",
    "\n",
    "Bagging and boosting are both:\n",
    "- ensemble methods to get N lerners from 1 lerner\n",
    "- generate several training data sets by random sampling\n",
    "- make final decision by averaging the N learniners (or using Majority Voting)\n",
    "- good for reducting varaince and provide higher stability\n",
    "\n",
    "\n",
    "> Q11: what is Support vector machines SVM Classification?\n",
    "\n",
    "SVMs (also called _Large margin classifier_) are a set of __supervised learning methods__ used for classification, regression and outliers detection.\n",
    "\n",
    "SVM is the _supervised_ __max-margin__ model with  assiciated learning algorithms for classification (but can also be used for regression, but there it is $\\epsilon$-sensitive). \n",
    "\n",
    "SVMs can perform _leanear_ and _non-lenear_ classification. using so-called _kernel trick_, implicitely mapping their inputs to a higher dimension feature sapce. \n",
    "\n",
    "The objective of the SVM is to find a _hyperplane_ in an N-dimensional space (where N is the number of features) that distinctly clssifies the data points. \n",
    "These hyperplanes are the _decision boundaries_ that separate the data. a\n",
    "\n",
    "__Advantages__:\n",
    "- Effective in high dimensional spaces.\n",
    "- Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "- Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "- _Versatile_: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "\n",
    "__Disadvantages__:\n",
    "\n",
    "- If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "\n",
    "- SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below).\n",
    "\n",
    "\n",
    "> Q12: What is Naive Bayes Classification and Gaussian Naive Bayes\n",
    "\n",
    "See [IBM article](https://www.ibm.com/topics/naive-bayes)\n",
    "\n",
    "In statistics, naive Bayes clssifiers are a family of linear _probabilistic classifiers_ that assume that the features are __conditionally indepenent__ given the target class. \n",
    "\n",
    "Naive Bayes classifiers are _supervised_ learning algorith, used for classification task. It is also considered as a _generative learning_ algorithm ,as it seeks to model the distribution of inputs, given class or category. Unlike _discriminative classifiers_ like `logistic regression` it does not find the feature importance. \n",
    "\n",
    "It is based on a Bayes theorem, the bayesian statistics. \n",
    "\n",
    "The _Bayes Theorem_ allows to invert the conditional probabilities and is written as \n",
    "\n",
    "$$\n",
    "P(Y|X) = \\frac{P(X|Y)}{P(X)}\n",
    "$$\n",
    "\n",
    "In bayesian statistics, the additional information later accuired affects the initial probability,  that are called _prior_ and _posterior_ probabilities.  \n",
    "\n",
    "- The _prior_ probability is the initial prob. of the even before additional information\n",
    "- The _posterior_ probability is the probability of the event after \"observing\" or including additional data. \n",
    "\n",
    "Another equation that helps\n",
    "\n",
    "$$\n",
    "P(\\text{diabities|+test}) = \\frac{P(\\text{+test|diabities})\\times P(\\text{diabetes})}{ P(\\text{+test|diabetes})\\times P(\\text{diabetes}) + P(\\text{+test|no diabetes})\\times P(\\text{no diabetes})}\n",
    "$$\n",
    "\n",
    "Naive Bayes assumes that all predictors are _conditionally independent_ and thet all features contribute _equally_ to the outcome. This makes model _very approximate_ but _computationally tractable_.  \n",
    "For each variable then there is _single probability_. \n",
    "\n",
    "The alorithm performs well with _small sample sizes_. \n",
    "\n",
    "The Naive classifier uses the _conditional probability_ as: \n",
    "\n",
    "$$\n",
    "\\text{posterior probability} = \\frac{\\text{(conditional prob.)}\\text{(prior prob.)}}{\\text{evidence (a.k.a. \"stabilizer\")}}\n",
    "$$\n",
    "\n",
    "The algorithm works as follows:\n",
    "- Using _labelled_ train data, calculate the __class conditional probabilities__ and __proior probabilities__.  The Naive Bayes classifier will return the class _with the highest posterior probability_ out of the group of classes, e.g., \"spam\" or \"not spam\" for a given corpus of text. \n",
    "\n",
    "$$\n",
    "\\hat{y} = \\argmax P(x|y)P(y)\n",
    "$$\n",
    "\n",
    "__Class-conditional probabilities__  are _individual likelihoods_ of each data point (e.g., word in the email), that are determined by computing the frequency of a given work for each category (e.g., spam or not spam). This is known is __Maximum Likelihood estimation__ (MLE).  \n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{P}(y=[\\text{word1 \\& word2}]|\\text{x=spam}) = P(\\text{word1}|\\text{spam}) P(\\text{word2}|\\text{spam})\n",
    "$$\n",
    "\n",
    "Prior probabilities here are the same as in Bayes Theorem. They are computed from the training data. \n",
    "\n",
    "The overall classification for an email is based on all the words in the email. \n",
    "\n",
    "The prior probability acts as _weight_ to the _class conditional_ probability when the two values are multipled, to get _individual posterior probability_. After, a maximum of a posterior (MAP) is computed to assign the class label. The final equation for the \"email\" example ls\n",
    "\n",
    "$$\n",
    "\\hat{\\text{class label}} = \\argmax_{y\\in Y} P(\\text{class label}) \\times \\Pi_{i\\in I} P(\\text{word}_i | \\text{class label})\n",
    "$$\n",
    "\n",
    "However, commonly it is used in a log form as \n",
    "\n",
    "$$\n",
    "\\hat{\\text{class label}} = \\argmax_{y\\in Y} \\log P(\\text{class label}) \\times \\sum_{i\\in I} \\log P(\\text{word}_i | \\text{class label})\n",
    "$$\n",
    "\n",
    "\n",
    "__Types of Naive Bias__:\n",
    "\n",
    "- __Gaussian Naive Bayes__: This is a variant of a classifier which is used with Gaussian distribution (normal distribution) and continous variables.The model is fitted by finding the mean and standard deviation of each class. \n",
    "\n",
    "- __Multinominal Naive Bayes__: here, it is assumed that the features are from _multinomian distributions_. Usefull when working with _descrete data_, e.g., frequency counts. Usually used within NLP (e.g., spam classification).\n",
    "\n",
    "- __Bernoulli Naive Bayes__: also used with Boolean varibles\n",
    "\n",
    "\n",
    "__Advantages of Naive Bayes__:\n",
    "\n",
    "- Less complex\n",
    "- Scales well (better than logistic regression), low memory requirement\n",
    "- Can work with high-dimensional data. (epsecially usefull with NLP, where word count is large)\n",
    "\n",
    "\n",
    "__Disadvantages of Naive Bayes__:\n",
    "\n",
    "- Subject to Zero frequency: (when categorical variable does not exist within training data). _Solution_: laplace smoothing\n",
    "- Unrealistic core assumptions\n",
    "\n",
    "\n",
    "__Application of Naive Bayes__:\n",
    "\n",
    "- Spam filtering\n",
    "- Document calssification\n",
    "- Sentiment analysis\n",
    "- Mental state prediction (sing fMPI data...)\n",
    "\n",
    "\n",
    "> Q12 What is the confusion Matrix?\n",
    "\n",
    "See [Wikipedia](https://en.wikipedia.org/wiki/Confusion_matrix)\n",
    "\n",
    "__Confunsion matrix__ is a special kind of _contingency table_ with two dimensions \n",
    "- actual \n",
    "- predicted \n",
    "It is also called _error matrix_, a specific table layout visualizing performance of a classification algorithm. (Usually used in supervised learning. In Unsupervised it is called _matching matrix_). \n",
    "\n",
    "Each row in the matrix is the _istance_: actual or predicted  \n",
    "Each column is the _class_: 1,2,3... cold, hot... etc\n",
    "\n",
    "For column in the table, comparint _actual_ and _predicted_ we can assign\n",
    "- __True Positivie__ `TP` if predicted True = actual True\n",
    "- __True negative__ `TN` if predicted False = actual False\n",
    "- __False Positivie__ `FP` if predicted True while actual is False (type I error)\n",
    "- __False Negative__ `FN` if predicted False while actual is True (type II error)\n",
    "\n",
    "\n",
    "We can also derive \n",
    "- __condition positive__ `P` total _number_ of actual positives in data\n",
    "- __condition negative__ `N` total _number_ of actual negatives in data\n",
    "\n",
    "These allow to estimate _rates_ as  \n",
    "\n",
    "- __True Positive Rate__ `TPR` = `sensitivity` = `recal` = `hit rate` = TP/P = TP/(TP+FN) = 1-FNR\n",
    "- __True Negative Rate__ `TNR` = `specificity` = `selectivity` = TN/N = TN/(TN+FP) = 1 - FPR\n",
    "- __False Negative Rate__ `FNR` = `miss value` = FN/P = FP/(FN+TP) = 1 - TPR\n",
    "- __False Postive Rate__ `FPR` = `fall out` = FP/N = FP / (FP + TN) = 1 - TNR\n",
    "- __Postivie Likelihood Ratio__ `LR+` = TPR / FPR\n",
    "- __Negative Likelihood Ratio__ `LR-` = FNR / TNR\n",
    "\n",
    "and other quantities such as \n",
    "\n",
    "- __Positive Predicted Value__ `Precision` = PPV = TP/(TP+FP) = 1- FDR\n",
    "- __Negative Predicted Value__ `NPV` = TN/(TN + FN) = 1- FOR\n",
    "- __False Omisison Rate__ `FOR` = FN / (FN + TN) = 1 - NPV\n",
    "\n",
    "From these we can compute the important ones:\n",
    "\n",
    "- __Accuracy__ `ACC` = (TP + TN) / (P + N) = (TP+TN) / (TP+TN+FP+FN)\n",
    "- __F1 scoor__ `F1` = 2 * (PPv *  TPR) / (PPV + TPR) = (2 TP) / (2 TP + FP + FN)\n",
    "\n",
    "__NOTE__: in computing accuracy and other quantities, it is assumed that the _const_ of both errors is the same. It may not apply when FP is valued differently from FN. \n",
    "\n",
    "- __Misclassification Rate__ `MR` = ` 1 - ACC is a metric that tells us the percentage of observations that were incorrectly predicted by some classification model. Misclassification Rate = # incorrect predictions / # total predictions\n",
    "\n",
    "__NOTE__: It doesn’t take into account how the data is distributed. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
