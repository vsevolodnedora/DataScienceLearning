{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Intervew perep. \n",
    "\n",
    "Based on the document called: \"Data Science Interview Questions (30 days of Interview Preparation) by INEURON.AI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q1: What is the difference between AI, DS, ML, and DL  \n",
    "\n",
    "- AI is ... \n",
    "- ML is a subset of AI (including supervised, unsupervised, reinforcment)\n",
    "- DL is a subset of ML\n",
    "\n",
    "DS leverges these and other tools e.g., data integration, distributed achitectures, automated ML, vata visualization, engineering, ect to extract insights from the data valuable to bunsienss operation and decision making  \n",
    "\n",
    "> Q2: What are the differences between Supervised, unsupervised and Reinforcment Learning? \n",
    "\n",
    "- ML is a study of algorithms that allow a program to solve the problem without explicit instructons, -- by utilizing learned realtionships from data. ML models are data-centric model. \n",
    "\n",
    "- Supervised learning: the data used is laballed, meaning that there mapping between data and a label is given and a model learns to predict a label given new data. This includes \n",
    "    - Regression\n",
    "    - Classification\n",
    "\n",
    "- Unsupervised Learning uses unlabelled data and learns patterns within the data, extracting features based on co-occurace and underlying distributions. Used for \n",
    "    - Clustering\n",
    "    - Anomaly detection\n",
    "    - Association\n",
    "    - Autoencoders\n",
    "\n",
    "- Reinforcment Learning utulizes an agent that interacts with the environment and tries to learn an optimal policy based on the reward function. In other words, it tries to find the most optimal solution, by trying various tires and adjusting it based on the reward given.  \n",
    "\n",
    "> Q3: What is the general architecture of ML?\n",
    "\n",
    "1. Business understanding\n",
    "2. Data acquisition and understanding; model selection (data sourding, pipeline, environemnt, cleaning, data exploration)\n",
    "3. Modelling (\n",
    "    - feature engeneering, \n",
    "    - feature selection (using backward elimination, correlation, PCA, and domain knowledge), data scaling, \n",
    "    - model training, \n",
    "    - model evaluation (checking the accuracy of the model, confusion matrix, cross-validation))\n",
    "    - model/data adjustments to improve metrics\n",
    "4. Deployment; local, cloud, specific machine \n",
    "    - scoring, performance monitoring\n",
    "5. Collecting feedback, adjusting, CI/CD\n",
    "\n",
    "> Q4: What is linear regression?\n",
    "\n",
    "Fiding a linear $y=ax+b$ relationship between a predictor $x$ and a target $y$, where $a$ is a slope (or weight) and $b$ is the intercept (or bais)\n",
    "\n",
    "> Q5: what is OLS stats model (ordinary least squares)\n",
    "\n",
    "OLS is a stats model that helps identiry features with significat effect on the output. Calling t on data and calling .summary() we get the statistical properties.  See [medium](https://medium.com/swlh/interpreting-linear-regression-through-statsmodels-summary-4796d359035a) for a full breakdown.  \n",
    "\n",
    "It also gives \"t\" value for each feature. The higher the 't-value' the more significant is the feautre. It also gives the 'P' valaue, the low walue of which helps regecting the null hypothesis.  \n",
    "If 'p' value is < 0.05 we can generally assume feature to be significant.\n",
    "\n",
    "> Q6: What is L1 regularization (L1-lasso)?\n",
    "\n",
    "Input data has variance (variations...) and trends. The goal of a model is to learn the trend without overfitting on the variations, which would lead to a poor performacne on a new, unseen data with difference variations.  \n",
    "A way to do this is by employing regularization. The _Lasso Regularization_, aka, __Least Absolute Shrinkage and Selection Operator__ adds an absolute magnitude of coefficient as a penalty to the loss function. \n",
    "'Lasso regularization 'shrinks' the coefficient of 'non-important' features. \n",
    "See also [medium](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c). \n",
    "It can effectively remove unimportant features \n",
    "\n",
    "$$\n",
    "L_1 = \\sum_{i} (y_i - \\sum_{j}x_{ij}W_j)^2 + \\lambda \\sum_j |W_j|\n",
    "$$\n",
    "\n",
    "This method is usfull when there is large st of features, while tradiational methods, like corss-validation, stepwise regression are usefull for when there is small set of features.  \n",
    "\n",
    "> Q7: What is L2 (or Ridge) regularization? \n",
    "\n",
    "In L2 the L2 norm of the _squared magnitude_ is used as a penalty. \n",
    "See also [medium](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c) and [builtin](https://builtin.com/data-science/l2-regularization#:~:text=L1%20Regularization%2C%20also%20called%20a,term%20to%20the%20loss%20function.)\n",
    "\n",
    "$$\n",
    "L_1 = \\sum_{i} (y_i - \\sum_{j}x_{ij}W_j)^2 + \\lambda \\sum_j ||W_j||^2\n",
    "$$\n",
    "\n",
    "Overall, these are tehniques to prevent overfitting, alongside cross-validation sampling, reducting number of features, pruning, regularization. \n",
    "\n",
    "The regression model with L2 regularization is called `ridge regrssion`. \n",
    "\n",
    "Regularization adds penalty as model complexity increases. \n",
    "\n",
    "Ridge regularization forces weights to be small but non-zero, does not lead to sparse solution. \n",
    "\n",
    "Ridge regression is not robust to outliers, as square terms can blow up. \n",
    "\n",
    "Rdige regression performs better when all features are important and weghts are approximately equal. \n",
    "\n",
    "> Q8: what is R^2? (Where to use it and where not to?)\n",
    "\n",
    "R^2 is the measure of how close the predicted regression line to the data. It is also called `coefficient of determination`. \n",
    "\n",
    "R^2 is the percentage of the response variable variation that is explained by the linear model. \n",
    "\n",
    "R^2 is Explained variation / Total Variation and in (0,100%)\n",
    "\n",
    "The higher the R^2 the better the regression model fits the data\n",
    "\n",
    "$$\n",
    "R^2 = 1-\\frac{SS_{\\rm reg}}{SS_{\\rm tot}}\n",
    "$$\n",
    "\n",
    "where SS is the sum squred error\n",
    "\n",
    "or \n",
    "\n",
    "$$\n",
    "R^2 = 1-\\frac{SS_{\\rm res}}{SS_{\\rm tot}} = \\frac{\\sum(y_i-\\hat{y}_i)^2}{\\sum(y_i-\\bar{y}_i)^2}\n",
    "$$\n",
    "\n",
    "__Problem__: Note that R^2 increases as we add independent varaibles regardless of their importance. \n",
    "\n",
    "__Solution__: Adjusted R^2:\n",
    "\n",
    "$$\n",
    "R^2_{\\rm adj} = 1-\\frac{(1-R^2)(N-1)}{N-p-1}\n",
    "$$\n",
    "\n",
    "where $p$ is the humber of predictors, $N$ is the total sample size. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
