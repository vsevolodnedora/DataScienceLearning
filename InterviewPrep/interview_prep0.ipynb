{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Intervew perep. \n",
    "\n",
    "Based on the document called: \"Data Science Interview Questions (30 days of Interview Preparation) by INEURON.AI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1\n",
    "\n",
    "> Q1: What is the difference between AI, DS, ML, and DL  \n",
    "\n",
    "- AI is ... \n",
    "- ML is a subset of AI (including supervised, unsupervised, reinforcment)\n",
    "- DL is a subset of ML\n",
    "\n",
    "DS leverges these and other tools e.g., data integration, distributed achitectures, automated ML, vata visualization, engineering, ect to extract insights from the data valuable to bunsienss operation and decision making  \n",
    "\n",
    "> Q2: What are the differences between Supervised, unsupervised and Reinforcment Learning? \n",
    "\n",
    "- ML is a study of algorithms that allow a program to solve the problem without explicit instructons, -- by utilizing learned realtionships from data. ML models are data-centric model. \n",
    "\n",
    "- Supervised learning: the data used is laballed, meaning that there mapping between data and a label is given and a model learns to predict a label given new data. This includes \n",
    "    - Regression\n",
    "    - Classification\n",
    "\n",
    "- Unsupervised Learning uses unlabelled data and learns patterns within the data, extracting features based on co-occurace and underlying distributions. Used for \n",
    "    - Clustering\n",
    "    - Anomaly detection\n",
    "    - Association\n",
    "    - Autoencoders\n",
    "\n",
    "- Reinforcment Learning utulizes an agent that interacts with the environment and tries to learn an optimal policy based on the reward function. In other words, it tries to find the most optimal solution, by trying various tires and adjusting it based on the reward given.  \n",
    "\n",
    "ML models can be:\n",
    "- Parameteric (regressions, mlp...)\n",
    "- Non-parametric (decision trees...)\n",
    "\n",
    "> Q3: What is the general architecture of ML?\n",
    "\n",
    "1. Business understanding\n",
    "2. Data acquisition and understanding; model selection (data sourding, pipeline, environemnt, cleaning, data exploration)\n",
    "3. Modelling (\n",
    "    - feature engeneering, \n",
    "    - feature selection (using backward elimination, correlation, PCA, and domain knowledge), data scaling, \n",
    "    - model training, \n",
    "    - model evaluation (checking the accuracy of the model, confusion matrix, cross-validation))\n",
    "    - model/data adjustments to improve metrics\n",
    "4. Deployment; local, cloud, specific machine \n",
    "    - scoring, performance monitoring\n",
    "5. Collecting feedback, adjusting, CI/CD\n",
    "\n",
    "> Q4: What is linear regression?\n",
    "\n",
    "Fiding a linear $y=ax+b$ relationship between a predictor $x$ and a target $y$, where $a$ is a slope (or weight) and $b$ is the intercept (or bais)\n",
    "\n",
    "> Q5: what is OLS stats model (ordinary least squares)\n",
    "\n",
    "OLS is a stats model that helps identiry features with significat effect on the output. Calling t on data and calling .summary() we get the statistical properties.  See [medium](https://medium.com/swlh/interpreting-linear-regression-through-statsmodels-summary-4796d359035a) for a full breakdown.  \n",
    "\n",
    "It also gives \"t\" value for each feature. The higher the 't-value' the more significant is the feautre. It also gives the 'P' valaue, the low walue of which helps regecting the null hypothesis.  \n",
    "If 'p' value is < 0.05 we can generally assume feature to be significant.\n",
    "\n",
    "> Q6: What is L1 regularization (L1-lasso)?\n",
    "\n",
    "Input data has variance (variations...) and trends. The goal of a model is to learn the trend without overfitting on the variations, which would lead to a poor performacne on a new, unseen data with difference variations.  \n",
    "A way to do this is by employing regularization. The _Lasso Regularization_, aka, __Least Absolute Shrinkage and Selection Operator__ adds an absolute magnitude of coefficient as a penalty to the loss function. \n",
    "'Lasso regularization 'shrinks' the coefficient of 'non-important' features. \n",
    "See also [medium](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c). \n",
    "It can effectively remove unimportant features \n",
    "\n",
    "$$\n",
    "L_1 = \\sum_{i} (y_i - \\sum_{j}x_{ij}W_j)^2 + \\lambda \\sum_j |W_j|\n",
    "$$\n",
    "\n",
    "This method is usfull when there is large st of features, while tradiational methods, like corss-validation, stepwise regression are usefull for when there is small set of features.  \n",
    "\n",
    "> Q7: What is L2 (or Ridge) regularization? \n",
    "\n",
    "In L2 the L2 norm of the _squared magnitude_ is used as a penalty. \n",
    "See also [medium](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c) and [builtin](https://builtin.com/data-science/l2-regularization#:~:text=L1%20Regularization%2C%20also%20called%20a,term%20to%20the%20loss%20function.)\n",
    "\n",
    "$$\n",
    "L_1 = \\sum_{i} (y_i - \\sum_{j}x_{ij}W_j)^2 + \\lambda \\sum_j ||W_j||^2\n",
    "$$\n",
    "\n",
    "Overall, these are tehniques to prevent overfitting, alongside cross-validation sampling, reducting number of features, pruning, regularization. \n",
    "\n",
    "The regression model with L2 regularization is called `ridge regrssion`. \n",
    "\n",
    "Regularization adds penalty as model complexity increases. \n",
    "\n",
    "Ridge regularization forces weights to be small but non-zero, does not lead to sparse solution. \n",
    "\n",
    "Ridge regression is not robust to outliers, as square terms can blow up. \n",
    "\n",
    "Rdige regression performs better when all features are important and weghts are approximately equal. \n",
    "\n",
    "> Q8: what is R^2? (Where to use it and where not to?)\n",
    "\n",
    "R^2 is the measure of how close the predicted regression line to the data. It is also called `coefficient of determination`. \n",
    "\n",
    "R^2 is the percentage of the response variable variation that is explained by the linear model. \n",
    "\n",
    "R^2 is Explained variation / Total Variation and in (0,100%)\n",
    "\n",
    "The higher the R^2 the better the regression model fits the data\n",
    "\n",
    "$$\n",
    "R^2 = 1-\\frac{SS_{\\rm reg}}{SS_{\\rm tot}}\n",
    "$$\n",
    "\n",
    "where SS is the sum squred error\n",
    "\n",
    "or \n",
    "\n",
    "$$\n",
    "R^2 = 1-\\frac{SS_{\\rm res}}{SS_{\\rm tot}} = \\frac{\\sum(y_i-\\hat{y}_i)^2}{\\sum(y_i-\\bar{y}_i)^2}\n",
    "$$\n",
    "\n",
    "__Problem__: Note that R^2 increases as we add independent varaibles regardless of their importance. \n",
    "\n",
    "__Solution__: Adjusted R^2:\n",
    "\n",
    "$$\n",
    "R^2_{\\rm adj} = 1-\\frac{(1-R^2)(N-1)}{N-p-1}\n",
    "$$\n",
    "\n",
    "where $p$ is the humber of predictors, $N$ is the total sample size. \n",
    "\n",
    "> Q9: What is the mean square error? MSE?\n",
    "\n",
    "MSE is a measure how the goodness of the fit, and expressed as \n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n}\\sum_{i-1}^n (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "> Q10: Why support vector regression? SVR? Difference between SVR and linear regression?\n",
    "\n",
    "The main difference is that in SVR we are trying to minimize the error _within a certain area distance from the prediction_. \n",
    "Then, the best fit line is the line within $\\epsilon$-distance from which lies the majority of points. This aray at $\\epsilon$-distance is referred to as epsilon distance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q1 & Q2: What is Logistic Regression? Difference with linear regression?\n",
    "\n",
    "Logistic regrssion (or _logit_ model) is a type of statistical model, that estimates the probability of an event occuring. There, the dependenct variable is bound [0,1]. There, the _logit_ transforamtion is applied on the odds. The probability of success is devided by the probability of failer. This is called $\\log$ odds. \n",
    "\n",
    "$$\n",
    "logit(\\pi) = \\frac{1}{1+\\exp(-\\pi)} \\\\\n",
    "\\ln(\\pi/1-\\pi) = \\beta_0 + \\beta_1 x_1 + ... +\\beta_k x_k\n",
    "$$\n",
    "\n",
    "where $logit(\\pi)$ is the dependent or response variable and $x$ is the independent variable. The $\\beta$ parameters are estimated via _maximum likelihood estimation_ (MLE). \n",
    "\n",
    "The method iterates over combinations of $\\beta$(s) to find the best fit to log odds. In other words, the algorithm tries to maximize the log likelihood function. \n",
    "\n",
    "En interperting the log odds, it is common to transform them into odds ratio (OR), that represents the odds that an outcome will happen given a certain event comapred to the odds tha an outcome occure on the absnce of that event. \n",
    "\n",
    "The difference between linear and logistic regressions is the following:\n",
    "- Linear regression is applied to continous dependent varaibles. \n",
    "- logistic regression is applied to categorical variables (descrte). Categorical variables are [1,0], [\"T\", \"F\"]. Logistic regression prodced probability \n",
    "\n",
    "\n",
    "> Q3: Why can't we do classification using regression? \n",
    "\n",
    "Linear regression is unbounded, while logistic regression gives a probability between [0,1]. \n",
    "\n",
    "\n",
    "\n",
    "> Q4: What is the decision tree? \n",
    "\n",
    "See Articule by [IBM](https://www.ibm.com/topics/decision-trees)\n",
    "\n",
    "Decision tree is a _non-parameteric_ supervised learning algorithm, used for classifiction and regression. It builds a hierchical, tree-like, structure that consists of \n",
    "- root node\n",
    "- branches \n",
    "- internal nodels\n",
    "- leaf nodes (represent all possible outcomes)\n",
    "\n",
    "Decision tree utilises _divide and concure_ strategy and uses a _greedy search_ to identify opticam split points within a tree. The splitting is done _top-down_ in a recursive manner untill all (or most) outcomes have been classed with labels. \n",
    "\n",
    "A decision tree can overfit, when the deision tree becomes _impure_, i.e., when data points can no longer be classified as _homogenous_ sets. This is prone to occure as the desision tree complexity grows. Thus, decision trees try to build the smallest possible tree. \n",
    "\n",
    "_Prunning_ is commonly used to prevent overfitting in decesion trees, which is a process of removing branches that split on features with low importance. \n",
    "\n",
    "\n",
    "> Q5: What os entropy, information gain, ginin index, reducing impurity?\n",
    "\n",
    "Entropy, information gain and Gini impurity are the methods that used to chose the best attribute at each node. They are _splitting criteria_. \n",
    "\n",
    "- _Entropy_ comes from the information theory and is defined as \n",
    "$$\n",
    "Entropy(S) = -\\sum_{c\\in C} p(c)\\log_2(p(c)) \\in [0,1]\n",
    "$$\n",
    "where $S$ is the data set, $c$ is the class in set $S$, $p(c)$ is the proportion of data points that belong to the class to the total number of points in $S$.  \n",
    "Entropy is 0 if all points belong to a class and $1$ if 50\\% of points belong to a class.  \n",
    "The smaller the entropy the better this attribute for splitting. \n",
    "\n",
    "- _Information gain_ is the difference in entropy before and after splitting the tree on a given attribute. The attributre with the highest information gain is the best one to use for splitting. It is the best at classifying the data. \n",
    "$$\n",
    "InfoGain(S,\\alpha) - Entropy(S) - \\sum\\frac{|S_v|}{|S|}Entropy(S_v)\n",
    "$$\n",
    "where $\\alpha$ is a certain attribut or a class label, $|S_v|/|S|$ represents the proportion of the values in $S_v$ to the number of values in $S$. \n",
    "\n",
    "- _Gini Impurity_ is the probability of incorrectly classifying random data point in the dataset if it were labeled based on the class distribution of the dataset. Similar to entropy, if set, S, is pure-i.e. belonging to one class, then, its impurity is zero.\n",
    "$$\n",
    "GI = 1 - \\sum_i(p_i)^2\n",
    "$$\n",
    "\n",
    "\n",
    "> Q6: How to control leaf height and Prunning?\n",
    "\n",
    "- Use _Maximum Depth_ to control leaf hight\n",
    "- Use _Minimum split size_ to stop further slitting when a number of points at a node reaches a certain minimum value. This limits the tree growth. \n",
    "- Use _Minimum leaf size_ to limit split nodes when the number of points in one of the child nodes is lower than the minimum leaf size.\n",
    "\n",
    "Prunning is separated into:\n",
    "- Pre-prunning or _early stopping criteria_. Set before building a tree and it stops tree from growing when the criterion is met or when a pure class is discovered. \n",
    "- Post-prunning - grow the tree fully monitoring the _Complexity Parameter_ (CP) and than cutting the tree wit the optimal CP. \n",
    "\n",
    "\n",
    "> Q7: How to handle a decision tree for numerical and categorical data?\n",
    "\n",
    "For numerical data, the splitting of a tree is done using a threshold value.  \n",
    "\n",
    "At every split, the decision tree will take the best variable at that moment. This will be done according to an impurity measure with the split branches. And the fact that the variable used to do split is categorical or continuous is irrelevant (in fact, decision trees categorize continuous variables by creating binary regions with the threshold).  \n",
    "\n",
    "\n",
    "> Q8: What is the radnom forest algorithm? \n",
    "\n",
    "_Random Forest_ is an _ensemble machine learning algorithm_ that follows the _bagging technique_. The base estimators in the random forest are _decision trees_. Random forest randomly selects a set of features that are used to decide the best split at each node of the decision tree.\n",
    "\n",
    "Algorithm:\n",
    "1. Random subsets are created from the original dataset (bootstrapping).\n",
    "2. At each node in the decision tree, only a random set of features are considered to decide the\n",
    "best split.\n",
    "3. A decision tree model is fitted on each of the subsets.\n",
    "4. The final prediction is calculated by averaging the predictions from all decision trees.\n",
    "To sum up, the Random forest randomly selects data points and features and builds multiple trees\n",
    "(Forest).\n",
    "Random Forest is used for feature importance selection. The attribute (.feature_importances_) is\n",
    "used to find feature importance\n",
    "\n",
    "Some Important Parameters:\n",
    "1. __n_estimators:__  It defines the number of decision trees to be created in a random forest.\n",
    "2. __criterion__: \"Gini\" or \"Entropy.\"\n",
    "3. __min_samples_split__: Used to define the minimum number of samples required in a leaf\n",
    "node before a split is attempted\n",
    "4. __max_features__: It defines the maximum number of features allowed for the split in each\n",
    "decision tree.\n",
    "5. __n_jobs__: The number of jobs to run in parallel for both fit and predict. Always keep (-1) to\n",
    "use all the cores for parallel processing.\n",
    "\n",
    "\n",
    "> Q9: What is Variance and Bias tradeoff?\n",
    "\n",
    "It is a relationshipo between model's complexity, accuracy of the prediction and how well it generalizes to unseen data. In general, as we increase the model complexity the _bias_ reduces, and model fits the data better. However, after a certain point it tends to perform worse on previously unseen data, i.e., it has high _variance_. \n",
    "\n",
    "High bias is also referred to as _underfitting_ as model may miss the important trends in data. \n",
    "\n",
    "High variance is reffred to as _overfitting_ as model starts to fit the random fluctuations in traning data. \n",
    "\n",
    "The _bais-variance tradoff_ is a big problem in sueprvised learning. \n",
    "\n",
    "Note model complexity is not the number of parameters in the model. It is an indicator, but a poor one. \n",
    "\n",
    "The problem is similar to the _accuracy and precision_ relationship. \n",
    "\n",
    "- Accuracy is the description of the bias (improved by narrowing down dataset, where bias is low, which leads to _underfitting_).\n",
    "- Precision is the description of the variance (improved by enlarging the dataset). \n",
    "\n",
    "\n",
    "> Q10: What are ensemble methods?\n",
    "\n",
    "Using multiple learning algorithms and combining their predictions to obtain better result.  \n",
    "Particluar example is combining _decision trees_ into an esamble model.  \n",
    "Each decision tree uses its own feature selection and thresholds. \n",
    "\n",
    "Bsing examples: _Bagging_ and _Boosting_, ``decrease the variance of a single estimate as they combine several estimates from different models``. \n",
    "\n",
    "Types of Ensemble Methods:\n",
    "- Bagging = Bootstrap Aggregating\n",
    "    - Bagging is a combination of Bootstrap and Aggregation, where multiple bootstrapped samples are pulled from a dataset and a _decision tree_ is build on each bootstrapped subsample. After, an algorithm is used to aggregate over decision trees to form the most efficient prediction. \n",
    "    - Bagging employs _homogeneous_ weak lerners that learn independently, in parallel. \n",
    "    - __Example__: Random Forest: Similar to _bagging_ but feature selection for each tree is assigned randomly. This each tree has a statistically different subset of data to work with. This provides better ensemble to aggregate over. \n",
    "    I.e., the decision trees are split on different features. \n",
    "- Boosting:\n",
    "    - In boostong weak lerners, (aka, trees), learn sequentially and adaptively. \n",
    "    - Each new weak lerner is built to correct for the errors of the previos. The process continues untill either all data is predicted correctly or maximum number of weak lerners is reached. \n",
    "    - __Examples__: AdaBoost, Gradient Boosting, XGBoost\n",
    "\n",
    "Bagging and boosting are both:\n",
    "- ensemble methods to get N lerners from 1 lerner\n",
    "- generate several training data sets by random sampling\n",
    "- make final decision by averaging the N learniners (or using Majority Voting)\n",
    "- good for reducting varaince and provide higher stability\n",
    "\n",
    "\n",
    "> Q11: what is Support vector machines SVM Classification?\n",
    "\n",
    "SVMs (also called _Large margin classifier_) are a set of __supervised learning methods__ used for classification, regression and outliers detection.\n",
    "\n",
    "SVM is the _supervised_ __max-margin__ model with  assiciated learning algorithms for classification (but can also be used for regression, but there it is $\\epsilon$-sensitive). \n",
    "\n",
    "SVMs can perform _leanear_ and _non-lenear_ classification. using so-called _kernel trick_, implicitely mapping their inputs to a higher dimension feature sapce. \n",
    "\n",
    "The objective of the SVM is to find a _hyperplane_ in an N-dimensional space (where N is the number of features) that distinctly clssifies the data points. \n",
    "These hyperplanes are the _decision boundaries_ that separate the data. a\n",
    "\n",
    "__Advantages__:\n",
    "- Effective in high dimensional spaces.\n",
    "- Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "- Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "- _Versatile_: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "\n",
    "__Disadvantages__:\n",
    "\n",
    "- If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "\n",
    "- SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below).\n",
    "\n",
    "\n",
    "> Q12: What is Naive Bayes Classification and Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
