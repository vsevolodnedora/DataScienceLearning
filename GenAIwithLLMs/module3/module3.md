# Reinforcment learning with human feedback (RLHf)

RLHF allows to allign LLM with human values; avoid generating harmful text. 

### Aligning models with human values

Recall GenAI project lifecycle.  
W already examined fine-tuning of the LLM, which allows t oget a better performing model for a given task. 

Now we need to make sure that the model does not use
- Toxic language
- Aggressive void
- Dangerous information 

Sometines model completion is just not what is required.  
Sometimes model gives stright up wrong aswer, or harmful answer. 

Morel must have `HHH`: 
- Helpfullness
- Honesty
- Harmlessness

This _allignmant of the model wih human values_ is the last step of _Adapt and align model_ of the _GenAI project lifecycle_. 


### Reinforcement learning from human feedback (RLHF)

Consider text summarization task.  

Finetyming will improve the ability of the model to summarize.  
Research showed that _fine-tuning with human feedback_ gives __better permforming model__ than _initial fine-tuning_. 

`RLHF` is the main method to fine-tune model. There reinforment learning (RL) is used to fine-tune LLM with human feed-back data. This gives _human-aligned LLM_.  
_This gives model that_:   
- maximizes helpfulness relevance
- minimizes harm
- avoid dangerous topic
- personalization of LLM

#### RL

RL is a type of machine learning where an _agent_ learns to make decisions related to a specific _goal_ by taking _actions_ in an _environment_ with the objective of _maximze revard_ recieved for actions. 

Agent continously learns by taking actions and observing changes in the envirnment.  

This is iterative proces, that leads to agent learning an optimal strategy of taking actions or _policy_. 

Agent -> [make actions] -> Environment; 
[Reward & new state] -> Agent;

__Example__:  
train model to play Tic-Tac-Toe.  
- _Objective_: Wind the game
- _Agent_: player
- _Environment_: 3x3 game board
- _State_: card configuration of the board
- _Action space_ includes all possible positions a player can chose based on the current board state
- _RL policity_: stategy that agent follows to make decisions; take actions
- _rewards_ results of agent's actions

__GOAL__: agnet must learn optimal policy for given envirnment that optmizes the reward via an iterative process. 

`Playout/Rollout` a sereis of actions and new states whil an agent is learning.

Agent is gradually learning policy; taking initially random actions; that optimzes the _long-term_ reward. 


#### LLM finetiming with RL

__In LLM case__: 
- Agent RL policy = _LLM_
- Objetive: _generate alinged text_
- Envirnment = _Context window of the model_
- Envirnoment state = _current context_ (text at a given time in the context window)
- Action = _act of generating text_
- Action space = _tocken vocabulary_ (all tockes that a model can chose from to generate a completion)

Statistical representation of the language, learned by LLM in training, guides LLM generation of the next token in a sequence. 

At each time, generation of the next token depends on:
- prompt text in the context
- probability distribution over the vocabluary space

Reward is based on how closly the completion is aligned with human preferences. 

Determing the reward is complicated. There are many criteria, for example: txic or non-toxic (encoded as 0 or 1).  
LLM weights are then ipdated iteratively to _maximize_ the reward obained from the human classifier. 

In practice, an _additional, reward model_ is used to generate reward for an LLM. This model generats the _degree of alignment_ of the LLM output to act as a reward.

This _reward model_ is usually trained via supervised learning using human examples.

In the context of LLMs, sequence of ations is called `rollout`. 


### RLHF: Obtaining feedback from humans

1. Select an LLM
The model must have some capability to carry out the task. It is usefull to start with the model that has already been pre-trained on a large variaty of tasks. 

2. Take a prompt dataset and generate a set of responses. 
Each prompt from the set is associated with a set of completions. 

3. Collect feedback from human labelers on completions generated by LLM (this is human feedback). 
    - Define model alignment criterion 
    - Assess _each completion_ based on this criterion using humans as assessers. Labeler assignes _scores_ for each completion. This is done for all prompts in dataset. 
    - Repeat the work using many labelers to minimize the effect of human error/disagreement (Make instructions _clear_ for labelers to minimize errors; see instruction tempalte for labelers)
    - Note: more possible completions give more data to train the LLM

4. Data restructuring
Convert collected data into pair-wise set of completions (use). Note that the total number of pairs is 2^N, where N is the number of possible completions for a given prompt. A good completion is assigned '0' and a bed one '1'. Then there is a set of vectors with two numbers (good and bad) [0,1] for each pair. 
    - Re-order the prompt so that the preferred answer is alwas first. Prop. of the next tocken with the _updated LLM_ / probs. of the next tockes with the _initial LLM_
### RLHF: Reward model

After training an RLHF model, no _human in the loop_ is required. 
This is also usually an LLM trained using supervised learning.  
For a given prompt, the model learns to favor the human-preferred completion, while minimizing log(sigmoid) difference of the reward difference. 

After training, the reward model can be used as a binary classifier to provide a set of logits across positive and negative classes. Logts are _unnormalized_ model output, before applying any activation functions. 

Softmax to logids gives probabilities. 


### RLHF: Fine-tuning with reinforcement learning

Start with a model that already has a good performance, e.g., _an instruction fine-tuned LLM_. 

A single iteration of RLHF process looks like: 
1. Pass a prompt into an instruct LLM -> ger completion
2. Send prompt and completion into Reward LLM to evaluate the completion. 
3. High values are associated with more alligned response
4. Pass the reward score into the RL algorithm to update LLM weights.  
5. This generates an RL-updated LLM 

Process continues untill the model is alligned.  
This generates a `Human-alligned model`. 

There are several RL algorithms:
- Proximal policy optimization (PPO) 


### Optional video: Proximal policy optimization

> Complicated video

PPO stands for proximal policy optimization.  
It optimises a policy (an LLM) to be more more aligned with human preferences. 
PPO updates weights of the LLM. Updates are small and within a boundedregion. Thus, updated LLM is close to the previos version -- hence, it is _proximal_ policy optimization. This also leads in a _more stable learning_. 

PPO is started with initial LLM. 
PPO cycle
- Phase 1: LLM caries out a number of experiemnts; completing certain prompts
- Phase 2: update LLM weights

Expected reward of the completion is an important quantity used in PPO objective. It is estimated in a separate _head_ of an LLM, called _value function_. 

Assume a number of prompts are given. Then there responds. Then there are reward scores for given completion by reward model.  
The value function estimates the _expected total reward_ for a given state. As a model generates a tocken of the completion, a _total future reward_ needs to be estimated, based on the current sequence of tockens. 

This value function generates _fuiture total reward_ that is a _baseline_ that allows to estimate the quality of completions agains the _aligment_ criterion.   

Equation:
Loss value = 0.5 || [estimated] future total reward - [known] future total reward ||

In Phase 2; model is updated; impact of the update is evaluated. 
PPO makes sure to keep model updates within a small region; called _trust region_. Ideally, a series of small updates will move the LLM to generate higher rewards.

_PPO policy objective is the main ingredient of the method_.  

The objective is to _find a policy with high expected reward_.  

Policy = min( (Prop. of the next tocken with the _updated LLM_ / probs. of the next tockes with the _initial LLM_) * (Advantage term), clip(two policies are near each other; they are guardrails; trust region) )

Advantage term = how much better the current action with respect to all possible actions at that state. 

Advantage estimate is valid _only if_ old and new policies are close to each other.

Policy includes _guardrails_ that makes sure that the model is unlikely to leave `trust region`. 

Note: policy loss moves the model towards alignemnt. Entropy loss allows the model to _retail creativity_. 

If the entropy is low, it is possible that the prompt is always completed in the same way. This is similar to temperature settings in LLM. However, _entropy_ influences the model __during training__. 

PPO loss = Policy Loss + c1*(Values loss) + c2*(Entropy loss)

Where c1 and c2 are hyperparameters.

PPO objective updates the model weight via back propagation over several steps. 

It is popular becase it has a balance between complecity and performance

#### Other methods
- Q-learning 
- Direct preference optimization