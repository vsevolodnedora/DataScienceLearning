# Reinforcment learning with human feedback (RLHf)

RLHF allows to allign LLM with human values; avoid generating harmful text. 

### Aligning models with human values

Recall GenAI project lifecycle.  
W already examined fine-tuning of the LLM, which allows t oget a better performing model for a given task. 

Now we need to make sure that the model does not use
- Toxic language
- Aggressive void
- Dangerous information 

Sometines model completion is just not what is required.  
Sometimes model gives stright up wrong aswer, or harmful answer. 

Morel must have `HHH`: 
- Helpfullness
- Honesty
- Harmlessness

This _allignmant of the model wih human values_ is the last step of _Adapt and align model_ of the _GenAI project lifecycle_. 


### Reinforcement learning from human feedback (RLHF)

Consider text summarization task.  

Finetyming will improve the ability of the model to summarize.  
Research showed that _fine-tuning with human feedback_ gives __better permforming model__ than _initial fine-tuning_. 

`RLHF` is the main method to fine-tune model. There reinforment learning (RL) is used to fine-tune LLM with human feed-back data. This gives _human-aligned LLM_.  
_This gives model that_:   
- maximizes helpfulness relevance
- minimizes harm
- avoid dangerous topic
- personalization of LLM

#### RL

RL is a type of machine learning where an _agent_ learns to make decisions related to a specific _goal_ by taking _actions_ in an _environment_ with the objective of _maximze revard_ recieved for actions. 

Agent continously learns by taking actions and observing changes in the envirnment.  

This is iterative proces, that leads to agent learning an optimal strategy of taking actions or _policy_. 

Agent -> [make actions] -> Environment; 
[Reward & new state] -> Agent;

__Example__:  
train model to play Tic-Tac-Toe.  
- _Objective_: Wind the game
- _Agent_: player
- _Environment_: 3x3 game board
- _State_: card configuration of the board
- _Action space_ includes all possible positions a player can chose based on the current board state
- _RL policity_: stategy that agent follows to make decisions; take actions
- _rewards_ results of agent's actions

__GOAL__: agnet must learn optimal policy for given envirnment that optmizes the reward via an iterative process. 

`Playout/Rollout` a sereis of actions and new states whil an agent is learning.

Agent is gradually learning policy; taking initially random actions; that optimzes the _long-term_ reward. 


#### LLM finetiming with RL

__In LLM case__: 
- Agent RL policy = _LLM_
- Objetive: _generate alinged text_
- Envirnment = _Context window of the model_
- Envirnoment state = _current context_ (text at a given time in the context window)
- Action = _act of generating text_
- Action space = _tocken vocabulary_ (all tockes that a model can chose from to generate a completion)

Statistical representation of the language, learned by LLM in training, guides LLM generation of the next token in a sequence. 

At each time, generation of the next token depends on:
- prompt text in the context
- probability distribution over the vocabluary space

Reward is based on how closly the completion is aligned with human preferences. 

Determing the reward is complicated. There are many criteria, for example: txic or non-toxic (encoded as 0 or 1).  
LLM weights are then ipdated iteratively to _maximize_ the reward obained from the human classifier. 

In practice, an _additional, reward model_ is used to generate reward for an LLM. This model generats the _degree of alignment_ of the LLM output to act as a reward.

This _reward model_ is usually trained via supervised learning using human examples.

In the context of LLMs, sequence of ations is called `rollout`. 


### RLHF: Obtaining feedback from humans

1. Select an LLM
The model must have some capability to carry out the task. It is usefull to start with the model that has already been pre-trained on a large variaty of tasks. 

2. Take a prompt dataset and generate a set of responses. 
Each prompt from the set is associated with a set of completions. 

3. Collect feedback from human labelers on completions generated by LLM (this is human feedback). 
    - Define model alignment criterion 
    - Assess _each completion_ based on this criterion using humans as assessers. Labeler assignes _scores_ for each completion. This is done for all prompts in dataset. 
    - Repeat the work using many labelers to minimize the effect of human error/disagreement (Make instructions _clear_ for labelers to minimize errors; see instruction tempalte for labelers)
    - Note: more possible completions give more data to train the LLM

4. Data restructuring
Convert collected data into pair-wise set of completions (use). Note that the total number of pairs is 2^N, where N is the number of possible completions for a given prompt. A good completion is assigned '0' and a bed one '1'. Then there is a set of vectors with two numbers (good and bad) [0,1] for each pair. 
    - Re-order the prompt so that the preferred answer is alwas first. Prop. of the next tocken with the _updated LLM_ / probs. of the next tockes with the _initial LLM_
### RLHF: Reward model

After training an RLHF model, no _human in the loop_ is required. 
This is also usually an LLM trained using supervised learning.  
For a given prompt, the model learns to favor the human-preferred completion, while minimizing log(sigmoid) difference of the reward difference. 

After training, the reward model can be used as a binary classifier to provide a set of logits across positive and negative classes. Logts are _unnormalized_ model output, before applying any activation functions. 

Softmax to logids gives probabilities. 


### RLHF: Fine-tuning with reinforcement learning

Start with a model that already has a good performance, e.g., _an instruction fine-tuned LLM_. 

A single iteration of RLHF process looks like: 
1. Pass a prompt into an instruct LLM -> ger completion
2. Send prompt and completion into Reward LLM to evaluate the completion. 
3. High values are associated with more alligned response
4. Pass the reward score into the RL algorithm to update LLM weights.  
5. This generates an RL-updated LLM 

Process continues untill the model is alligned.  
This generates a `Human-alligned model`. 

There are several RL algorithms:
- Proximal policy optimization (PPO) 


### Optional video: Proximal policy optimization

> Complicated video

PPO stands for proximal policy optimization.  
It optimises a policy (an LLM) to be more more aligned with human preferences. 
PPO updates weights of the LLM. Updates are small and within a boundedregion. Thus, updated LLM is close to the previos version -- hence, it is _proximal_ policy optimization. This also leads in a _more stable learning_. 

PPO is started with initial LLM. 
PPO cycle
- Phase 1: LLM caries out a number of experiemnts; completing certain prompts
- Phase 2: update LLM weights

Expected reward of the completion is an important quantity used in PPO objective. It is estimated in a separate _head_ of an LLM, called _value function_. 

Assume a number of prompts are given. Then there responds. Then there are reward scores for given completion by reward model.  
The value function estimates the _expected total reward_ for a given state. As a model generates a tocken of the completion, a _total future reward_ needs to be estimated, based on the current sequence of tockens. 

This value function generates _fuiture total reward_ that is a _baseline_ that allows to estimate the quality of completions agains the _aligment_ criterion.   

Equation:
Loss value = 0.5 || [estimated] future total reward - [known] future total reward ||

In Phase 2; model is updated; impact of the update is evaluated. 
PPO makes sure to keep model updates within a small region; called _trust region_. Ideally, a series of small updates will move the LLM to generate higher rewards.

_PPO policy objective is the main ingredient of the method_.  

The objective is to _find a policy with high expected reward_.  

Policy = min( (Prop. of the next tocken with the _updated LLM_ / probs. of the next tockes with the _initial LLM_) * (Advantage term), clip(two policies are near each other; they are guardrails; trust region) )

Advantage term = how much better the current action with respect to all possible actions at that state. 

Advantage estimate is valid _only if_ old and new policies are close to each other.

Policy includes _guardrails_ that makes sure that the model is unlikely to leave `trust region`. 

Note: policy loss moves the model towards alignemnt. Entropy loss allows the model to _retail creativity_. 

If the entropy is low, it is possible that the prompt is always completed in the same way. This is similar to temperature settings in LLM. However, _entropy_ influences the model __during training__. 

PPO loss = Policy Loss + c1*(Values loss) + c2*(Entropy loss)

Where c1 and c2 are hyperparameters.

PPO objective updates the model weight via back propagation over several steps. 

It is popular becase it has a balance between complecity and performance

#### Other methods
- Q-learning 
- Direct preference optimization


### RLHF: Reward hacking

Recap:
RLHF - fine-tuning process to allign LLM with human values.  
Use _reward model_ to assess model completions agains human preference metric. 
Use _RL_ aka PPO to update weights of LLM.  
Use multi-iteration cycle. Untill the desired degree of alignment is achieved. 

In RL there exists `reward hacking` where an agent learns to _chear_ to facour actions that maximize the reward but _do not follow_ original objective.  
_Example_: addition of workds to completion that increase the score for the metric that is being used for alignment.  

#### Avoiding reward hacking

Consider a _reference model_ that has weights frozen and is used to evaluate the output of the RL-updated model. Then, at each iterations compare the completions of the reference and aligned model using _KL Divergence_ (shift penalty).  
`KL (Kullback-Leibler) Divergence` is a statistical measure of hau different two statistical distributions are. It allows to see how far the updated model has diverged from the reference (The entire LLM is used for computing it).  
KL-Dirvergence is often used in RL, especially with PPO, where it helps guiding optimization process to ensuire that the updated policy does not deviate too much from the original one.  
In PPO the ipdate is _iterative_ to ensure stability. Constraints, enforced by KL-Divergence are used to ensire that the iterations are small. 

KL divergence is calculated _for each tocken_ in the vocabluary of the LLM. Using the _soft-max_ the probability is smaller than the whole vocabluary size. This is still compute expensive. After, computing, it is added as an extra term to the reward function. This penalizes the model if it shifts too far from the original one. 

__NOTE__: two full copies of the LLM are required for this task.  

__NOTE__: RLHF can be combined with PEFT; then _only_ the PEFT adapter weights are changed, not of the full LLM. 

This allows us to re-use LLM for both tasks and to have just one LLM. 

To evaluiate the final model, _Summarization Dataset_ can be used to assess the model perforamce using the number, e.g., _toxicity score_. 
1. Create a _baseline_ toxicity score for an original LLM by evaluating its completions of the summaization dataset with the reward model that can assess toxic language
2. Evaluate human allinged model on the same dataset. 
3. Compare toxicity scores. 


### Scaling human feedback

Reward model requires a lot of huma-labelled data to be trained. (1000+ people an 10+ propmpts for each). This is expansive to gather. Human effort is a limited resource. 
Solutions: _Scaling through model self-supervision_.  
- `Consittutional AI` - Training a reward model using _set of rules_ in principles that guven the model behaviour. Train the model to self-critque and revise its responses to comply with these principles. 

__NOTE__: it is also usefull It is usefull with scaling data and address other limitations of alligned LLM.  
An alligned LLM may provide _harmful information_ as it tries to anaser the quations as best as it can. E.g., a user politely asked to learn to jailbreak and the model alligned for helpfulness does help. A model with _consitutional principles_ can balance competing interests and minimize the harm. 

Training Consitutional AI model has two stages
- Supervised model: prompt the model trying to generate harmfull responses: `Red Teaming`. 
- Ask the model to critique its own harmfull responses; revise them to comply with the rules. 
- Fine-tune the model provided Parir of prompts with harmfull and constitutional responses using RL. 
This process sometimes called `Reinforcment learning with AI feedback (RLAIF)`

At the end, the new, _Reward model_ is trained that can later be used to further align the original fine-tuned LLM to get Consitutional aligned LLM

