# LLM-powered applications

This is the last stage of the Generatative AI project life-cycle.  
_Application integration_.  
- Optimize and deploy model for inference (adress: spead, compute, trade-offs)
- Augment model and build LLM-powered applications (address: external data, connecting to external resources) (address: API, model consumption)

Consider _model optimization_.  
LLMs requrie large compute and storage; speed for inference must be low. 
- On premises (eg., edge devices)
- On a cloud

Reduce model size (reduces loading time/latency). Challenge: maintain performance.  
Techniques: 
- __Distillation__: use a larger _Teacher LLM_ to teach a smaller _Student LLM_.  The smaller one can be used for inference. It learns to _statistically mimic_ the hehaviour of the larger model. 
    - Start with fine-tuned LLM as a teacher model
    - Create a smaller student model
    - Freeze the teacher model wieghts and create a training dataset with it
    - Generate completions with student model. 
    - Apply _distillation loss_ functions to the completions to train the student model. This is done using the tocken districution of the teacher model _soft-max layer_.
    - As teacher model is already fine-tuned; it does not have much variation in its soft-max layer with respect to the ground trooth. Thus, _temperature_ is added, to increase the model _creativity_. 
    This givens `Soft labels` (teacher model output) and `Soft predictions` (completions generated by student model) and `Distillation loss` between them; 
    - In parallel, train the student LLM to generate predictions based on the _ground truth training data_ with temperatue 1. This gives `hard predictions` and `hard labels` and `Student loss` between them.
    - Both, Distillation and Stident Losses are used to update weights of the student model via back-prop.
    - The key benifit is the reduced model size and compleixty
    - Dowside: model is not effective for generative decoder tasks; only for encoder-only models; e.g., BERT, that have _a lot of represntation reduncancy_. 
- __Post-training quantization__ (PTQ) form a lower-precision representation of modelweights; this reduces model size and compute. It can be applied to just model weights, or weights and activation layers as well. The latter affects the performance of the model more strongly. 
    - Requries extra _calibration step_ to statistically capture the dynamic range of the original parameter values. 
- __Model prunning__: remove parts of the model that do not contribute much to the performance. 
    - Generally, weights with values close to 0 can be removed. Some methods, however, require full re-training of the mode, while others can be done with PEFT/LoRA. 
    - Post-training prunning 
    - This reduces size and improves performance.
    - It depends on the model state (how many 0 weights there are)


### Generative AI Project Lifecycle Cheat Sheet

1. The most time-consuming and expensive part is _model pre-training_. It is recommneded to start with pre-existing model. Than this part can be skipped. 
2. Assess model performacne with _model engineering_. 
3. Prompt-tuning and fine-tuning to improve model performance (full FT, PEFT); can be completed within a single day.
4. RLHFl If reward model needs to be trained, it will take time, (gathering human fedback). So first, check if a reward model already exists. 
5. Allignment; quick if model cahnges are not large. 


### Using the LLM in applications

Models have the following limitations: 
- Pre-training limits the model knowladge of current events. Out-of-date knowladge. 
- Complex mathematical tasks. Models do not carry out mathematical operations and do not give correct answer. 
- Halucination: model gives answer when it does not know an answer. 

__Solutions:__ 
- eonnect model to external data-sources/tools (components).  

Application:  
User input -> `Orchestration Library` -> {LLM, tools, data, API, etc} -> output  

Example of an orchestration library is _LangChain_.  

#### Connecting LLM to external datasources

Retrieval-augmented generation (RAG) - framework to build LLM-powered systems that make use of external datasources and applications.  
This avoids model retraining on new data.   
Access to new data is given _at inference_ time.  
This improves model completion accuracy and relevance. 

Consider one of the origianl RAG implementation by Lewis et al 2020 "Retrival-Augmented Generation for Knowladge-Intensive NLP Tasks".  

Main componrnt: `Retriever` that consists of a 
- Query encoder (tasks user's input prompt, encodes it into a form that can be used to get the data)
- External information sources (vector store, SQL database, CSV file)
Components are _trained together_ to learn to find the most relevant documents to the input quiry documents. 
- Retiever returns a single (group of) docs; combines the new infor with the original user quiry. 
- The retrieved info is combined with the original quiry and passed to LLM (making an expended prompt). 

RAG allows to augment the model with 
- External documents (private Wikis, expert systems)
- Access to internet (wikipedia)
- Web bages
- Databases
- Vector Store (usefull for LLMs, as those work with vector representation of language).  

RAG implementations are complex: 
- Data must fit inside the context window. Most text sources are way too long. External datasources are changed, each of which can be fit into the context window. 
- Dat must be in the format that allows its _relevance_ to be assessed at inference time: __Embedding vectors__. Remember, LLM work with _vector representation of the text_ not with the text itself. This allows LLM to find _semantically related_ words via e.g., cosine similarity. RAG takes small chunks of data and processes them via LLM to create embedding vectors for each. New represenations of data are stoed in _vector stores_ which allow for fast search and retrieval.  
`Vector database` is a particluar implementation of a vector store, where each vector also has a `key`. This allows for the RAG generated completion to include citations to the original document. 


### Interacting with external applications
Consider customer servace bot.  

Connecting an LLM with external applications rquires an LLM to be able t otrigger actions, e.g., use python interpreter.   
The heart is _prompt and completion_.  

LLM,the application's reasoning engine, determins what actions to take.  
Actions are generated, if LLM completions contain certain important information:
- Plan actions ( set of instructions ); Understandable and corresponding to _allowed_ actions. 
- Completion must be _formated_ in a way that broader application understands it. e.g., sentence sturction, script in python or SQL command. 
- Collect information to _Validate an action_. E.g., ask user for extra information to validate the retrieved answer. For all tasks _prompt engineering_ is requried.


### Helping LLMs reason and plan with chain-of-thought

An LLM must _reason through the stesp_ an applications must take to achieve final result.  
Multiple-step reasoning with math involved are very challenging for LLMs.  

__NOTE__: such complex tasks also benefit from prompt engineering and on/few-shot inference.  

One approach that was found usefull is to prompt a model to _think like a human_ i.e., break down the problem into small subproblems/steps.  
There in a example solution given in a prompt, several steps are presented to guide the model through required thinking stages/resoning steps. The full set of steps is called `chain of through`.  

`Chain of thought prompting` - asking a model to mimic this behaviour: set of reasoning steps. Instructions to teach a model to _reason through the task_. This is included in a few-shot reasoning step to form a _chain-of-thought prompt_.   
This helps solving math, physics etc.  

Limited math skills can still be a problem, if a math is complex.  


### Program-aided language models (PAL)

An LLM is not a calculator. It just tries to predict the next tocken to complete the prompt.  

A model can interact with _external application_ that is better at math. 

This generally refers to `programm-aided LLM` or PAL (see Luyu Gao abd collaborators). Pair an LLM with code interpreter. A model uses _chain-of-thought_ prompting to generate usable Python script, that in trun are passed into an interpreter to execute.  
An LLM generates comletions where reasoning steps are accompanied by a computer code.  
__NOTE__ model output must be specified. This is done via one or few-shot inference. 

Inference with PAL requires an LLM to have 
- a prompt with one or more example. Each example must include a question, and resoning steps in lines of python code (as comments). 
- Append a new question/problem to the prompt.  
- Pass it to LLM that generates a completion in a form of a python script
- Pass script to a python interpreter.  
- Append text with the answer into the initial prompt we started with. 
- Pass  updated prompt into LLM
- The final competion will contain the correct answer. 

This is a powerfull technique for more complex math problems.  

Automation of this problem involves an `orchistrator`, a technical component that manages the flow of information, and initiation of calls to external data sources or applications. It chooses what actions to take based on the output of an LLM. 
- LLM is an application _reasoning_ engine (it creates a plan that orchestrator with interprete and execute)

In reality, many external data sources and multiple dicision points, validation actions and calls to other applications must be managed. 


### [ReAct](https://arxiv.org/abs/2210.03629): Combining reasoning and action 

`ReAct` is a _prompting strategy_ that combines chain-of-thought resoning with action planning. Based on ap paper that proposes a method to aswer question from _Hot Pot QA_ - multi-step question-answering benchmark. It requries reasoning from multiple Wiki passages.  
It uses _structured examples_ to show LLM how to reason through a prblem and _decide_ what actions to take to solve it.  
In that case, allowed actions were:
- Search (look for a Wiki entry about a particular topic)
- lookup (search for a string on a wiki page)
- finish (model has found an aswer)

The prompt has the following structure:
- Question
- Thought
- Action
- Observation

The cycle is repeated untill the final answer is found.  

Note: an LLM can only chose a limited number of actions that is specified in the example prompt text. 

#### Summary:

1. Start with the ReAct example prompt (depending on the LLM used, multiple examples might be needed).
2. Pre-append the instructions
3. Append the question to be answered at the end. 

The full prompt can then be used for inference. 

#### LangChain

A framework to develop applications powered by LLMs.  
It provides one with modular pieces that contain components needed to work woth LLms:
- prompt examples
- memory (to store interactions with LLM)
- prebuilt tools:
    - calls to dataset and APIs
combining all of it results in a Chain. 

There are several pre-made Chains optimized for different use-cases. 


If an application might need multiple passess to fullfill the user query, a different approach is needed, as the user moves through the workflow. Here one can use `Agent` construct from LangChain, to read the input from the user and determine which tool or tools to use to complete the task. There are Agents for PAL and ReAct.  
Agents can also be chained.  
This is under active development.  

The ability of the model to reason depends on the model Size. 


### LLM application architectures

Building blocks to create an LLM powered applications:
- Infrastructure layer (provides the compute, storage); Serving LLM; storing application components. Either on-premises or cloud. 
- LLM layer (foundation models or models adapted for specific tasks); deployed on a specific infrastructure (based on the task - real-time or near real-time inference); external sources for RAG; Completions are returned to the user/application; mechanisms to capture and store outputs. Stored user completions may be used to _augment_ the fixed window size of the LLM. 
- Additional tools and frameworkds for LLMs (PAL, ReAct, LangChain Model Hubs); Model Humbs allow to centrally manage model
- User interface through which an application can be consimed (RestAPI or a webpage); Security components needed. 

This is an application stack. 


### Optional video: AWS Sagemaker JumpStart

It is a _Model Hub_ that allows to build and deply foundation models and integrate them with an application. Allows also for fine-tuning and deplpoyments. 
Note. It requies GPUs that are served _on-demand_ and subjected the samy type of pricing. Cost monitorying best practicies are needed to optimize cost. 