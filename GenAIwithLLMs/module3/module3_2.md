# LLM-powered applications

This is the last stage of the Generatative AI project life-cycle.  
_Application integration_.  
- Optimize and deploy model for inference (adress: spead, compute, trade-offs)
- Augment model and build LLM-powered applications (address: external data, connecting to external resources) (address: API, model consumption)

Consider _model optimization_.  
LLMs requrie large compute and storage; speed for inference must be low. 
- On premises (eg., edge devices)
- On a cloud

Reduce model size (reduces loading time/latency). Challenge: maintain performance.  
Techniques: 
- __Distillation__: use a larger _Teacher LLM_ to teach a smaller _Student LLM_.  The smaller one can be used for inference. It learns to _statistically mimic_ the hehaviour of the larger model. 
    - Start with fine-tuned LLM as a teacher model
    - Create a smaller student model
    - Freeze the teacher model wieghts and create a training dataset with it
    - Generate completions with student model. 
    - Apply _distillation loss_ functions to the completions to train the student model. This is done using the tocken districution of the teacher model _soft-max layer_.
    - As teacher model is already fine-tuned; it does not have much variation in its soft-max layer with respect to the ground trooth. Thus, _temperature_ is added, to increase the model _creativity_. 
    This givens `Soft labels` (teacher model output) and `Soft predictions` (completions generated by student model) and `Distillation loss` between them; 
    - In parallel, train the student LLM to generate predictions based on the _ground truth training data_ with temperatue 1. This gives `hard predictions` and `hard labels` and `Student loss` between them.
    - Both, Distillation and Stident Losses are used to update weights of the student model via back-prop.
    - The key benifit is the reduced model size and compleixty
    - Dowside: model is not effective for generative decoder tasks; only for encoder-only models; e.g., BERT, that have _a lot of represntation reduncancy_. 
- __Post-training quantization__ (PTQ) form a lower-precision representation of modelweights; this reduces model size and compute. It can be applied to just model weights, or weights and activation layers as well. The latter affects the performance of the model more strongly. 
    - Requries extra _calibration step_ to statistically capture the dynamic range of the original parameter values. 
- __Model prunning__: remove parts of the model that do not contribute much to the performance. 
    - Generally, weights with values close to 0 can be removed. Some methods, however, require full re-training of the mode, while others can be done with PEFT/LoRA. 
    - Post-training prunning 
    - This reduces size and improves performance.
    - It depends on the model state (how many 0 weights there are)


### Generative AI Project Lifecycle Cheat Sheet

1. The most time-consuming and expensive part is _model pre-training_. It is recommneded to start with pre-existing model. Than this part can be skipped. 
2. Assess model performacne with _model engineering_. 
3. Prompt-tuning and fine-tuning to improve model performance (full FT, PEFT); can be completed within a single day.
4. RLHFl If reward model needs to be trained, it will take time, (gathering human fedback). So first, check if a reward model already exists. 
5. Allignment; quick if model cahnges are not large. 


### Using the LLM in applications

Models have the following limitations: 
- Pre-training limits the model knowladge of current events. Out-of-date knowladge. 
- Complex mathematical tasks. Models do not carry out mathematical operations and do not give correct answer. 
- Halucination: model gives answer when it does not know an answer. 

__Solutions:__ 
- eonnect model to external data-sources/tools (components).  

Application:  
User input -> `Orchestration Library` -> {LLM, tools, data, API, etc} -> output  

Example of an orchestration library is _LangChain_.  

#### Connecting LLM to external datasources

Retrieval-augmented generation (RAG) - framework to build LLM-powered systems that make use of external datasources and applications.  
This avoids model retraining on new data.   
Access to new data is given _at inference_ time.  
This improves model completion accuracy and relevance. 

Consider one of the origianl RAG implementation by Lewis et al 2020 "Retrival-Augmented Generation for Knowladge-Intensive NLP Tasks".  

Main componrnt: `Retriever` that consists of a 
- Query encoder (tasks user's input prompt, encodes it into a form that can be used to get the data)
- External information sources (vector store, SQL database, CSV file)
Components are _trained together_ to learn to find the most relevant documents to the input quiry documents. 
- Retiever returns a single (group of) docs; combines the new infor with the original user quiry. 
- The retrieved info is combined with the original quiry and passed to LLM (making an expended prompt). 

RAG allows to augment the model with 
- External documents (private Wikis, expert systems)
- Access to internet (wikipedia)
- Web bages
- Databases
- Vector Store (usefull for LLMs, as those work with vector representation of language).  

RAG implementations are complex: 
- Data must fit inside the context window. Most text sources are way too long. External datasources are changed, each of which can be fit into the context window. 
- Dat must be in the format that allows its _relevance_ to be assessed at inference time: __Embedding vectors__. Remember, LLM work with _vector representation of the text_ not with the text itself. This allows LLM to find _semantically related_ words via e.g., cosine similarity. RAG takes small chunks of data and processes them via LLM to create embedding vectors for each. New represenations of data are stoed in _vector stores_ which allow for fast search and retrieval.  
`Vector database` is a particluar implementation of a vector store, where each vector also has a `key`. This allows for the RAG generated completion to include citations to the original document. 


### Interacting with external applications
Consider customer servace bot.  

Connecting an LLM with external applications rquires an LLM to be able t otrigger actions, e.g., use python interpreter.   
The heart is _prompt and completion_.  

LLM,the application's reasoning engine, determins what actions to take.  
Actions are generated, if LLM completions contain certain important information:
- Plan actions ( set of instructions ); Understandable and corresponding to _allowed_ actions. 
- Completion must be _formated_ in a way that broader application understands it. e.g., sentence sturction, script in python or SQL command. 
- Collect information to _Validate an action_. E.g., ask user for extra information to validate the retrieved answer. For all tasks _prompt engineering_ is requried.

