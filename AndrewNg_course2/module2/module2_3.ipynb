{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced optimization\n",
    "\n",
    "- `ADAM` (adaptive moment estimation) Automatically adjust $\\alpha$ when the algorithm sees that the $\\alpha$ is too small and changes in derivatives are too small. Each parameter has its own learning parameter with its own corrections. \n",
    "$\\texttt{model.compile(optimizer=tf.keras.optimizers.Adam(learning\\_rate=1e-3))\\dots}$ where $\\alpha$ here is the initial global value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional layer types\n",
    "- `Dense()` (when each neuron is connected to another)  \n",
    "$\\vec{a}_{[2]}^{1} = g(\\vec{w}_1^{[2]}\\cdot\\vec{a}^{[1]}+b_1^{[2]})$\n",
    "- `Convolutional layer` Each neutorn is connected to a **limited** neutrons from previous layer. This speeds up computation, uses less training data and is less prone to overfitting. For example, consider a 1D data, a EKG signal. Classification problem. Given $\\{x_i\\}$ and $\\{y_i\\}$. Each layer is than given its own `window` of the input signal. The second layer also applies a _window_ to the output of the previous layer. Finally, the final layer uses all previous layers. **Note** here you need to specify the window size. This is another free parameter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back propagation algorithm\n",
    "\n",
    "- Computation graph. Steps to compute a cost $J(w,b)=0.5(a-y)^2$ step by step, as  \n",
    "$\\rightarrow[c = wx]\\rightarrow[a=c+b]\\rightarrow[d=a-y]\\rightarrow[J=0.5d^2]\\rightarrow$. Set of nodes, connected by agents. \n",
    "This is forward propagation graph. \n",
    "Next, derivatibes are needed. $\\partial J / \\partial w$. Doing this you are goind bachwards on the graph, i.e., `back propagation`(using chain rule for derivatives $\\partial J/\\partial a = \\partial d/\\partial a \\times \\partial J \\partial d$)   \n",
    "$\\rightarrow[c = wx]\\rightarrow[\\partial J/\\partial c, \\partial J/\\partial b]\\rightarrow[\\partial J/\\partial a]\\\\leftarrow[\\partial J/\\partial d]\\leftarrow$ slight change in $J$\n",
    "\n",
    "Backprob is an efficient way to compute derivatives. Because $\\partial J/\\partial a$ is computed only once for both $\\partial J/ \\partial w$ and $\\partial J/ \\partial b$. Thus, if there are $N$ node, and $P$ parameters, the derivative needs $N+P$ steps and not $N\\times P$ steps.  \n",
    "\n",
    "`Backprob algorithm done using computational graph is a more efficient way to compute derivatives`. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Neural Network \n",
    "\n",
    "Done using automatic differentiation via backprob."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
