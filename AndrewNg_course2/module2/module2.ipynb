{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a NN with TensorFlow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a logistic reg. model\n",
    "1. Compute model $f_{\\vec{w},b}(\\vec{x})$ as  \n",
    "$\\texttt{z = np.dot(w,x)+b}$  \n",
    "$\\texttt{fx = 1 / (1+np.exp(-z))}$  \n",
    "$\\textcolor{yellow}{\\texttt{model = Sequential([ Dense(units=20, activation='sigmoid') \\\\ Dense(...) ])}}$\n",
    "2. Specify the loss $L(f_{\\vec{w},b}(\\vec{x})),y$ to see how well model performs on a **single** data point from a dataset as  \n",
    "$\\texttt{loss = - y * np.log(fx) - (1-y) * np.log(1-np.lo(1-fx))}$  \n",
    "This is for **binary classification** and is called `binary crossentropy loss function` (termin from statistics)\n",
    "And compute **cost function** as an average of a loss fonction computed for all training examples\n",
    "$J(\\vec{w},b) = \\frac{1}{m}\\sum_{i=1}^m L(f_{\\vec{w},b}(\\vec{x}^{(i)}),y^{(i)})$   \n",
    "$\\textcolor{yellow}{\\texttt{model.compile(loss=BinaryCrossentropy())}}$\n",
    "3. Use an algorithm, i.e., **gradient descend** to minimize the cost function $J(\\vec{w},b)$. In Tensorflow it is done via `back propagation`  \n",
    "$\\texttt{w = w - alpha * djdw}$  \n",
    "$\\texttt{b = b - alpha * djdb}$  \n",
    "$\\textcolor{yellow}{\\texttt{model.fit(X,y,epochs=100)}}$\n",
    "\n",
    "Other loss functions are available in tensorflow, e.g., $\\texttt{MeanSquareError()}$  \n",
    "Tensortflow uses back-propagation to compute partial derivatives\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Activation Functions\n",
    "When there is a range of outputs is possible $[0,1]$ that takes more than 2 parameters. Instead of `sigmoid function`, use `relu` $g(z)=\\max(0,z)$ (rectified linear unit).  \n",
    "Most common are:  \n",
    "- Linear activation function $g(z) = z =\\vec{w}\\cdot\\vec{x}+b$ (no activation function...)\n",
    "- Sigmoid $g(z) = 1 / (1+e^{-z})$ $0<g(z)<1$\n",
    "- ReLU $g(z) = \\max(0,z)$\n",
    "\n",
    "The choice of an activation function is determined by the label of a layer. For _outut layer_,  \n",
    "- For classification problem - use **sigmoid**\n",
    "- for regression problem - use **linear** (if negative $y$ are present) or **ReLU** if $y>0$.  \n",
    "\n",
    "For Hidden layers **ReLU** is most used (why?.. faster) unless you are working with binary classification prblem, then use **sigmoid** (as it does not become flat on both sides)\n",
    "\n",
    "**NOTE** You cant use linear regression all the time. This just simplifes to a simgle linear regession and thus cannot be used to fit anything more complex. Activation function should be non-linear. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
