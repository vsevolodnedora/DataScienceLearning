{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advices\n",
    "\n",
    "When there is a large error in training of a ML algorithm:  \n",
    "- Ger more training examples\n",
    "- Try smaller sets of features\n",
    "- Get additional features\n",
    "- Add polynomial features ($x^1\\dots x^N$)\n",
    "- Increase/decrease $\\alpha$\n",
    "\n",
    "## Machine learning diagnostics\n",
    "What would help with algorithm performance/accuracy\n",
    "\n",
    "# Evaulating performance\n",
    "Consider a data set of housing prices:\n",
    "\n",
    "- Split dataset into 'training set' and 'test set' ($70\\% / 30\\%$). Train the model on the former and evaluate on the latter. $m_{\\rm train}$ and $n_{\\rm test}$, $x_{\\rm test}^{(m_{\\rm train})}$ $x_{\\rm train}^{(m_{\\rm test})}$,  \n",
    "Fit parameters by minimising usual cost fuction over training dataset: $ J(\\vec{w},b)=\\dots$  \n",
    "Evaluate the model over by computing cost func $J_{\\rm test}(\\vec{w},b)$. over test dataset **without** the regularization term $\\sum_{j=1}^{n}w_j^2$.  \n",
    "Compute **training error**: $J_{\\rm train}(\\vec{w},b) = \\frac{1}{2m_{\\rm train}} \\Big[ \\sum_{i=1}^{m_{\\rm train}} \\big( f_{\\vec{w},b}(\\vec{x}_{\\rm train}^{(i)}) - y_{\\rm train}^{(i)} \\big)^2 \\Big]$. \n",
    "\n",
    "Thus, if there is an overfitting, the model will give large error for test set. \n",
    "\n",
    "For **classification problem** instead of logistic loss, use a **fraction of the train set** that algorithm has missclassified. \n",
    "$$\n",
    "\\hat{y} = \n",
    "\\begin{cases}\n",
    "1 \\text{ if } f_{\\vec{w},b}(\\vec{x}^{(i)}) > 0.5 \\\\ \n",
    "0 \\text{ if } f_{\\vec{w},b}(\\vec{x}^{(i)}) < 0.5 \\\\ \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "then ${\\rm count} \\hat{y}\\neq y$ and $J_{\\rm test}$ and $J_{\\rm train}$ are `fractions` of misclassified data.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic model selection:\n",
    "\n",
    "Training error is not a good representaion of the model performance.  \n",
    "We need a general test, a test on the test data.  \n",
    "\n",
    "For instance. We start with a simplest polynomial, \n",
    "\n",
    "$$\n",
    "\\text{1D poly : } f_{\\vec{w},b}(\\vec{x}) = w_1x + b \\rightarrow w^{<1>}, b^{<1>} \\rightarrow J_{\\rm test}(w^{<1>}, b^{<1>})\\\\\n",
    "\\dots \\\\\n",
    "\\text{ND poly : } f_{\\vec{w},b}(\\vec{x}) = \\sum_{1}^N w_i x^{i} + b \\rightarrow w^{<N>}, b^{<N>} \\rightarrow J_{\\rm test}(w^{<N>}, b^{<N>})\n",
    "$$\n",
    "\n",
    "One can compare all the $J(test)$ and chose the model with the smallest one. However, this is `underestimation of an actual error` because extra parameter, $d$ was chosen based on the **test set**. \n",
    "\n",
    "## Automatic model selection \n",
    "### Training / cross validation / test set\n",
    "\n",
    "Splitting data in 3 subsers: $60\\%$ $x^{(m_{\\rm train})}$, $20\\%$ $x^{(m_{\\rm cv})}$ and $20\\%$ $x^{(m_{\\rm test})}$.  \n",
    "`Cross validation (validation/or development) set`: (cross check the accuracy of the model) \n",
    "\n",
    "For each set compute (no **regularization term here!**)\n",
    "$$\n",
    "\\text{Training error: } \\dots \\\\\n",
    "\\text{Cross alidation error: } \\dots \\\\\n",
    "\\text{Test error: } \\dots \\\\\n",
    "$$\n",
    "\n",
    "Then for each model in your model selection options evaluate these on the cross validation set: \n",
    "$$\n",
    "\\text{1D poly : } f_{\\vec{w},b}(\\vec{x}) = w_1x + b \\rightarrow w^{<1>}, b^{<1>} \\rightarrow J_{\\rm cv}(w^{<1>}, b^{<1>})\\\\\n",
    "\\dots \\\\\n",
    "\\text{ND poly : } f_{\\vec{w},b}(\\vec{x}) = \\sum_{1}^N w_i x^{i} + b \\rightarrow w^{<N>}, b^{<N>} \\rightarrow J_{\\rm cv}(w^{<N>}, b^{<N>})\n",
    "$$\n",
    "\n",
    "The model with the lowest $cv$ error is the best option. \n",
    "The `generalized error ` is estimated by performing this analysis on the **test data**. \n",
    "\n",
    "> Pick model with the smallest cross validation error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-20 09:18:08.050656: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# for array computations and loading data\n",
    "import numpy as np\n",
    "\n",
    "# for building linear regression models and preparing data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# for building and training neural networks\n",
    "import tensorflow as tf\n",
    "\n",
    "# custom functions\n",
    "import utils\n",
    "\n",
    "# reduce display precision on numpy arrays\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# suppress warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyBlastAfterglow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
