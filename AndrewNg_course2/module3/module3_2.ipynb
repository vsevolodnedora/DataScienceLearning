{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diognosing bias and variance\n",
    "\n",
    "Recall: `high bias` = underfitting and `high variance` = overfitting.  \n",
    "Diagnosisi: **training set** and **validation set**. \n",
    "- Underfitting (high bias) - If $J_{\\rm train}$ is large, and $J_{\\rm cv}$ is \n",
    "- Overfitting If $J_{\\rm cv} \\gg J_{\\rm train}$ \n",
    "- Look for $J_{\\rm cv} \\sim J_{\\rm train}$ and both are low\n",
    "\n",
    "For a polynomial regressing one can plot the $J_{\\rm train} = f(d)$, where $d$ is the polynomial degree.  \n",
    "Consider _minimum_ of $J_{\\rm cv} = f(d)$. It is at an **optimal degree**\n",
    "\n",
    "**Complex situation**: high bias and high varaiance, where part of the data is overfitted an part of the data is underfitting... \n",
    "\n",
    "## Regularization \\& bias \\& variance\n",
    "\n",
    "Choice of $\\lambda$ is outcome. \n",
    "Consider $d=4$ polynomial. \n",
    "- If $\\lambda$ is large, the model forces $w_{i}\\rightarrow 0$ and model just gives a mean value. \n",
    "- If $\\lambda$ is very small, the model does not do regularization and ovefits the data and  $J_{\\rm cv} \\gg J_{\\rm train}$. \n",
    "- If $\\lambda$ is in the middle, it might help decreasing overfitting.  \n",
    "> To chose the right $\\lambda$. Consider several values of $\\lambda_i$ and compute the $J_{\\rm cv}$ and then select the value that gives the smallest $J_{\\rm cv}$. This is similar to _grid search_. See also _hyperparameter tuning_ in 100 page book on DS.\n",
    "\n",
    "For reporting use the $J_{\\rm test}$   \n",
    "Plotting $J_{\\rm cv} = f(\\lambda)$ we see a $J_{\\rm cv}(\\lambda)$ func has a minimum while $J_{\\rm train}(\\lambda)$ is a monotonic increasing function. \n",
    "\n",
    "## Establishing a baseline level of performance\n",
    "\n",
    "Consider a speech recognition system and $J_{\\rm train} = 10.8\\%$ and $J_{\\rm cv}=14.8\\%$. To assess the magnitude of these values consider the human level performance (a reference value) and it is $J_{\\rm human} = 10.6\\%$. Than, as it is close to humans, the performance is good on a training data. However, $J_{\\rm cv} > J_{\\rm train}$ and the problem _seem to have variance problem_. \n",
    "- If $J_{\\rm train} - J_{\\rm base}$ is large -- bias problem (high bias)\n",
    "- if $J_{\\rm cv} - J_{\\rm train}$ is large -- variance problem (high variance)\n",
    "\n",
    "> Compare the baseline with training and cross-validation errors. If the difference between the ofrmer two is large: **high bias**, if the difference between latter two is large **high variance**. \n",
    "\n",
    "## Learning curve\n",
    "\n",
    "Plot the $J_{\\rm cv}$ and $J_{\\rm train}$ as a function of $m_{\\rm train}$ (size of the training set). These two curves would get closer and closer as $m_{\\rm train}$ increases. \n",
    "\n",
    "In the case of **high bias** the training error will plateu at a certain $m_{\\rm train}$. Similarly, the $J_{\\rm cv}$ will decrease and plateu after a while. Comparing this with the horizontal line of the bas-line. \n",
    "> Having a high bias, increasing the training set does not help (due to plateus)\n",
    "\n",
    "Consider a high variance case. Than the gap between increasing $J_{\\rm train}(m_{\\rm train})$ and decreasing $J_{\\rm cv}$ would be much larger. The base-line can actually lie above the $J_{\\rm train}$, as it overfits the data and performs thus better than, e.g., human analysis. Increasing the $m_{\\rm train}$, however, would bring both curves closer to the base line.\n",
    "> Having high variance, increasing the training set size may help."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical advices\n",
    "\n",
    "Previously, if the large errors were endentified, the following was advised:\n",
    "- More training examples $\\rightarrow$ ($\\textcolor{yellow}{\\text{high variance}}$)\n",
    "- Smaller set of features (e.g., remove redundant features) $\\rightarrow$ ($\\textcolor{yellow}{\\text{high variance}}$)\n",
    "- Addiational features (add more meaningfull features) $\\rightarrow$($\\textcolor{cyan}{\\text{high bias}}$)\n",
    "- Polynomial features (like adding more meaningful features)$\\rightarrow$ ($\\textcolor{cyan}{\\text{high bias}}$)\n",
    "- Decrease $\\lambda$ $\\rightarrow$ ($\\textcolor{cyan}{\\text{high bias}}$)\n",
    "- Increase $\\lambda$ $\\rightarrow$ ($\\textcolor{yellow}{\\text{high variance}}$)\n",
    "\n",
    "However, these _do_ help with high variance or high bias. \n",
    "\n",
    "### Bias-variance tradeoff\n",
    "\n",
    "> Large NN on small datasets -- low-biass machines  \n",
    "\n",
    "Algorithm:  \n",
    "Consider $J_{\\rm train}$. If it is large, make a bigger NN. When it does sufficiently good, assess it on cross-validating set. If $J_{\\rm cv}$ is large, get more data... untill it performs well, but then consider the $J_{\\rm train}$ again. \n",
    "\n",
    "> A largern NN with well chosen **regularization** would do better than smaller NN.\n",
    "\n",
    "To regularize a NN consider:  \n",
    "$\\texttt{Dense(unit=N, activation=\"sigmoid\", kernel\\_regularizer=L2(0.01))}$  \n",
    "(remember that only weights are regularized)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
