{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Gentle Introduction to Graph Neural Networks\n",
    "This notebook follows https://distill.pub/2021/gnn-intro/ \n",
    "\n",
    "! A `Graph` represents the relations (_edges_) between a collection of entities (_nodes_).  \n",
    "> A Graph has\n",
    "- $V$ vertex (node identity)\n",
    "- $E$ edge (directions)\n",
    "- $U$ global (master node)\n",
    "\n",
    "! Information in the form of scalars or embeddings can be sotred at each graph node or edge\n",
    "\n",
    "> Types of graphs:\n",
    "- Directed\n",
    "- Undirected\n",
    "\n",
    "> Bad examples of graphs:\n",
    "- Images (pixes as node, with 8 connecttions to neighbours)\n",
    "- Texts (directed graph, connections between adjacent words)\n",
    "\n",
    "> Adjecency matrix is $n_{\\rm nodes}\\times n_{\\rm nodes}$. Non-zero, where nodes have an edge.  \n",
    "- For an image as a graph, this matrix is semi-diagonal __banded structure__ in adjacency matrix. \n",
    "- For text adjacency is diagonal (one connection) \n",
    "\n",
    "Graphs are __redundant__ when data structure is _regular_ (pixes, words are regular).  \n",
    "Most __usefull__ when data is _heterogeneously_ structured (i.e., when number of neighbours is different for various nodes)  \n",
    "\n",
    "> Good examples of graphs:\n",
    "- Moleculas (adjecency matrix while symmetric is not banded anymore)\n",
    "- Social networks as graphs (adjecency matrix _is not_ identical)\n",
    "- Citation networks as graphs (why cited whome) (directed graph)\n",
    "- In CV taged objects in a scene may be used as nodes in a graph\n",
    "- ML models, math equations... (see _dataflow graph_)\n",
    "\n",
    "> Tasks on graphs (solved with GNNs)\n",
    "- Graph-level (property of the _entire_ graph aka image classification or sentiment analysis)\n",
    "- Node-level (property if a node aka image segmentation (a role of a pixel in the image) or word-level anlaysis of speach)\n",
    "- Edge-level (what nodes share an edge, and what is the property of the edge, aka image scene understanding)\n",
    "\n",
    "> Information on a Graph\n",
    "- Nodes (e.g., with node feature matrix $N$ where each node has an index)\n",
    "- Edges (non-trivial, varaible number, space-inefficient, non-unique).  \n",
    "__Adjacency lists__ a possible solution. Each connectivity is desctibed with a _tuple_ (i,j). This represenation is _permutation invariant_.  \n",
    "- Global-context\n",
    "- Connectivity\n",
    "\n",
    "! For each graph/edge/node there exists a vector to complete the tensor\n",
    "\n",
    "> A GNN is an optimizable transformation on all attributes of the graph (nodes, edges, global-context) that preserves graph symmetries (permutation invariances). \n",
    "\n",
    "### Example (neglecting massege passing)\n",
    "\n",
    "Example of method of construction: _message passing neural network_ using _Graph Nets_ architecture.  \n",
    "Model accepts the graph as an input loaded into nodes, edges, global context and progressively transform these embeddings _without_ changing the connectivity.  \n",
    "Graph in; graph out.  \n",
    "\n",
    "! A GNN is using a _separate_ multilayer perceptron (MLP) $f$ on _each_ component of the graph.  \n",
    "For each node and edge vecotors, $f$ is applied:\n",
    "$$\n",
    "U_n - f_{U_n} - U_{n+1} \\\\\n",
    "V_n - f_{U_n} - V_{n+1} \\\\\n",
    "E_n - f_{U_n} - E_{n+1} \n",
    "$$\n",
    "via a _graph independent layer_. \n",
    "The result is a new graph with changed properties but __not changing__ the _adjecency list_ in this case. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exchange information between nodes and edges can be done with __pooling__ $\\rho$ which is\n",
    "- Gather all embeddings for each item (that is to be pooled) and concatenate into ! matrix\n",
    "- Agregade the embeddings via e.g., sum operation\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "V_{n} \\\\\n",
    "E_{n}\n",
    "\\end{cases}\n",
    "- \\rho_{E_n-V_n}\n",
    "- C_{v_{n}}\n",
    "- \\text{node prediction}\n",
    "$$\n",
    "\n",
    "where $\\rho$ is a pooling operation and $C$ is a final classification operation\n",
    "\n",
    "! Global features are predicted by performing `Global Average Pooling` in CNNs\n",
    "\n",
    "! Key of a GNN is a way to pass information. Pooling is one a the ways. \n",
    "\n",
    "> Pooling _indie_ the GNN layer allows to use connectivity information when learning\n",
    "\n",
    "This is done by _massege passing_, where neighbouring nodes/edges echange information and influence each other's updated embeddings as:\n",
    "- Collect all _neighbouring_ node embeddings (or _messeges_) with function $g()$\n",
    "- aggregate all messeges with an aggredate function (e.g., sum)\n",
    "- _all_ pooled messeges are then passed through an _update function_ e.g., a NN.  \n",
    "\n",
    "> This is the simpleset messege passing in GNN layer (collect neighbours, aggregate/pool, update)\n",
    "\n",
    "! Messege passing is akin convolution -- both are aggregating information \n",
    "\n",
    "! Stacking layers with information passing allows to aggregate the information in the entire network\n",
    "\n",
    "$$\n",
    "U_n - f_{U_n} - U_{n+1} \\\\\n",
    "V_n  - \\rho_{V_n-V_n}- f_{U_n} - V_{n+1} \\\\\n",
    "E_n - f_{U_n} - E_{n+1} \n",
    "$$\n",
    "\n",
    "where $\\rho$ is a pooling function\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
